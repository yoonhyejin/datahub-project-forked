"use strict";(self.webpackChunkdocs_website=self.webpackChunkdocs_website||[]).push([[6126],{3905:(e,t,n)=>{n.d(t,{Zo:()=>u,kt:()=>m});var a=n(67294);function l(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){l(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function o(e,t){if(null==e)return{};var n,a,l=function(e,t){if(null==e)return{};var n,a,l={},r=Object.keys(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||(l[n]=e[n]);return l}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(l[n]=e[n])}return l}var s=a.createContext({}),p=function(e){var t=a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},u=function(e){var t=p(e.components);return a.createElement(s.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},c=a.forwardRef((function(e,t){var n=e.components,l=e.mdxType,r=e.originalType,s=e.parentName,u=o(e,["components","mdxType","originalType","parentName"]),c=p(n),m=l,f=c["".concat(s,".").concat(m)]||c[m]||d[m]||r;return n?a.createElement(f,i(i({ref:t},u),{},{components:n})):a.createElement(f,i({ref:t},u))}));function m(e,t){var n=arguments,l=t&&t.mdxType;if("string"==typeof e||l){var r=n.length,i=new Array(r);i[0]=c;var o={};for(var s in t)hasOwnProperty.call(t,s)&&(o[s]=t[s]);o.originalType=e,o.mdxType="string"==typeof e?e:l,i[1]=o;for(var p=2;p<r;p++)i[p]=n[p];return a.createElement.apply(null,i)}return a.createElement.apply(null,n)}c.displayName="MDXCreateElement"},18679:(e,t,n)=>{n.d(t,{Z:()=>i});var a=n(67294),l=n(86010);const r="tabItem_Ymn6";function i(e){let{children:t,hidden:n,className:i}=e;return a.createElement("div",{role:"tabpanel",className:(0,l.Z)(r,i),hidden:n},t)}},34259:(e,t,n)=>{n.d(t,{Z:()=>m});var a=n(83117),l=n(67294),r=n(86010),i=n(51048),o=n(33609),s=n(1943),p=n(72957);const u="tabList__CuJ",d="tabItem_LNqP";function c(e){const{lazy:t,block:n,defaultValue:i,values:c,groupId:m,className:f}=e,g=l.Children.map(e.children,(e=>{if((0,l.isValidElement)(e)&&"value"in e.props)return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})),k=c??g.map((e=>{let{props:{value:t,label:n,attributes:a}}=e;return{value:t,label:n,attributes:a}})),h=(0,o.l)(k,((e,t)=>e.value===t.value));if(h.length>0)throw new Error(`Docusaurus error: Duplicate values "${h.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`);const b=null===i?i:i??g.find((e=>e.props.default))?.props.value??g[0].props.value;if(null!==b&&!k.some((e=>e.value===b)))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${b}" but none of its children has the corresponding value. Available values are: ${k.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);const{tabGroupChoices:N,setTabGroupChoices:y}=(0,s.U)(),[_,v]=(0,l.useState)(b),w=[],{blockElementScrollPositionUntilNextRender:S}=(0,p.o5)();if(null!=m){const e=N[m];null!=e&&e!==_&&k.some((t=>t.value===e))&&v(e)}const x=e=>{const t=e.currentTarget,n=w.indexOf(t),a=k[n].value;a!==_&&(S(t),v(a),null!=m&&y(m,String(a)))},T=e=>{let t=null;switch(e.key){case"Enter":x(e);break;case"ArrowRight":{const n=w.indexOf(e.currentTarget)+1;t=w[n]??w[0];break}case"ArrowLeft":{const n=w.indexOf(e.currentTarget)-1;t=w[n]??w[w.length-1];break}}t?.focus()};return l.createElement("div",{className:(0,r.Z)("tabs-container",u)},l.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,r.Z)("tabs",{"tabs--block":n},f)},k.map((e=>{let{value:t,label:n,attributes:i}=e;return l.createElement("li",(0,a.Z)({role:"tab",tabIndex:_===t?0:-1,"aria-selected":_===t,key:t,ref:e=>w.push(e),onKeyDown:T,onClick:x},i,{className:(0,r.Z)("tabs__item",d,i?.className,{"tabs__item--active":_===t})}),n??t)}))),t?(0,l.cloneElement)(g.filter((e=>e.props.value===_))[0],{className:"margin-top--md"}):l.createElement("div",{className:"margin-top--md"},g.map(((e,t)=>(0,l.cloneElement)(e,{key:t,hidden:e.props.value!==_})))))}function m(e){const t=(0,i.Z)();return l.createElement(c,(0,a.Z)({key:String(t)},e))}},39278:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>u,contentTitle:()=>s,default:()=>m,frontMatter:()=>o,metadata:()=>p,toc:()=>d});var a=n(83117),l=(n(67294),n(3905)),r=n(34259),i=n(18679);const o={sidebar_position:41,title:"S3 Data Lake",slug:"/generated/ingestion/sources/s3",custom_edit_url:"https://github.com/datahub-project/datahub/blob/master/docs/generated/ingestion/sources/s3.md"},s="S3 Data Lake",p={unversionedId:"docs/generated/ingestion/sources/s3",id:"docs/generated/ingestion/sources/s3",title:"S3 Data Lake",description:"Module s3",source:"@site/genDocs/docs/generated/ingestion/sources/s3.md",sourceDirName:"docs/generated/ingestion/sources",slug:"/generated/ingestion/sources/s3",permalink:"/datahub-project-forked/docs/generated/ingestion/sources/s3",draft:!1,editUrl:"https://github.com/datahub-project/datahub/blob/master/docs/generated/ingestion/sources/s3.md",tags:[],version:"current",sidebarPosition:41,frontMatter:{sidebar_position:41,title:"S3 Data Lake",slug:"/generated/ingestion/sources/s3",custom_edit_url:"https://github.com/datahub-project/datahub/blob/master/docs/generated/ingestion/sources/s3.md"},sidebar:"overviewSidebar",previous:{title:"Redshift",permalink:"/datahub-project-forked/docs/generated/ingestion/sources/redshift"},next:{title:"SageMaker",permalink:"/datahub-project-forked/docs/generated/ingestion/sources/sagemaker"}},u={},d=[{value:"Module <code>s3</code>",id:"module-s3",level:2},{value:"Important Capabilities",id:"important-capabilities",level:3},{value:"CLI based Ingestion",id:"cli-based-ingestion",level:3},{value:"Install the Plugin",id:"install-the-plugin",level:4},{value:"Starter Recipe",id:"starter-recipe",level:3},{value:"Config Details",id:"config-details",level:3},{value:"Path Specs",id:"path-specs",level:3},{value:"Compatibility",id:"compatibility",level:3},{value:"Code Coordinates",id:"code-coordinates",level:3},{value:"Questions",id:"questions",level:2}],c={toc:d};function m(e){let{components:t,...n}=e;return(0,l.kt)("wrapper",(0,a.Z)({},c,n,{components:t,mdxType:"MDXLayout"}),(0,l.kt)("h1",{id:"s3-data-lake"},"S3 Data Lake"),(0,l.kt)("h2",{id:"module-s3"},"Module ",(0,l.kt)("inlineCode",{parentName:"h2"},"s3")),(0,l.kt)("p",null,(0,l.kt)("img",{parentName:"p",src:"https://img.shields.io/badge/support%20status-incubating-blue",alt:"Incubating"})),(0,l.kt)("h3",{id:"important-capabilities"},"Important Capabilities"),(0,l.kt)("table",null,(0,l.kt)("thead",{parentName:"table"},(0,l.kt)("tr",{parentName:"thead"},(0,l.kt)("th",{parentName:"tr",align:null},"Capability"),(0,l.kt)("th",{parentName:"tr",align:null},"Status"),(0,l.kt)("th",{parentName:"tr",align:null},"Notes"))),(0,l.kt)("tbody",{parentName:"table"},(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("a",{parentName:"td",href:"/datahub-project-forked/docs/metadata-ingestion/docs/dev_guides/sql_profiles"},"Data Profiling")),(0,l.kt)("td",{parentName:"tr",align:null},"\u2705"),(0,l.kt)("td",{parentName:"tr",align:null},"Optionally enabled via configuration")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"Extract Tags"),(0,l.kt)("td",{parentName:"tr",align:null},"\u2705"),(0,l.kt)("td",{parentName:"tr",align:null},"Can extract S3 object/bucket tags if enabled")))),(0,l.kt)("p",null,"This plugin extracts:"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"Row and column counts for each table"),(0,l.kt)("li",{parentName:"ul"},"For each column, if profiling is enabled:",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"null counts and proportions"),(0,l.kt)("li",{parentName:"ul"},"distinct counts and proportions"),(0,l.kt)("li",{parentName:"ul"},"minimum, maximum, mean, median, standard deviation, some quantile values"),(0,l.kt)("li",{parentName:"ul"},"histograms or frequencies of unique values")))),(0,l.kt)("p",null,"This connector supports both local files as well as those stored on AWS S3 (which must be identified using the prefix ",(0,l.kt)("inlineCode",{parentName:"p"},"s3://"),"). Supported file types are as follows:"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"CSV"),(0,l.kt)("li",{parentName:"ul"},"TSV"),(0,l.kt)("li",{parentName:"ul"},"JSON"),(0,l.kt)("li",{parentName:"ul"},"Parquet"),(0,l.kt)("li",{parentName:"ul"},"Apache Avro")),(0,l.kt)("p",null,"Schemas for Parquet and Avro files are extracted as provided."),(0,l.kt)("p",null,"Schemas for schemaless formats (CSV, TSV, JSON) are inferred. For CSV and TSV files, we consider the first 100 rows by default, which can be controlled via the ",(0,l.kt)("inlineCode",{parentName:"p"},"max_rows")," recipe parameter (see ",(0,l.kt)("a",{parentName:"p",href:"#config-details"},"below"),")\nJSON file schemas are inferred on the basis of the entire file (given the difficulty in extracting only the first few objects of the file), which may impact performance.\nWe are working on using iterator-based JSON parsers to avoid reading in the entire JSON object."),(0,l.kt)("p",null,"Note that because the profiling is run with PySpark, we require Spark 3.0.3 with Hadoop 3.2 to be installed (see ",(0,l.kt)("a",{parentName:"p",href:"#compatibility"},"compatibility")," for more details). If profiling, make sure that permissions for ",(0,l.kt)("strong",{parentName:"p"},"s3a://")," access are set because Spark and Hadoop use the s3a:// protocol to interface with AWS (schema inference outside of profiling requires s3:// access).\nEnabling profiling will slow down ingestion runs."),(0,l.kt)("h3",{id:"cli-based-ingestion"},"CLI based Ingestion"),(0,l.kt)("h4",{id:"install-the-plugin"},"Install the Plugin"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-shell"},"pip install 'acryl-datahub[s3]'\n")),(0,l.kt)("h3",{id:"starter-recipe"},"Starter Recipe"),(0,l.kt)("p",null,"Check out the following recipe to get started with ingestion! See ",(0,l.kt)("a",{parentName:"p",href:"#config-details"},"below")," for full configuration options."),(0,l.kt)("p",null,"For general pointers on writing and running a recipe, see our ",(0,l.kt)("a",{parentName:"p",href:"/datahub-project-forked/docs/metadata-ingestion#recipes"},"main recipe guide"),"."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-yaml"},'source:\n  type: s3\n  config:\n    path_specs:\n      -\n        include: "s3://covid19-lake/covid_knowledge_graph/csv/nodes/*.*"\n\n    aws_config:\n      aws_access_key_id: *****\n      aws_secret_access_key: *****\n      aws_region: us-east-2\n    env: "PROD"\n    profiling:\n      enabled: false\n\n# sink configs\n\n')),(0,l.kt)("h3",{id:"config-details"},"Config Details"),(0,l.kt)(r.Z,{mdxType:"Tabs"},(0,l.kt)(i.Z,{value:"options",label:"Options",default:!0,mdxType:"TabItem"},(0,l.kt)("p",null,"Note that a ",(0,l.kt)("inlineCode",{parentName:"p"},".")," is used to denote nested fields in the YAML recipe."),(0,l.kt)("details",{open:!0},(0,l.kt)("summary",null,"View All Configuration Options"),(0,l.kt)("table",null,(0,l.kt)("thead",{parentName:"table"},(0,l.kt)("tr",{parentName:"thead"},(0,l.kt)("th",{parentName:"tr",align:null},"Field"),(0,l.kt)("th",{parentName:"tr",align:null},"Required"),(0,l.kt)("th",{parentName:"tr",align:null},"Type"),(0,l.kt)("th",{parentName:"tr",align:null},"Description"),(0,l.kt)("th",{parentName:"tr",align:null},"Default"))),(0,l.kt)("tbody",{parentName:"table"},(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"env"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"string"),(0,l.kt)("td",{parentName:"tr",align:null},"The environment that all assets produced by this connector belong to"),(0,l.kt)("td",{parentName:"tr",align:null},"PROD")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"platform_instance"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"string"),(0,l.kt)("td",{parentName:"tr",align:null},"The instance of the platform that all assets produced by this recipe belong to"),(0,l.kt)("td",{parentName:"tr",align:null},"None")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"path_specs"),(0,l.kt)("td",{parentName:"tr",align:null},"\u2705"),(0,l.kt)("td",{parentName:"tr",align:null},"Array of object"),(0,l.kt)("td",{parentName:"tr",align:null},"List of PathSpec. See ",(0,l.kt)("a",{parentName:"td",href:"#path-spec"},"below")," the details about PathSpec"),(0,l.kt)("td",{parentName:"tr",align:null},"None")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"platform"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"string"),(0,l.kt)("td",{parentName:"tr",align:null},"The platform that this source connects to (either 's3' or 'file'). If not specified, the platform will be inferred from the path_specs."),(0,l.kt)("td",{parentName:"tr",align:null})),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"use_s3_bucket_tags"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"boolean"),(0,l.kt)("td",{parentName:"tr",align:null},"Whether or not to create tags in datahub from the s3 bucket"),(0,l.kt)("td",{parentName:"tr",align:null},"None")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"use_s3_object_tags"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"boolean"),(0,l.kt)("td",{parentName:"tr",align:null},"# Whether or not to create tags in datahub from the s3 object"),(0,l.kt)("td",{parentName:"tr",align:null},"None")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"update_schema_on_partition_file_updates"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"boolean"),(0,l.kt)("td",{parentName:"tr",align:null},"Whether to update the table schema when schema in files within the partitions are updated."),(0,l.kt)("td",{parentName:"tr",align:null},"False")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"spark_driver_memory"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"string"),(0,l.kt)("td",{parentName:"tr",align:null},"Max amount of memory to grant Spark."),(0,l.kt)("td",{parentName:"tr",align:null},"4g")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"max_rows"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"integer"),(0,l.kt)("td",{parentName:"tr",align:null},"Maximum number of rows to use when inferring schemas for TSV and CSV files."),(0,l.kt)("td",{parentName:"tr",align:null},"100")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"verify_ssl"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"Generic dict"),(0,l.kt)("td",{parentName:"tr",align:null},"Either a boolean, in which case it controls whether we verify the server's TLS certificate, or a string, in which case it must be a path to a CA bundle to use."),(0,l.kt)("td",{parentName:"tr",align:null},"True")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"aws_config"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"AwsConnectionConfig (see below for fields)"),(0,l.kt)("td",{parentName:"tr",align:null},"AWS configuration"),(0,l.kt)("td",{parentName:"tr",align:null})),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"aws_config.aws_access_key_id"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"string"),(0,l.kt)("td",{parentName:"tr",align:null},"AWS access key ID. Can be auto-detected, see ",(0,l.kt)("a",{parentName:"td",href:"https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html"},"https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html")," for details."),(0,l.kt)("td",{parentName:"tr",align:null},"None")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"aws_config.aws_secret_access_key"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"string"),(0,l.kt)("td",{parentName:"tr",align:null},"AWS secret access key. Can be auto-detected, see ",(0,l.kt)("a",{parentName:"td",href:"https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html"},"https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html")," for details."),(0,l.kt)("td",{parentName:"tr",align:null},"None")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"aws_config.aws_session_token"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"string"),(0,l.kt)("td",{parentName:"tr",align:null},"AWS session token. Can be auto-detected, see ",(0,l.kt)("a",{parentName:"td",href:"https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html"},"https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html")," for details."),(0,l.kt)("td",{parentName:"tr",align:null},"None")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"aws_config.aws_role"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"Generic dict"),(0,l.kt)("td",{parentName:"tr",align:null},"AWS roles to assume. If using the string format, the role ARN can be specified directly. If using the object format, the role can be specified in the RoleArn field and additional available arguments are documented at ",(0,l.kt)("a",{parentName:"td",href:"https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sts.html?highlight=assume_role#STS.Client.assume_role"},"https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sts.html?highlight=assume_role#STS.Client.assume_role")),(0,l.kt)("td",{parentName:"tr",align:null},"None")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"aws_config.aws_profile"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"string"),(0,l.kt)("td",{parentName:"tr",align:null},"Named AWS profile to use. Only used if access key / secret are unset. If not set the default will be used"),(0,l.kt)("td",{parentName:"tr",align:null},"None")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"aws_config.aws_region"),(0,l.kt)("td",{parentName:"tr",align:null},"\u2753 (required if aws_config is set)"),(0,l.kt)("td",{parentName:"tr",align:null},"string"),(0,l.kt)("td",{parentName:"tr",align:null},"AWS region code."),(0,l.kt)("td",{parentName:"tr",align:null},"None")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"aws_config.aws_endpoint_url"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"string"),(0,l.kt)("td",{parentName:"tr",align:null},"Autodetected. See ",(0,l.kt)("a",{parentName:"td",href:"https://boto3.amazonaws.com/v1/documentation/api/latest/reference/core/session.html"},"https://boto3.amazonaws.com/v1/documentation/api/latest/reference/core/session.html")),(0,l.kt)("td",{parentName:"tr",align:null},"None")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"aws_config.aws_proxy"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"Dict","[str,string]"),(0,l.kt)("td",{parentName:"tr",align:null},"Autodetected. See ",(0,l.kt)("a",{parentName:"td",href:"https://boto3.amazonaws.com/v1/documentation/api/latest/reference/core/session.html"},"https://boto3.amazonaws.com/v1/documentation/api/latest/reference/core/session.html")),(0,l.kt)("td",{parentName:"tr",align:null})),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"profile_patterns"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"AllowDenyPattern (see below for fields)"),(0,l.kt)("td",{parentName:"tr",align:null},"regex patterns for tables to profile"),(0,l.kt)("td",{parentName:"tr",align:null},"{'allow': ","['.*']",", 'deny': [], 'ignoreCase': True}")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"profile_patterns.allow"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"Array of string"),(0,l.kt)("td",{parentName:"tr",align:null},"List of regex patterns to include in ingestion"),(0,l.kt)("td",{parentName:"tr",align:null},"['.*']")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"profile_patterns.deny"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"Array of string"),(0,l.kt)("td",{parentName:"tr",align:null},"List of regex patterns to exclude from ingestion."),(0,l.kt)("td",{parentName:"tr",align:null},"[]")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"profile_patterns.ignoreCase"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"boolean"),(0,l.kt)("td",{parentName:"tr",align:null},"Whether to ignore case sensitivity during pattern matching."),(0,l.kt)("td",{parentName:"tr",align:null},"True")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"profiling"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"DataLakeProfilerConfig (see below for fields)"),(0,l.kt)("td",{parentName:"tr",align:null},"Data profiling configuration"),(0,l.kt)("td",{parentName:"tr",align:null},"{'enabled': False, 'profile_table_level_only': False, 'max_number_of_fields_to_profile': None, 'include_field_null_count': True, 'include_field_min_value': True, 'include_field_max_value': True, 'include_field_mean_value': True, 'include_field_median_value': True, 'include_field_stddev_value': True, 'include_field_quantiles': True, 'include_field_distinct_value_frequencies': True, 'include_field_histogram': True, 'include_field_sample_values': True}")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"profiling.enabled"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"boolean"),(0,l.kt)("td",{parentName:"tr",align:null},"Whether profiling should be done."),(0,l.kt)("td",{parentName:"tr",align:null},"False")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"profiling.profile_table_level_only"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"boolean"),(0,l.kt)("td",{parentName:"tr",align:null},"Whether to perform profiling at table-level only or include column-level profiling as well."),(0,l.kt)("td",{parentName:"tr",align:null},"False")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"profiling.max_number_of_fields_to_profile"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"integer"),(0,l.kt)("td",{parentName:"tr",align:null},"A positive integer that specifies the maximum number of columns to profile for any table. ",(0,l.kt)("inlineCode",{parentName:"td"},"None")," implies all columns. The cost of profiling goes up significantly as the number of columns to profile goes up."),(0,l.kt)("td",{parentName:"tr",align:null},"None")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"profiling.include_field_null_count"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"boolean"),(0,l.kt)("td",{parentName:"tr",align:null},"Whether to profile for the number of nulls for each column."),(0,l.kt)("td",{parentName:"tr",align:null},"True")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"profiling.include_field_min_value"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"boolean"),(0,l.kt)("td",{parentName:"tr",align:null},"Whether to profile for the min value of numeric columns."),(0,l.kt)("td",{parentName:"tr",align:null},"True")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"profiling.include_field_max_value"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"boolean"),(0,l.kt)("td",{parentName:"tr",align:null},"Whether to profile for the max value of numeric columns."),(0,l.kt)("td",{parentName:"tr",align:null},"True")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"profiling.include_field_mean_value"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"boolean"),(0,l.kt)("td",{parentName:"tr",align:null},"Whether to profile for the mean value of numeric columns."),(0,l.kt)("td",{parentName:"tr",align:null},"True")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"profiling.include_field_median_value"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"boolean"),(0,l.kt)("td",{parentName:"tr",align:null},"Whether to profile for the median value of numeric columns."),(0,l.kt)("td",{parentName:"tr",align:null},"True")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"profiling.include_field_stddev_value"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"boolean"),(0,l.kt)("td",{parentName:"tr",align:null},"Whether to profile for the standard deviation of numeric columns."),(0,l.kt)("td",{parentName:"tr",align:null},"True")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"profiling.include_field_quantiles"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"boolean"),(0,l.kt)("td",{parentName:"tr",align:null},"Whether to profile for the quantiles of numeric columns."),(0,l.kt)("td",{parentName:"tr",align:null},"True")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"profiling.include_field_distinct_value_frequencies"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"boolean"),(0,l.kt)("td",{parentName:"tr",align:null},"Whether to profile for distinct value frequencies."),(0,l.kt)("td",{parentName:"tr",align:null},"True")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"profiling.include_field_histogram"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"boolean"),(0,l.kt)("td",{parentName:"tr",align:null},"Whether to profile for the histogram for numeric fields."),(0,l.kt)("td",{parentName:"tr",align:null},"True")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"profiling.include_field_sample_values"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"boolean"),(0,l.kt)("td",{parentName:"tr",align:null},"Whether to profile for the sample values for all columns."),(0,l.kt)("td",{parentName:"tr",align:null},"True")))))),(0,l.kt)(i.Z,{value:"schema",label:"Schema",mdxType:"TabItem"},(0,l.kt)("p",null,"The ",(0,l.kt)("a",{parentName:"p",href:"https://json-schema.org/"},"JSONSchema")," for this configuration is inlined below."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-javascript"},'{\n  "title": "DataLakeSourceConfig",\n  "description": "Any source that connects to a platform should inherit this class",\n  "type": "object",\n  "properties": {\n    "env": {\n      "title": "Env",\n      "description": "The environment that all assets produced by this connector belong to",\n      "default": "PROD",\n      "type": "string"\n    },\n    "platform_instance": {\n      "title": "Platform Instance",\n      "description": "The instance of the platform that all assets produced by this recipe belong to",\n      "type": "string"\n    },\n    "path_specs": {\n      "title": "Path Specs",\n      "description": "List of PathSpec. See [below](#path-spec) the details about PathSpec",\n      "type": "array",\n      "items": {\n        "$ref": "#/definitions/PathSpec"\n      }\n    },\n    "platform": {\n      "title": "Platform",\n      "description": "The platform that this source connects to (either \'s3\' or \'file\'). If not specified, the platform will be inferred from the path_specs.",\n      "default": "",\n      "type": "string"\n    },\n    "aws_config": {\n      "title": "Aws Config",\n      "description": "AWS configuration",\n      "allOf": [\n        {\n          "$ref": "#/definitions/AwsConnectionConfig"\n        }\n      ]\n    },\n    "use_s3_bucket_tags": {\n      "title": "Use S3 Bucket Tags",\n      "description": "Whether or not to create tags in datahub from the s3 bucket",\n      "type": "boolean"\n    },\n    "use_s3_object_tags": {\n      "title": "Use S3 Object Tags",\n      "description": "# Whether or not to create tags in datahub from the s3 object",\n      "type": "boolean"\n    },\n    "update_schema_on_partition_file_updates": {\n      "title": "Update Schema On Partition File Updates",\n      "description": "Whether to update the table schema when schema in files within the partitions are updated.",\n      "default": false,\n      "type": "boolean"\n    },\n    "profile_patterns": {\n      "title": "Profile Patterns",\n      "description": "regex patterns for tables to profile ",\n      "default": {\n        "allow": [\n          ".*"\n        ],\n        "deny": [],\n        "ignoreCase": true\n      },\n      "allOf": [\n        {\n          "$ref": "#/definitions/AllowDenyPattern"\n        }\n      ]\n    },\n    "profiling": {\n      "title": "Profiling",\n      "description": "Data profiling configuration",\n      "default": {\n        "enabled": false,\n        "profile_table_level_only": false,\n        "max_number_of_fields_to_profile": null,\n        "include_field_null_count": true,\n        "include_field_min_value": true,\n        "include_field_max_value": true,\n        "include_field_mean_value": true,\n        "include_field_median_value": true,\n        "include_field_stddev_value": true,\n        "include_field_quantiles": true,\n        "include_field_distinct_value_frequencies": true,\n        "include_field_histogram": true,\n        "include_field_sample_values": true\n      },\n      "allOf": [\n        {\n          "$ref": "#/definitions/DataLakeProfilerConfig"\n        }\n      ]\n    },\n    "spark_driver_memory": {\n      "title": "Spark Driver Memory",\n      "description": "Max amount of memory to grant Spark.",\n      "default": "4g",\n      "type": "string"\n    },\n    "max_rows": {\n      "title": "Max Rows",\n      "description": "Maximum number of rows to use when inferring schemas for TSV and CSV files.",\n      "default": 100,\n      "type": "integer"\n    },\n    "verify_ssl": {\n      "title": "Verify Ssl",\n      "description": "Either a boolean, in which case it controls whether we verify the server\'s TLS certificate, or a string, in which case it must be a path to a CA bundle to use.",\n      "default": true,\n      "anyOf": [\n        {\n          "type": "boolean"\n        },\n        {\n          "type": "string"\n        }\n      ]\n    }\n  },\n  "required": [\n    "path_specs"\n  ],\n  "additionalProperties": false,\n  "definitions": {\n    "PathSpec": {\n      "title": "PathSpec",\n      "type": "object",\n      "properties": {\n        "include": {\n          "title": "Include",\n          "description": "Path to table (s3 or local file system). Name variable {table} is used to mark the folder with dataset. In absence of {table}, file level dataset will be created. Check below examples for more details.",\n          "type": "string"\n        },\n        "exclude": {\n          "title": "Exclude",\n          "description": "list of paths in glob pattern which will be excluded while scanning for the datasets",\n          "type": "array",\n          "items": {\n            "type": "string"\n          }\n        },\n        "file_types": {\n          "title": "File Types",\n          "description": "Files with extenstions specified here (subset of default value) only will be scanned to create dataset. Other files will be omitted.",\n          "default": [\n            "csv",\n            "tsv",\n            "json",\n            "parquet",\n            "avro"\n          ],\n          "type": "array",\n          "items": {\n            "type": "string"\n          }\n        },\n        "default_extension": {\n          "title": "Default Extension",\n          "description": "For files without extension it will assume the specified file type. If it is not set the files without extensions will be skipped.",\n          "type": "string"\n        },\n        "table_name": {\n          "title": "Table Name",\n          "description": "Display name of the dataset.Combination of named variables from include path and strings",\n          "type": "string"\n        },\n        "enable_compression": {\n          "title": "Enable Compression",\n          "description": "Enable or disable processing compressed files. Currently .gz and .bz files are supported.",\n          "default": true,\n          "type": "boolean"\n        },\n        "sample_files": {\n          "title": "Sample Files",\n          "description": "Not listing all the files but only taking a handful amount of sample file to infer the schema. File count and file size calculation will be disabled. This can affect performance significantly if enabled",\n          "default": true,\n          "type": "boolean"\n        }\n      },\n      "required": [\n        "include"\n      ],\n      "additionalProperties": false\n    },\n    "AwsAssumeRoleConfig": {\n      "title": "AwsAssumeRoleConfig",\n      "type": "object",\n      "properties": {\n        "RoleArn": {\n          "title": "Rolearn",\n          "description": "ARN of the role to assume.",\n          "type": "string"\n        },\n        "ExternalId": {\n          "title": "Externalid",\n          "description": "External ID to use when assuming the role.",\n          "type": "string"\n        }\n      },\n      "required": [\n        "RoleArn"\n      ]\n    },\n    "AwsConnectionConfig": {\n      "title": "AwsConnectionConfig",\n      "description": "Common AWS credentials config.\\n\\nCurrently used by:\\n    - Glue source\\n    - SageMaker source\\n    - dbt source",\n      "type": "object",\n      "properties": {\n        "aws_access_key_id": {\n          "title": "Aws Access Key Id",\n          "description": "AWS access key ID. Can be auto-detected, see https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html for details.",\n          "type": "string"\n        },\n        "aws_secret_access_key": {\n          "title": "Aws Secret Access Key",\n          "description": "AWS secret access key. Can be auto-detected, see https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html for details.",\n          "type": "string"\n        },\n        "aws_session_token": {\n          "title": "Aws Session Token",\n          "description": "AWS session token. Can be auto-detected, see https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html for details.",\n          "type": "string"\n        },\n        "aws_role": {\n          "title": "Aws Role",\n          "description": "AWS roles to assume. If using the string format, the role ARN can be specified directly. If using the object format, the role can be specified in the RoleArn field and additional available arguments are documented at https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sts.html?highlight=assume_role#STS.Client.assume_role",\n          "anyOf": [\n            {\n              "type": "string"\n            },\n            {\n              "type": "array",\n              "items": {\n                "anyOf": [\n                  {\n                    "type": "string"\n                  },\n                  {\n                    "$ref": "#/definitions/AwsAssumeRoleConfig"\n                  }\n                ]\n              }\n            }\n          ]\n        },\n        "aws_profile": {\n          "title": "Aws Profile",\n          "description": "Named AWS profile to use. Only used if access key / secret are unset. If not set the default will be used",\n          "type": "string"\n        },\n        "aws_region": {\n          "title": "Aws Region",\n          "description": "AWS region code.",\n          "type": "string"\n        },\n        "aws_endpoint_url": {\n          "title": "Aws Endpoint Url",\n          "description": "Autodetected. See https://boto3.amazonaws.com/v1/documentation/api/latest/reference/core/session.html",\n          "type": "string"\n        },\n        "aws_proxy": {\n          "title": "Aws Proxy",\n          "description": "Autodetected. See https://boto3.amazonaws.com/v1/documentation/api/latest/reference/core/session.html",\n          "type": "object",\n          "additionalProperties": {\n            "type": "string"\n          }\n        }\n      },\n      "required": [\n        "aws_region"\n      ],\n      "additionalProperties": false\n    },\n    "AllowDenyPattern": {\n      "title": "AllowDenyPattern",\n      "description": "A class to store allow deny regexes",\n      "type": "object",\n      "properties": {\n        "allow": {\n          "title": "Allow",\n          "description": "List of regex patterns to include in ingestion",\n          "default": [\n            ".*"\n          ],\n          "type": "array",\n          "items": {\n            "type": "string"\n          }\n        },\n        "deny": {\n          "title": "Deny",\n          "description": "List of regex patterns to exclude from ingestion.",\n          "default": [],\n          "type": "array",\n          "items": {\n            "type": "string"\n          }\n        },\n        "ignoreCase": {\n          "title": "Ignorecase",\n          "description": "Whether to ignore case sensitivity during pattern matching.",\n          "default": true,\n          "type": "boolean"\n        }\n      },\n      "additionalProperties": false\n    },\n    "DataLakeProfilerConfig": {\n      "title": "DataLakeProfilerConfig",\n      "type": "object",\n      "properties": {\n        "enabled": {\n          "title": "Enabled",\n          "description": "Whether profiling should be done.",\n          "default": false,\n          "type": "boolean"\n        },\n        "profile_table_level_only": {\n          "title": "Profile Table Level Only",\n          "description": "Whether to perform profiling at table-level only or include column-level profiling as well.",\n          "default": false,\n          "type": "boolean"\n        },\n        "max_number_of_fields_to_profile": {\n          "title": "Max Number Of Fields To Profile",\n          "description": "A positive integer that specifies the maximum number of columns to profile for any table. `None` implies all columns. The cost of profiling goes up significantly as the number of columns to profile goes up.",\n          "exclusiveMinimum": 0,\n          "type": "integer"\n        },\n        "include_field_null_count": {\n          "title": "Include Field Null Count",\n          "description": "Whether to profile for the number of nulls for each column.",\n          "default": true,\n          "type": "boolean"\n        },\n        "include_field_min_value": {\n          "title": "Include Field Min Value",\n          "description": "Whether to profile for the min value of numeric columns.",\n          "default": true,\n          "type": "boolean"\n        },\n        "include_field_max_value": {\n          "title": "Include Field Max Value",\n          "description": "Whether to profile for the max value of numeric columns.",\n          "default": true,\n          "type": "boolean"\n        },\n        "include_field_mean_value": {\n          "title": "Include Field Mean Value",\n          "description": "Whether to profile for the mean value of numeric columns.",\n          "default": true,\n          "type": "boolean"\n        },\n        "include_field_median_value": {\n          "title": "Include Field Median Value",\n          "description": "Whether to profile for the median value of numeric columns.",\n          "default": true,\n          "type": "boolean"\n        },\n        "include_field_stddev_value": {\n          "title": "Include Field Stddev Value",\n          "description": "Whether to profile for the standard deviation of numeric columns.",\n          "default": true,\n          "type": "boolean"\n        },\n        "include_field_quantiles": {\n          "title": "Include Field Quantiles",\n          "description": "Whether to profile for the quantiles of numeric columns.",\n          "default": true,\n          "type": "boolean"\n        },\n        "include_field_distinct_value_frequencies": {\n          "title": "Include Field Distinct Value Frequencies",\n          "description": "Whether to profile for distinct value frequencies.",\n          "default": true,\n          "type": "boolean"\n        },\n        "include_field_histogram": {\n          "title": "Include Field Histogram",\n          "description": "Whether to profile for the histogram for numeric fields.",\n          "default": true,\n          "type": "boolean"\n        },\n        "include_field_sample_values": {\n          "title": "Include Field Sample Values",\n          "description": "Whether to profile for the sample values for all columns.",\n          "default": true,\n          "type": "boolean"\n        }\n      },\n      "additionalProperties": false\n    }\n  }\n}\n')))),(0,l.kt)("h3",{id:"path-specs"},"Path Specs"),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"Example - Dataset per file")),(0,l.kt)("p",null,"Bucket structure:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"test-s3-bucket\n\u251c\u2500\u2500 employees.csv\n\u2514\u2500\u2500 food_items.csv\n")),(0,l.kt)("p",null,"Path specs config"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"path_specs:\n    - include: s3://test-s3-bucket/*.csv\n\n")),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"Example - Datasets with partitions")),(0,l.kt)("p",null,"Bucket structure:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"test-s3-bucket\n\u251c\u2500\u2500 orders\n\u2502\xa0\xa0 \u2514\u2500\u2500 year=2022\n\u2502\xa0\xa0     \u2514\u2500\u2500 month=2\n\u2502\xa0\xa0         \u251c\u2500\u2500 1.parquet\n\u2502\xa0\xa0         \u2514\u2500\u2500 2.parquet\n\u2514\u2500\u2500 returns\n    \u2514\u2500\u2500 year=2021\n        \u2514\u2500\u2500 month=2\n            \u2514\u2500\u2500 1.parquet\n\n")),(0,l.kt)("p",null,"Path specs config:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"path_specs:\n    - include: s3://test-s3-bucket/{table}/{partition_key[0]}={partition[0]}/{partition_key[1]}={partition[1]}/*.parquet\n")),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"Example - Datasets with partition and exclude")),(0,l.kt)("p",null,"Bucket structure:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"test-s3-bucket\n\u251c\u2500\u2500 orders\n\u2502\xa0\xa0 \u2514\u2500\u2500 year=2022\n\u2502\xa0\xa0     \u2514\u2500\u2500 month=2\n\u2502\xa0\xa0         \u251c\u2500\u2500 1.parquet\n\u2502\xa0\xa0         \u2514\u2500\u2500 2.parquet\n\u2514\u2500\u2500 tmp_orders\n    \u2514\u2500\u2500 year=2021\n        \u2514\u2500\u2500 month=2\n            \u2514\u2500\u2500 1.parquet\n\n\n")),(0,l.kt)("p",null,"Path specs config:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"path_specs:\n    - include: s3://test-s3-bucket/{table}/{partition_key[0]}={partition[0]}/{partition_key[1]}={partition[1]}/*.parquet\n      exclude: \n        - **/tmp_orders/**\n")),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"Example - Datasets of mixed nature")),(0,l.kt)("p",null,"Bucket structure:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"test-s3-bucket\n\u251c\u2500\u2500 customers\n\u2502\xa0\xa0 \u251c\u2500\u2500 part1.json\n\u2502\xa0\xa0 \u251c\u2500\u2500 part2.json\n\u2502\xa0\xa0 \u251c\u2500\u2500 part3.json\n\u2502\xa0\xa0 \u2514\u2500\u2500 part4.json\n\u251c\u2500\u2500 employees.csv\n\u251c\u2500\u2500 food_items.csv\n\u251c\u2500\u2500 tmp_10101000.csv\n\u2514\u2500\u2500  orders\n \xa0\xa0 \u2514\u2500\u2500 year=2022\n\xa0 \xa0     \u2514\u2500\u2500 month=2\n\xa0\xa0          \u251c\u2500\u2500 1.parquet\n\xa0\xa0          \u251c\u2500\u2500 2.parquet\n\xa0\xa0          \u2514\u2500\u2500 3.parquet\n\n")),(0,l.kt)("p",null,"Path specs config:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"path_specs:\n    - include: s3://test-s3-bucket/*.csv\n      exclude:\n        - **/tmp_10101000.csv\n    - include: s3://test-s3-bucket/{table}/*.json\n    - include: s3://test-s3-bucket/{table}/{partition_key[0]}={partition[0]}/{partition_key[1]}={partition[1]}/*.parquet\n")),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"Valid path_specs.include")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"s3://my-bucket/foo/tests/bar.avro # single file table   \ns3://my-bucket/foo/tests/*.* # mulitple file level tables\ns3://my-bucket/foo/tests/{table}/*.avro #table without partition\ns3://my-bucket/foo/tests/{table}/*/*.avro #table where partitions are not specified\ns3://my-bucket/foo/tests/{table}/*.* # table where no partitions as well as data type specified\ns3://my-bucket/{dept}/tests/{table}/*.avro # specifying keywords to be used in display name\ns3://my-bucket/{dept}/tests/{table}/{partition_key[0]}={partition[0]}/{partition_key[1]}={partition[1]}/*.avro # specify partition key and value format\ns3://my-bucket/{dept}/tests/{table}/{partition[0]}/{partition[1]}/{partition[2]}/*.avro # specify partition value only format\ns3://my-bucket/{dept}/tests/{table}/{partition[0]}/{partition[1]}/{partition[2]}/*.* # for all extensions\ns3://my-bucket/*/{table}/{partition[0]}/{partition[1]}/{partition[2]}/*.* # table is present at 2 levels down in bucket\ns3://my-bucket/*/*/{table}/{partition[0]}/{partition[1]}/{partition[2]}/*.* # table is present at 3 levels down in bucket\n")),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"Valid path_specs.exclude")),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"*","*/tests/**"),(0,l.kt)("li",{parentName:"ul"},"s3://my-bucket/hr/**"),(0,l.kt)("li",{parentName:"ul"},"*",(0,l.kt)("em",{parentName:"li"},"/tests/"),".csv"),(0,l.kt)("li",{parentName:"ul"},"s3://my-bucket/foo/*/my_table/**")),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"Notes")),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"{table} represents folder for which dataset will be created."),(0,l.kt)("li",{parentName:"ul"},"include path must end with (",(0,l.kt)("em",{parentName:"li"},".")," or *.","[ext]",") to represent leaf level."),(0,l.kt)("li",{parentName:"ul"},"if *.","[ext]"," is provided then only files with specified type will be scanned."),(0,l.kt)("li",{parentName:"ul"},"/*/ represents single folder."),(0,l.kt)("li",{parentName:"ul"},"{partition","[i]","} represents value of partition."),(0,l.kt)("li",{parentName:"ul"},"{partition_key","[i]","} represents name of the partition."),(0,l.kt)("li",{parentName:"ul"},"While extracting, \u201ci\u201d will be used to match partition_key to partition."),(0,l.kt)("li",{parentName:"ul"},"all folder levels need to be specified in include. Only exclude path can have ** like matching."),(0,l.kt)("li",{parentName:"ul"},"exclude path cannot have named variables ( {} )."),(0,l.kt)("li",{parentName:"ul"},"Folder names should not contain {, }, *, / in their names."),(0,l.kt)("li",{parentName:"ul"},"{folder} is reserved for internal working. please do not use in named variables.")),(0,l.kt)("p",null,"If you would like to write a more complicated function for resolving file names, then a {transformer} would be a good fit."),(0,l.kt)("admonition",{type:"caution"},(0,l.kt)("p",{parentName:"admonition"},"Specify as long fixed prefix ( with out /*/ ) as possible in ",(0,l.kt)("inlineCode",{parentName:"p"},"path_specs.include"),". This will reduce the scanning time and cost, specifically on AWS S3")),(0,l.kt)("admonition",{type:"caution"},(0,l.kt)("p",{parentName:"admonition"},"Running profiling against many tables or over many rows can run up significant costs.\nWhile we've done our best to limit the expensiveness of the queries the profiler runs, you\nshould be prudent about the set of tables profiling is enabled on or the frequency\nof the profiling runs.")),(0,l.kt)("admonition",{type:"caution"},(0,l.kt)("p",{parentName:"admonition"},"If you are ingesting datasets from AWS S3, we recommend running the ingestion on a server in the same region to avoid high egress costs.")),(0,l.kt)("h3",{id:"compatibility"},"Compatibility"),(0,l.kt)("p",null,"Profiles are computed with PyDeequ, which relies on PySpark. Therefore, for computing profiles, we currently require Spark 3.0.3 with Hadoop 3.2 to be installed and the ",(0,l.kt)("inlineCode",{parentName:"p"},"SPARK_HOME")," and ",(0,l.kt)("inlineCode",{parentName:"p"},"SPARK_VERSION")," environment variables to be set. The Spark+Hadoop binary can be downloaded ",(0,l.kt)("a",{parentName:"p",href:"https://www.apache.org/dyn/closer.lua/spark/spark-3.0.3/spark-3.0.3-bin-hadoop3.2.tgz"},"here"),"."),(0,l.kt)("p",null,"For an example guide on setting up PyDeequ on AWS, see ",(0,l.kt)("a",{parentName:"p",href:"https://aws.amazon.com/blogs/big-data/testing-data-quality-at-scale-with-pydeequ/"},"this guide"),"."),(0,l.kt)("h3",{id:"code-coordinates"},"Code Coordinates"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"Class Name: ",(0,l.kt)("inlineCode",{parentName:"li"},"datahub.ingestion.source.s3.source.S3Source")),(0,l.kt)("li",{parentName:"ul"},"Browse on ",(0,l.kt)("a",{parentName:"li",href:"https://github.com/datahub-project/datahub/blob/master/metadata-ingestion/src/datahub/ingestion/source/s3/source.py"},"GitHub"))),(0,l.kt)("h2",{id:"questions"},"Questions"),(0,l.kt)("p",null,"If you've got any questions on configuring ingestion for S3 Data Lake, feel free to ping us on ",(0,l.kt)("a",{parentName:"p",href:"https://slack.datahubproject.io"},"our Slack")))}m.isMDXComponent=!0}}]);
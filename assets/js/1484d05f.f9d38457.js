"use strict";(self.webpackChunkdocs_website=self.webpackChunkdocs_website||[]).push([[7952],{3905:(e,t,a)=>{a.d(t,{Zo:()=>d,kt:()=>u});var n=a(67294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},i=Object.keys(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var l=n.createContext({}),p=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},d=function(e){var t=p(e.components);return n.createElement(l.Provider,{value:t},e.children)},c={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},m=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,i=e.originalType,l=e.parentName,d=s(e,["components","mdxType","originalType","parentName"]),m=p(a),u=r,h=m["".concat(l,".").concat(u)]||m[u]||c[u]||i;return a?n.createElement(h,o(o({ref:t},d),{},{components:a})):n.createElement(h,o({ref:t},d))}));function u(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=a.length,o=new Array(i);o[0]=m;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:r,o[1]=s;for(var p=2;p<i;p++)o[p]=a[p];return n.createElement.apply(null,o)}return n.createElement.apply(null,a)}m.displayName="MDXCreateElement"},66085:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>c,frontMatter:()=>i,metadata:()=>s,toc:()=>p});var n=a(83117),r=(a(67294),a(3905));const i={title:"Java Emitter",slug:"/metadata-integration/java/as-a-library",custom_edit_url:"https://github.com/datahub-project/datahub/blob/master/metadata-integration/java/as-a-library.md"},o="Java Emitter",s={unversionedId:"metadata-integration/java/as-a-library",id:"metadata-integration/java/as-a-library",title:"Java Emitter",description:"In some cases, you might want to construct Metadata events directly and use programmatic ways to emit that metadata to DataHub. Use-cases are typically push-based and include emitting metadata events from CI/CD pipelines, custom orchestrators etc.",source:"@site/genDocs/metadata-integration/java/as-a-library.md",sourceDirName:"metadata-integration/java",slug:"/metadata-integration/java/as-a-library",permalink:"/datahub-project-forked/docs/metadata-integration/java/as-a-library",draft:!1,editUrl:"https://github.com/datahub-project/datahub/blob/master/metadata-integration/java/as-a-library.md",tags:[],version:"current",frontMatter:{title:"Java Emitter",slug:"/metadata-integration/java/as-a-library",custom_edit_url:"https://github.com/datahub-project/datahub/blob/master/metadata-integration/java/as-a-library.md"},sidebar:"overviewSidebar",previous:{title:"Python Emitter",permalink:"/datahub-project-forked/docs/metadata-ingestion/as-a-library"},next:{title:"Lite (Experimental)",permalink:"/datahub-project-forked/docs/datahub_lite"}},l={},p=[{value:"Installation",id:"installation",level:2},{value:"Gradle",id:"gradle",level:3},{value:"Maven",id:"maven",level:3},{value:"REST Emitter",id:"rest-emitter",level:2},{value:"Usage",id:"usage",level:3},{value:"REST Emitter Code",id:"rest-emitter-code",level:3},{value:"Kafka Emitter",id:"kafka-emitter",level:2},{value:"Usage",id:"usage-1",level:3},{value:"Kafka Emitter Code",id:"kafka-emitter-code",level:3},{value:"File Emitter",id:"file-emitter",level:2},{value:"Usage",id:"usage-2",level:3},{value:"File Emitter Code",id:"file-emitter-code",level:3},{value:"Support for S3, GCS etc.",id:"support-for-s3-gcs-etc",level:3},{value:"Other Languages",id:"other-languages",level:2}],d={toc:p};function c(e){let{components:t,...a}=e;return(0,r.kt)("wrapper",(0,n.Z)({},d,a,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"java-emitter"},"Java Emitter"),(0,r.kt)("p",null,"In some cases, you might want to construct Metadata events directly and use programmatic ways to emit that metadata to DataHub. Use-cases are typically push-based and include emitting metadata events from CI/CD pipelines, custom orchestrators etc."),(0,r.kt)("p",null,"The ",(0,r.kt)("a",{parentName:"p",href:"https://mvnrepository.com/artifact/io.acryl/datahub-client"},(0,r.kt)("inlineCode",{parentName:"a"},"io.acryl:datahub-client"))," Java package offers REST emitter API-s, which can be easily used to emit metadata from your JVM-based systems. For example, the Spark lineage integration uses the Java emitter to emit metadata events from Spark jobs."),(0,r.kt)("h2",{id:"installation"},"Installation"),(0,r.kt)("p",null,"Follow the specific instructions for your build system to declare a dependency on the appropriate version of the package. "),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("em",{parentName:"strong"},"Note")),": Check the ",(0,r.kt)("a",{parentName:"p",href:"https://mvnrepository.com/artifact/io.acryl/datahub-client"},"Maven repository")," for the latest version of the package before following the instructions below."),(0,r.kt)("h3",{id:"gradle"},"Gradle"),(0,r.kt)("p",null,"Add the following to your build.gradle."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-gradle"},"implementation 'io.acryl:datahub-client:__version__'\n")),(0,r.kt)("h3",{id:"maven"},"Maven"),(0,r.kt)("p",null,"Add the following to your ",(0,r.kt)("inlineCode",{parentName:"p"},"pom.xml"),"."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-xml"},"\x3c!-- https://mvnrepository.com/artifact/io.acryl/datahub-client --\x3e\n<dependency>\n    <groupId>io.acryl</groupId>\n    <artifactId>datahub-client</artifactId>\n    \x3c!-- replace __version__ with the latest version number --\x3e\n    <version>__version__</version>\n</dependency>\n")),(0,r.kt)("h2",{id:"rest-emitter"},"REST Emitter"),(0,r.kt)("p",null,"The REST emitter is a thin wrapper on top of the ",(0,r.kt)("a",{parentName:"p",href:"https://hc.apache.org/httpcomponents-client-4.5.x/index.html"},(0,r.kt)("inlineCode",{parentName:"a"},"Apache HttpClient"))," library. It supports non-blocking emission of metadata and handles the details of JSON serialization of metadata aspects over the wire."),(0,r.kt)("p",null,"Constructing a REST Emitter follows a lambda-based fluent builder pattern. The config parameters mirror the Python emitter ",(0,r.kt)("a",{parentName:"p",href:"/datahub-project-forked/docs/metadata-ingestion/sink_docs/datahub#config-details"},"configuration")," for the most part. In addition, you can also customize the HttpClient that is constructed under the hood by passing in customizations to the HttpClient builder."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-java"},'import datahub.client.rest.RestEmitter;\n//...\nRestEmitter emitter = RestEmitter.create(b -> b\n                                              .server("http://localhost:8080")\n//Auth token for Managed DataHub              .token(AUTH_TOKEN_IF_NEEDED)\n//Override default timeout of 10 seconds      .timeoutSec(OVERRIDE_DEFAULT_TIMEOUT_IN_SECONDS)\n//Add additional headers                      .extraHeaders(Collections.singletonMap("Session-token", "MY_SESSION"))\n// Customize HttpClient\'s connection ttl      .customizeHttpAsyncClient(c -> c.setConnectionTimeToLive(30, TimeUnit.SECONDS))\n                                    );\n')),(0,r.kt)("h3",{id:"usage"},"Usage"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-java"},'import com.linkedin.dataset.DatasetProperties;\nimport com.linkedin.events.metadata.ChangeType;\nimport datahub.event.MetadataChangeProposalWrapper;\nimport datahub.client.rest.RestEmitter;\nimport datahub.client.Callback;\n// ... followed by\n\n// Creates the emitter with the default coordinates and settings\nRestEmitter emitter = RestEmitter.createWithDefaults(); \n\nMetadataChangeProposalWrapper mcpw = MetadataChangeProposalWrapper.builder()\n        .entityType("dataset")\n        .entityUrn("urn:li:dataset:(urn:li:dataPlatform:bigquery,my-project.my-dataset.user-table,PROD)")\n        .upsert()\n        .aspect(new DatasetProperties().setDescription("This is the canonical User profile dataset"))\n        .build();\n\n// Blocking call using future\nFuture<MetadataWriteResponse> requestFuture = emitter.emit(mcpw, null).get();\n\n// Non-blocking using callback\nemitter.emit(mcpw, new Callback() {\n      @Override\n      public void onCompletion(MetadataWriteResponse response) {\n        if (response.isSuccess()) {\n          System.out.println(String.format("Successfully emitted metadata event for %s", mcpw.getEntityUrn()));\n        } else {\n          // Get the underlying http response\n          HttpResponse httpResponse = (HttpResponse) response.getUnderlyingResponse();\n          System.out.println(String.format("Failed to emit metadata event for %s, aspect: %s with status code: %d",\n              mcpw.getEntityUrn(), mcpw.getAspectName(), httpResponse.getStatusLine().getStatusCode()));\n          // Print the server side exception if it was captured\n          if (response.getServerException() != null) {\n            System.out.println(String.format("Server side exception was %s", response.getServerException()));\n          }\n        }\n      }\n\n      @Override\n      public void onFailure(Throwable exception) {\n        System.out.println(\n            String.format("Failed to emit metadata event for %s, aspect: %s due to %s", mcpw.getEntityUrn(),\n                mcpw.getAspectName(), exception.getMessage()));\n      }\n    });\n')),(0,r.kt)("h3",{id:"rest-emitter-code"},"REST Emitter Code"),(0,r.kt)("p",null,"If you're interested in looking at the REST emitter code, it is available ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/datahub-project/datahub/blob/master/metadata-integration/java/datahub-client/src/main/java/datahub/client/rest/RestEmitter.java"},"here"),"."),(0,r.kt)("h2",{id:"kafka-emitter"},"Kafka Emitter"),(0,r.kt)("p",null,"The Kafka emitter is a thin wrapper on top of the SerializingProducer class from ",(0,r.kt)("inlineCode",{parentName:"p"},"confluent-kafka")," and offers a non-blocking interface for sending metadata events to DataHub. Use this when you want to decouple your metadata producer from the uptime of your datahub metadata server by utilizing Kafka as a highly available message bus. For example, if your DataHub metadata service is down due to planned or unplanned outages, you can still continue to collect metadata from your mission critical systems by sending it to Kafka. Also use this emitter when throughput of metadata emission is more important than acknowledgement of metadata being persisted to DataHub's backend store."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("em",{parentName:"strong"},"Note")),": The Kafka emitter uses Avro to serialize the Metadata events to Kafka. Changing the serializer will result in unprocessable events as DataHub currently expects the metadata events over Kafka to be serialized in Avro."),(0,r.kt)("h3",{id:"usage-1"},"Usage"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-java"},'\n\nimport java.io.IOException;\nimport java.util.concurrent.ExecutionException;\nimport com.linkedin.dataset.DatasetProperties;\nimport datahub.client.kafka.KafkaEmitter;\nimport datahub.client.kafka.KafkaEmitterConfig;\nimport datahub.event.MetadataChangeProposalWrapper;\n\n// ... followed by\n\n// Creates the emitter with the default coordinates and settings\nKafkaEmitterConfig.KafkaEmitterConfigBuilder builder = KafkaEmitterConfig.builder(); KafkaEmitterConfig config = builder.build();\nKafkaEmitter emitter = new KafkaEmitter(config);\n \n//Test if topic is available\n\nif(emitter.testConnection()){\n \n    MetadataChangeProposalWrapper mcpw = MetadataChangeProposalWrapper.builder()\n            .entityType("dataset")\n            .entityUrn("urn:li:dataset:(urn:li:dataPlatform:bigquery,my-project.my-dataset.user-table,PROD)")\n            .upsert()\n            .aspect(new DatasetProperties().setDescription("This is the canonical User profile dataset"))\n            .build();\n    \n    // Blocking call using future\n    Future<MetadataWriteResponse> requestFuture = emitter.emit(mcpw, null).get();\n    \n    // Non-blocking using callback\n    emitter.emit(mcpw, new Callback() {\n    \n          @Override\n          public void onFailure(Throwable exception) {\n            System.out.println("Failed to send with: " + exception);\n          }\n          @Override\n          public void onCompletion(MetadataWriteResponse metadataWriteResponse) {\n            if (metadataWriteResponse.isSuccess()) {\n              RecordMetadata metadata = (RecordMetadata) metadataWriteResponse.getUnderlyingResponse();\n              System.out.println("Sent successfully over topic: " + metadata.topic());\n            } else {\n              System.out.println("Failed to send with: " + metadataWriteResponse.getUnderlyingResponse());\n            }\n          }\n        });\n\n}\nelse {\n    System.out.println("Kafka service is down.");\n}\n')),(0,r.kt)("h3",{id:"kafka-emitter-code"},"Kafka Emitter Code"),(0,r.kt)("p",null,"If you're interested in looking at the Kafka emitter code, it is available ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/datahub-project/datahub/blob/master/metadata-integration/java/datahub-client/src/main/java/datahub/client/kafka/KafkaEmitter.java"},"here"),"."),(0,r.kt)("h2",{id:"file-emitter"},"File Emitter"),(0,r.kt)("p",null,"The File emitter writes metadata change proposal events (MCPs) into a JSON file that can be later handed off to the Python ",(0,r.kt)("a",{parentName:"p",href:"/datahub-project-forked/docs/generated/ingestion/sources/file"},"File source")," for ingestion. This works analogous to the ",(0,r.kt)("a",{parentName:"p",href:"/datahub-project-forked/docs/metadata-ingestion/sink_docs/file"},"File sink")," in Python. This mechanism can be used when the system producing metadata events doesn't have direct connection to DataHub's REST server or Kafka brokers. The generated JSON file can be transferred later and then ingested into DataHub using the ",(0,r.kt)("a",{parentName:"p",href:"/datahub-project-forked/docs/generated/ingestion/sources/file"},"File source"),"."),(0,r.kt)("h3",{id:"usage-2"},"Usage"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-java"},'\n\nimport datahub.client.file.FileEmitter;\nimport datahub.client.file.FileEmitterConfig;\nimport datahub.event.MetadataChangeProposalWrapper;\n\n// ... followed by\n\n\n// Define output file co-ordinates\nString outputFile = "/my/path/output.json";\n\n//Create File Emitter\nFileEmitter emitter = new FileEmitter(FileEmitterConfig.builder().fileName(outputFile).build());\n\n// A couple of sample metadata events\nMetadataChangeProposalWrapper mcpwOne = MetadataChangeProposalWrapper.builder()\n        .entityType("dataset")\n        .entityUrn("urn:li:dataset:(urn:li:dataPlatform:bigquery,my-project.my-dataset.user-table,PROD)")\n        .upsert()\n        .aspect(new DatasetProperties().setDescription("This is the canonical User profile dataset"))\n        .build();\n\nMetadataChangeProposalWrapper mcpwTwo = MetadataChangeProposalWrapper.builder()\n        .entityType("dataset")\n        .entityUrn("urn:li:dataset:(urn:li:dataPlatform:bigquery,my-project.my-dataset.fact-orders-table,PROD)")\n        .upsert()\n        .aspect(new DatasetProperties().setDescription("This is the canonical Fact table for orders"))\n        .build();\n\nMetadataChangeProposalWrapper[] mcpws = { mcpwOne, mcpwTwo };\nfor (MetadataChangeProposalWrapper mcpw : mcpws) {\n   emitter.emit(mcpw);\n}\nemitter.close(); // calling close() is important to ensure file gets closed cleanly\n    \n')),(0,r.kt)("h3",{id:"file-emitter-code"},"File Emitter Code"),(0,r.kt)("p",null,"If you're interested in looking at the File emitter code, it is available ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/datahub-project/datahub/blob/master/metadata-integration/java/datahub-client/src/main/java/datahub/client/file/FileEmitter.java"},"here"),"."),(0,r.kt)("h3",{id:"support-for-s3-gcs-etc"},"Support for S3, GCS etc."),(0,r.kt)("p",null,"The File emitter only supports writing to the local filesystem currently. If you're interested in adding support for S3, GCS etc., contributions are welcome! "),(0,r.kt)("h2",{id:"other-languages"},"Other Languages"),(0,r.kt)("p",null,"Emitter API-s are also supported for:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"/datahub-project-forked/docs/metadata-ingestion/as-a-library"},"Python"))))}c.isMDXComponent=!0}}]);
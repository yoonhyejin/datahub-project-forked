"use strict";(self.webpackChunkdocs_website=self.webpackChunkdocs_website||[]).push([[336],{3905:(e,t,a)=>{a.d(t,{Zo:()=>p,kt:()=>m});var n=a(67294);function o(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function r(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){o(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,n,o=function(e,t){if(null==e)return{};var a,n,o={},i=Object.keys(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||(o[a]=e[a]);return o}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(o[a]=e[a])}return o}var l=n.createContext({}),c=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):r(r({},t),e)),a},p=function(e){var t=c(e.components);return n.createElement(l.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},d=n.forwardRef((function(e,t){var a=e.components,o=e.mdxType,i=e.originalType,l=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),d=c(a),m=o,h=d["".concat(l,".").concat(m)]||d[m]||u[m]||i;return a?n.createElement(h,r(r({ref:t},p),{},{components:a})):n.createElement(h,r({ref:t},p))}));function m(e,t){var a=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var i=a.length,r=new Array(i);r[0]=d;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:o,r[1]=s;for(var c=2;c<i;c++)r[c]=a[c];return n.createElement.apply(null,r)}return n.createElement.apply(null,a)}d.displayName="MDXCreateElement"},89419:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>r,default:()=>u,frontMatter:()=>i,metadata:()=>s,toc:()=>c});var n=a(83117),o=(a(67294),a(3905));const i={title:"Ingestion",slug:"/managed-datahub/metadata-ingestion-with-acryl/ingestion",custom_edit_url:"https://github.com/datahub-project/datahub/blob/master/docs/managed-datahub/metadata-ingestion-with-acryl/ingestion.md"},r="Ingestion",s={unversionedId:"docs/managed-datahub/metadata-ingestion-with-acryl/ingestion",id:"docs/managed-datahub/metadata-ingestion-with-acryl/ingestion",title:"Ingestion",description:"Acryl Metadata Ingestion functions similarly to that in open source DataHub. Sources are configured via the UI Ingestion or via a Recipe, ingestion recipes can be scheduled using your system of choice, and metadata can be pushed from anywhere.",source:"@site/genDocs/docs/managed-datahub/metadata-ingestion-with-acryl/ingestion.md",sourceDirName:"docs/managed-datahub/metadata-ingestion-with-acryl",slug:"/managed-datahub/metadata-ingestion-with-acryl/ingestion",permalink:"/datahub-project-forked/docs/managed-datahub/metadata-ingestion-with-acryl/ingestion",draft:!1,editUrl:"https://github.com/datahub-project/datahub/blob/master/docs/managed-datahub/metadata-ingestion-with-acryl/ingestion.md",tags:[],version:"current",frontMatter:{title:"Ingestion",slug:"/managed-datahub/metadata-ingestion-with-acryl/ingestion",custom_edit_url:"https://github.com/datahub-project/datahub/blob/master/docs/managed-datahub/metadata-ingestion-with-acryl/ingestion.md"},sidebar:"overviewSidebar",previous:{title:"Getting Started with Acryl DataHub",permalink:"/datahub-project-forked/docs/managed-datahub/welcome-acryl"},next:{title:"Entity Events API",permalink:"/datahub-project-forked/docs/managed-datahub/datahub-api/entity-events-api"}},l={},c=[{value:"Batch Ingestion",id:"batch-ingestion",level:2},{value:"Step 1: Install DataHub CLI",id:"step-1-install-datahub-cli",level:3},{value:"<strong>Install from Gemfury Private Repository</strong>",id:"install-from-gemfury-private-repository",level:4},{value:"Install from PyPI for OSS Release",id:"install-from-pypi-for-oss-release",level:4},{value:"Step 2: Install Connector Plugins",id:"step-2-install-connector-plugins",level:3},{value:"Step 3: Write a Recipe",id:"step-3-write-a-recipe",level:3},{value:"Step 4: Running Ingestion",id:"step-4-running-ingestion",level:3},{value:"Step 5: Scheduling Ingestion",id:"step-5-scheduling-ingestion",level:3}],p={toc:c};function u(e){let{components:t,...i}=e;return(0,o.kt)("wrapper",(0,n.Z)({},p,i,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"ingestion"},"Ingestion"),(0,o.kt)("p",null,"Acryl Metadata Ingestion functions similarly to that in open source DataHub. Sources are configured via the",(0,o.kt)("a",{parentName:"p",href:"/datahub-project-forked/docs/ui-ingestion"}," UI Ingestion")," or via a ",(0,o.kt)("a",{parentName:"p",href:"/datahub-project-forked/docs/metadata-ingestion#recipes"},"Recipe"),", ingestion recipes can be scheduled using your system of choice, and metadata can be pushed from anywhere.\nThis document will describe the steps required to ingest metadata from your data sources."),(0,o.kt)("h2",{id:"batch-ingestion"},"Batch Ingestion"),(0,o.kt)("p",null,"Batch ingestion involves extracting metadata from a source system in bulk. Typically, this happens on a predefined schedule using the ",(0,o.kt)("a",{parentName:"p",href:"/datahub-project-forked/docs/metadata-ingestion#install-from-pypi"},"Metadata Ingestion "),"framework.\nThe metadata that is extracted includes point-in-time instances of dataset, chart, dashboard, pipeline, user, group, usage, and task metadata."),(0,o.kt)("h3",{id:"step-1-install-datahub-cli"},"Step 1: Install DataHub CLI"),(0,o.kt)("p",null,"Regardless of how you ingest metadata, you'll need your account subdomain and API key handy."),(0,o.kt)("h4",{id:"install-from-gemfury-private-repository"},(0,o.kt)("strong",{parentName:"h4"},"Install from Gemfury Private Repository")),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Installing from command line with pip")),(0,o.kt)("p",null,"Determine the version you would like to install and obtain a read access token by requesting a one-time-secret from the Acryl team then run the following command:"),(0,o.kt)("p",null,(0,o.kt)("inlineCode",{parentName:"p"},"python3 -m pip install acryl-datahub==<VERSION> --index-url https://<TOKEN>:@pypi.fury.io/acryl-data/")),(0,o.kt)("h4",{id:"install-from-pypi-for-oss-release"},"Install from PyPI for OSS Release"),(0,o.kt)("p",null,"Run the following commands in your terminal:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"python3 -m pip install --upgrade pip wheel setuptools\npython3 -m pip install --upgrade acryl-datahub\npython3 -m datahub version\n")),(0,o.kt)("p",null,(0,o.kt)("em",{parentName:"p"},"Note: Requires Python 3.6+")),(0,o.kt)("p",null,"Your command line should return the proper version of DataHub upon executing these commands successfully."),(0,o.kt)("h3",{id:"step-2-install-connector-plugins"},"Step 2: Install Connector Plugins"),(0,o.kt)("p",null,"Our CLI follows a plugin architecture. You must install connectors for different data sources individually. For a list of all supported data sources, see ",(0,o.kt)("a",{parentName:"p",href:"/datahub-project-forked/docs/metadata-ingestion#installing-plugins"},"the open source docs"),".\nOnce you've found the connectors you care about, simply install them using ",(0,o.kt)("inlineCode",{parentName:"p"},"pip install"),".\nFor example, to install the ",(0,o.kt)("inlineCode",{parentName:"p"},"mysql")," connector, you can run"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"pip install --upgrade acryl-datahub[mysql]\n")),(0,o.kt)("h3",{id:"step-3-write-a-recipe"},"Step 3: Write a Recipe"),(0,o.kt)("p",null,(0,o.kt)("a",{parentName:"p",href:"/datahub-project-forked/docs/metadata-ingestion#recipes"},"Recipes")," are yaml configuration files that serve as input to the Metadata Ingestion framework. Each recipe file define a single source to read from and a single destination to push the metadata.\nThe two most important pieces of the file are the ",(0,o.kt)("em",{parentName:"p"},"source")," and ","_","sin",(0,o.kt)("em",{parentName:"p"},"k configuration blocks.\nThe ","_","source")," configuration block defines where to extract metadata from. This can be an OLTP database system, a data warehouse, or something as simple as a file. Each source has custom configuration depending on what is required to access metadata from the source. To see configurations required for each supported source, refer to the ",(0,o.kt)("a",{parentName:"p",href:"/datahub-project-forked/docs/metadata-ingestion#sources"},"Sources")," documentation.\nThe ",(0,o.kt)("em",{parentName:"p"},"sink")," configuration block defines where to push metadata into. Each sink type requires specific configurations, the details of which are detailed in the ",(0,o.kt)("a",{parentName:"p",href:"/datahub-project-forked/docs/metadata-ingestion#sinks"},"Sinks")," documentation.\nIn Acryl DataHub deployments, you ",(0,o.kt)("em",{parentName:"p"},"must")," use a sink of type ",(0,o.kt)("inlineCode",{parentName:"p"},"datahub-rest"),", which simply means that metadata will be pushed to the REST endpoints exposed by your DataHub instance. The required configurations for this sink are"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("strong",{parentName:"li"},"server"),": the location of the REST API exposed by your instance of DataHub"),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("strong",{parentName:"li"},"token"),": a unique API key used to authenticate requests to your instance's REST API")),(0,o.kt)("p",null,"The token can be retrieved by logging in as admin. You can go to Settings page and generate a Personal Access Token with your desired expiration date.\n",(0,o.kt)("img",{src:a(84156).Z,width:"2348",height:"627"})),(0,o.kt)("p",null,(0,o.kt)("img",{src:a(91549).Z,width:"2366",height:"752"})),(0,o.kt)("p",null,'To configure your instance of DataHub as the destination for ingestion, set the "server" field of your recipe to point to your Acryl instance\'s domain suffixed by the path ',(0,o.kt)("inlineCode",{parentName:"p"},"/gms"),", as shown below.\nA complete example of a DataHub recipe file, which reads from MySQL and writes into a DataHub instance:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},'# example-recipe.yml\n\n# MySQL source configuration\nsource:\n  type: mysql\n  config:\n    username: root\n    password: password\n    host_port: localhost:3306\n\n# Recipe sink configuration.\nsink:\n  type: "datahub-rest"\n  config:\n    server: "https://<your domain name>.acryl.io/gms"\n    token: <Your API key>\n')),(0,o.kt)("admonition",{type:"info"},(0,o.kt)("p",{parentName:"admonition"},"Your API key is a signed JSON Web Token that is valid for 6 months from the date of issuance. Please keep this key secure & avoid sharing it.")),(0,o.kt)("p",null,"If your key is compromised for any reason, please reach out to the Acryl team at ",(0,o.kt)("a",{parentName:"p",href:"mailto:support@acryl.io."},"support@acryl.io."),":::"),(0,o.kt)("h3",{id:"step-4-running-ingestion"},"Step 4: Running Ingestion"),(0,o.kt)("p",null,"The final step requires invoking the DataHub CLI to ingest metadata based on your recipe configuration file.\nTo do so, simply run ",(0,o.kt)("inlineCode",{parentName:"p"},"datahub ingest")," with a pointer to your YAML recipe file:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"datahub ingest -c ./example-recipe.yml\n")),(0,o.kt)("h3",{id:"step-5-scheduling-ingestion"},"Step 5: Scheduling Ingestion"),(0,o.kt)("p",null,"Ingestion can either be run in an ad-hoc manner by a system administrator or scheduled for repeated executions. Most commonly, ingestion will be run on a daily cadence.\nTo schedule your ingestion job, we recommend using a job schedule like ",(0,o.kt)("a",{parentName:"p",href:"https://airflow.apache.org/"},"Apache Airflow"),". In cases of simpler deployments, a CRON job scheduled on an always-up machine can also work.\nNote that each source system will require a separate recipe file. This allows you to schedule ingestion from different sources independently or together."),(0,o.kt)("p",null,(0,o.kt)("em",{parentName:"p"},"Looking for information on real-time ingestion? Click")," ",(0,o.kt)("a",{parentName:"p",href:"/datahub-project-forked/docs/lineage/airflow"},(0,o.kt)("em",{parentName:"a"},"here")),(0,o.kt)("em",{parentName:"p"},".")),(0,o.kt)("p",null,(0,o.kt)("em",{parentName:"p"},"Note: Real-time ingestion setup is not recommended for an initial POC as it generally takes longer to configure and is prone to inevitable system errors.")))}u.isMDXComponent=!0},84156:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/home-(1)-7cd3a324a88f811cb9cce7bd061e5c4d.png"},91549:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/settings-e8cbefc2cda184c6276ef745938570f8.png"}}]);
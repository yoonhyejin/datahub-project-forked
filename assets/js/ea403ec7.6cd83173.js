"use strict";(self.webpackChunkdocs_website=self.webpackChunkdocs_website||[]).push([[8387],{3905:(e,t,a)=>{a.d(t,{Zo:()=>u,kt:()=>c});var n=a(67294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function l(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function o(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},i=Object.keys(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var d=n.createContext({}),s=function(e){var t=n.useContext(d),a=t;return e&&(a="function"==typeof e?e(t):l(l({},t),e)),a},u=function(e){var t=s(e.components);return n.createElement(d.Provider,{value:t},e.children)},p={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},m=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,i=e.originalType,d=e.parentName,u=o(e,["components","mdxType","originalType","parentName"]),m=s(a),c=r,h=m["".concat(d,".").concat(c)]||m[c]||p[c]||i;return a?n.createElement(h,l(l({ref:t},u),{},{components:a})):n.createElement(h,l({ref:t},u))}));function c(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=a.length,l=new Array(i);l[0]=m;var o={};for(var d in t)hasOwnProperty.call(t,d)&&(o[d]=t[d]);o.originalType=e,o.mdxType="string"==typeof e?e:r,l[1]=o;for(var s=2;s<i;s++)l[s]=a[s];return n.createElement.apply(null,l)}return n.createElement.apply(null,a)}m.displayName="MDXCreateElement"},22212:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>d,contentTitle:()=>l,default:()=>p,frontMatter:()=>i,metadata:()=>o,toc:()=>s});var n=a(83117),r=(a(67294),a(3905));const i={title:"DataHub CLI",sidebar_label:"CLI",slug:"/cli",custom_edit_url:"https://github.com/datahub-project/datahub/blob/master/docs/cli.md"},l="DataHub CLI",o={unversionedId:"docs/cli",id:"docs/cli",title:"DataHub CLI",description:"DataHub comes with a friendly cli called datahub that allows you to perform a lot of common operations using just the command line. Acryl Data maintains the pypi package for datahub.",source:"@site/genDocs/docs/cli.md",sourceDirName:"docs",slug:"/cli",permalink:"/datahub-project-forked/docs/cli",draft:!1,editUrl:"https://github.com/datahub-project/datahub/blob/master/docs/cli.md",tags:[],version:"current",frontMatter:{title:"DataHub CLI",sidebar_label:"CLI",slug:"/cli",custom_edit_url:"https://github.com/datahub-project/datahub/blob/master/docs/cli.md"},sidebar:"overviewSidebar",previous:{title:"Aspect Versioning",permalink:"/datahub-project-forked/docs/advanced/aspect-versioning"},next:{title:"Python Emitter",permalink:"/datahub-project-forked/docs/metadata-ingestion/as-a-library"}},d={},s=[{value:"Installation",id:"installation",level:2},{value:"Using pip",id:"using-pip",level:3},{value:"User Guide",id:"user-guide",level:2},{value:"docker",id:"docker",level:3},{value:"ingest",id:"ingest",level:3},{value:"init",id:"init",level:3},{value:"Environment variables supported",id:"environment-variables-supported",level:3},{value:"check",id:"check",level:3},{value:"lite (experimental)",id:"lite-experimental",level:3},{value:"delete",id:"delete",level:3},{value:"get",id:"get",level:3},{value:"put",id:"put",level:3},{value:"put aspect",id:"put-aspect",level:4},{value:"put platform",id:"put-platform",level:4},{value:"timeline",id:"timeline",level:3},{value:"telemetry",id:"telemetry",level:3},{value:"migrate",id:"migrate",level:3},{value:"dataplatform2instance",id:"dataplatform2instance",level:4},{value:"Dry Run",id:"dry-run",level:5},{value:"Real Migration (with soft-delete)",id:"real-migration-with-soft-delete",level:5},{value:"Alternate Installation Options",id:"alternate-installation-options",level:2},{value:"Using docker",id:"using-docker",level:3},{value:"Install from source",id:"install-from-source",level:3},{value:"Installing Plugins",id:"installing-plugins",level:2},{value:"Sources",id:"sources",level:3},{value:"Sinks",id:"sinks",level:3},{value:"Check the active plugins",id:"check-the-active-plugins",level:3},{value:"Release Notes and CLI versions",id:"release-notes-and-cli-versions",level:2}],u={toc:s};function p(e){let{components:t,...a}=e;return(0,r.kt)("wrapper",(0,n.Z)({},u,a,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"datahub-cli"},"DataHub CLI"),(0,r.kt)("p",null,"DataHub comes with a friendly cli called ",(0,r.kt)("inlineCode",{parentName:"p"},"datahub")," that allows you to perform a lot of common operations using just the command line. ",(0,r.kt)("a",{parentName:"p",href:"https://acryldata.io"},"Acryl Data")," maintains the ",(0,r.kt)("a",{parentName:"p",href:"https://pypi.org/project/acryl-datahub/"},"pypi package")," for ",(0,r.kt)("inlineCode",{parentName:"p"},"datahub"),"."),(0,r.kt)("h2",{id:"installation"},"Installation"),(0,r.kt)("h3",{id:"using-pip"},"Using pip"),(0,r.kt)("p",null,"We recommend Python virtual environments (venv-s) to namespace pip modules. Here's an example setup:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},"python3 -m venv venv             # create the environment\nsource venv/bin/activate         # activate the environment\n")),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("em",{parentName:"strong"},"NOTE:"))," If you install ",(0,r.kt)("inlineCode",{parentName:"p"},"datahub")," in a virtual environment, that same virtual environment must be re-activated each time a shell window or session is created."),(0,r.kt)("p",null,"Once inside the virtual environment, install ",(0,r.kt)("inlineCode",{parentName:"p"},"datahub")," using the following commands"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},'# Requires Python 3.7+\npython3 -m pip install --upgrade pip wheel setuptools\npython3 -m pip install --upgrade acryl-datahub\n# validate that the install was successful\ndatahub version\n# If you see "command not found", try running this instead: python3 -m datahub version\n')),(0,r.kt)("p",null,"If you run into an error, try checking the ",(0,r.kt)("a",{parentName:"p",href:"/datahub-project-forked/docs/metadata-ingestion/developing#Common-setup-issues"},(0,r.kt)("em",{parentName:"a"},"common setup issues")),"."),(0,r.kt)("p",null,"Other installation options such as installation from source and running the cli inside a container are available further below in the guide ",(0,r.kt)("a",{parentName:"p",href:"#alternate-installation-options"},"here")),(0,r.kt)("h2",{id:"user-guide"},"User Guide"),(0,r.kt)("p",null,"The ",(0,r.kt)("inlineCode",{parentName:"p"},"datahub")," cli allows you to do many things, such as quickstarting a DataHub docker instance locally, ingesting metadata from your sources into a DataHub server or a DataHub lite instance, as well as retrieving, modifying and exploring metadata.\nLike most command line tools, ",(0,r.kt)("inlineCode",{parentName:"p"},"--help")," is your best friend. Use it to discover the capabilities of the cli and the different commands and sub-commands that are supported."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-console"},"Usage: datahub [OPTIONS] COMMAND [ARGS]...\n\nOptions:\n  --debug / --no-debug\n  --version             Show the version and exit.\n  --help                Show this message and exit.\n\nCommands:\n  check      Helper commands for checking various aspects of DataHub.\n  delete     Delete metadata from datahub using a single urn or a combination of filters\n  docker     Helper commands for setting up and interacting with a local DataHub instance using Docker.\n  get        Get metadata for an entity with an optional list of aspects to project.\n  ingest     Ingest metadata into DataHub.\n  init       Configure which datahub instance to connect to\n  put        Update a single aspect of an entity\n  telemetry  Toggle telemetry.\n  version    Print version number and exit.\n")),(0,r.kt)("p",null,"The following top-level commands listed below are here mainly to give the reader a high-level picture of what are the kinds of things you can accomplish with the cli.\nWe've ordered them roughly in the order we expect you to interact with these commands as you get deeper into the ",(0,r.kt)("inlineCode",{parentName:"p"},"datahub"),"-verse."),(0,r.kt)("h3",{id:"docker"},"docker"),(0,r.kt)("p",null,"The ",(0,r.kt)("inlineCode",{parentName:"p"},"docker")," command allows you to start up a local DataHub instance using ",(0,r.kt)("inlineCode",{parentName:"p"},"datahub docker quickstart"),". You can also check if the docker cluster is healthy using ",(0,r.kt)("inlineCode",{parentName:"p"},"datahub docker check"),"."),(0,r.kt)("h3",{id:"ingest"},"ingest"),(0,r.kt)("p",null,"The ",(0,r.kt)("inlineCode",{parentName:"p"},"ingest")," command allows you to ingest metadata from your sources using ingestion configuration files, which we call recipes.\nSource specific crawlers are provided by plugins and might sometimes need additional extras to be installed. See ",(0,r.kt)("a",{parentName:"p",href:"#installing-plugins"},"installing plugins")," for more information.\n",(0,r.kt)("a",{parentName:"p",href:"/datahub-project-forked/docs/how/delete-metadata"},"Removing Metadata from DataHub")," contains detailed instructions about how you can use the ingest command to perform operations like rolling-back previously ingested metadata through the ",(0,r.kt)("inlineCode",{parentName:"p"},"rollback")," sub-command and listing all runs that happened through ",(0,r.kt)("inlineCode",{parentName:"p"},"list-runs")," sub-command."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-console"},"Usage: datahub [datahub-options] ingest [command-options]\n\nCommand Options:\n  -c / --config        Config file in .toml or .yaml format\n  -n / --dry-run       Perform a dry run of the ingestion, essentially skipping writing to sink\n  --preview            Perform limited ingestion from the source to the sink to get a quick preview\n  --preview-workunits  The number of workunits to produce for preview\n  --strict-warnings    If enabled, ingestion runs with warnings will yield a non-zero error code\n")),(0,r.kt)("h3",{id:"init"},"init"),(0,r.kt)("p",null,"The init command is used to tell ",(0,r.kt)("inlineCode",{parentName:"p"},"datahub")," about where your DataHub instance is located. The CLI will point to localhost DataHub by default.\nRunning ",(0,r.kt)("inlineCode",{parentName:"p"},"datahub init")," will allow you to customize the datahub instance you are communicating with."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("em",{parentName:"strong"},"Note")),": Provide your GMS instance's host when the prompt asks you for the DataHub host."),(0,r.kt)("h3",{id:"environment-variables-supported"},"Environment variables supported"),(0,r.kt)("p",null,"The environment variables listed below take precedence over the DataHub CLI config created through the ",(0,r.kt)("inlineCode",{parentName:"p"},"init")," command."),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"DATAHUB_SKIP_CONFIG")," (default ",(0,r.kt)("inlineCode",{parentName:"li"},"false"),") - Set to ",(0,r.kt)("inlineCode",{parentName:"li"},"true")," to skip creating the configuration file."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"DATAHUB_GMS_URL")," (default ",(0,r.kt)("inlineCode",{parentName:"li"},"http://localhost:8080"),") - Set to a URL of GMS instance"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"DATAHUB_GMS_HOST")," (default ",(0,r.kt)("inlineCode",{parentName:"li"},"localhost"),") - Set to a host of GMS instance. Prefer using ",(0,r.kt)("inlineCode",{parentName:"li"},"DATAHUB_GMS_URL")," to set the URL."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"DATAHUB_GMS_PORT")," (default ",(0,r.kt)("inlineCode",{parentName:"li"},"8080"),") - Set to a port of GMS instance. Prefer using ",(0,r.kt)("inlineCode",{parentName:"li"},"DATAHUB_GMS_URL")," to set the URL."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"DATAHUB_GMS_PROTOCOL")," (default ",(0,r.kt)("inlineCode",{parentName:"li"},"http"),") - Set to a protocol like ",(0,r.kt)("inlineCode",{parentName:"li"},"http")," or ",(0,r.kt)("inlineCode",{parentName:"li"},"https"),". Prefer using ",(0,r.kt)("inlineCode",{parentName:"li"},"DATAHUB_GMS_URL")," to set the URL."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"DATAHUB_GMS_TOKEN")," (default ",(0,r.kt)("inlineCode",{parentName:"li"},"None"),") - Used for communicating with DataHub Cloud."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"DATAHUB_TELEMETRY_ENABLED")," (default ",(0,r.kt)("inlineCode",{parentName:"li"},"true"),") - Set to ",(0,r.kt)("inlineCode",{parentName:"li"},"false")," to disable telemetry. If CLI is being run in an environment with no access to public internet then this should be disabled."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"DATAHUB_TELEMETRY_TIMEOUT")," (default ",(0,r.kt)("inlineCode",{parentName:"li"},"10"),") - Set to a custom integer value to specify timeout in secs when sending telemetry."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"DATAHUB_DEBUG")," (default ",(0,r.kt)("inlineCode",{parentName:"li"},"false"),") - Set to ",(0,r.kt)("inlineCode",{parentName:"li"},"true")," to enable debug logging for CLI. Can also be achieved through ",(0,r.kt)("inlineCode",{parentName:"li"},"--debug")," option of the CLI."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"DATAHUB_VERSION")," (default ",(0,r.kt)("inlineCode",{parentName:"li"},"head"),") - Set to a specific version to run quickstart with the particular version of docker images."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"ACTIONS_VERSION")," (default ",(0,r.kt)("inlineCode",{parentName:"li"},"head"),") - Set to a specific version to run quickstart with that image tag of ",(0,r.kt)("inlineCode",{parentName:"li"},"datahub-actions")," container.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},"DATAHUB_SKIP_CONFIG=false\nDATAHUB_GMS_URL=http://localhost:8080\nDATAHUB_GMS_TOKEN=\nDATAHUB_TELEMETRY_ENABLED=true\nDATAHUB_TELEMETRY_TIMEOUT=10\nDATAHUB_DEBUG=false\n")),(0,r.kt)("h3",{id:"check"},"check"),(0,r.kt)("p",null,"The datahub package is composed of different plugins that allow you to connect to different metadata sources and ingest metadata from them.\nThe ",(0,r.kt)("inlineCode",{parentName:"p"},"check")," command allows you to check if all plugins are loaded correctly as well as validate an individual MCE-file."),(0,r.kt)("h3",{id:"lite-experimental"},"lite (experimental)"),(0,r.kt)("p",null,"The lite group of commands allow you to run an embedded, lightweight DataHub instance for command line exploration of your metadata. This is intended more for developer tool oriented usage rather than as a production server instance for DataHub. See ",(0,r.kt)("a",{parentName:"p",href:"/datahub-project-forked/docs/datahub_lite"},"DataHub Lite")," for more information about how you can ingest metadata into DataHub Lite and explore your metadata easily."),(0,r.kt)("h3",{id:"delete"},"delete"),(0,r.kt)("p",null,"The ",(0,r.kt)("inlineCode",{parentName:"p"},"delete")," command allows you to delete metadata from DataHub. Read this ",(0,r.kt)("a",{parentName:"p",href:"/datahub-project-forked/docs/how/delete-metadata"},"guide")," to understand how you can delete metadata from DataHub."),(0,r.kt)("admonition",{type:"info"},(0,r.kt)("p",{parentName:"admonition"},"Deleting metadata using DataHub's CLI and GraphQL API is a simple, systems-level action. If you attempt to delete an Entity with children, such as a Container, it will not automatically delete the children, you will instead need to delete each child by URN in addition to deleting the parent.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-console"},'datahub delete --urn "urn:li:dataset:(urn:li:dataPlatform:hive,SampleHiveDataset,PROD)" --soft\n')),(0,r.kt)("h3",{id:"get"},"get"),(0,r.kt)("p",null,"The ",(0,r.kt)("inlineCode",{parentName:"p"},"get")," command allows you to easily retrieve metadata from DataHub, by using the REST API. This works for both versioned aspects and timeseries aspects. For timeseries aspects, it fetches the latest value.\nFor example the following command gets the ownership aspect from the dataset ",(0,r.kt)("inlineCode",{parentName:"p"},"urn:li:dataset:(urn:li:dataPlatform:hive,SampleHiveDataset,PROD)")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell-session"},'$ datahub get --urn "urn:li:dataset:(urn:li:dataPlatform:hive,SampleHiveDataset,PROD)" --aspect ownership\n{\n  "value": {\n    "com.linkedin.metadata.snapshot.DatasetSnapshot": {\n      "aspects": [\n        {\n          "com.linkedin.metadata.key.DatasetKey": {\n            "name": "SampleHiveDataset",\n            "origin": "PROD",\n            "platform": "urn:li:dataPlatform:hive"\n          }\n        },\n        {\n          "com.linkedin.common.Ownership": {\n            "lastModified": {\n              "actor": "urn:li:corpuser:jdoe",\n              "time": 1581407189000\n            },\n            "owners": [\n              {\n                "owner": "urn:li:corpuser:jdoe",\n                "type": "DATAOWNER"\n              },\n              {\n                "owner": "urn:li:corpuser:datahub",\n                "type": "DATAOWNER"\n              }\n            ]\n          }\n        }\n      ],\n      "urn": "urn:li:dataset:(urn:li:dataPlatform:hive,SampleHiveDataset,PROD)"\n    }\n  }\n}\n')),(0,r.kt)("h3",{id:"put"},"put"),(0,r.kt)("p",null,"The ",(0,r.kt)("inlineCode",{parentName:"p"},"put")," group of commands allows you to write metadata into DataHub. This is a flexible way for you to issue edits to metadata from the command line."),(0,r.kt)("h4",{id:"put-aspect"},"put aspect"),(0,r.kt)("p",null,"The ",(0,r.kt)("strong",{parentName:"p"},"put aspect")," (also the default ",(0,r.kt)("inlineCode",{parentName:"p"},"put"),") command instructs ",(0,r.kt)("inlineCode",{parentName:"p"},"datahub")," to set a specific aspect for an entity to a specified value.\nFor example, the command shown below sets the ",(0,r.kt)("inlineCode",{parentName:"p"},"ownership")," aspect of the dataset ",(0,r.kt)("inlineCode",{parentName:"p"},"urn:li:dataset:(urn:li:dataPlatform:hive,SampleHiveDataset,PROD)")," to the value in the file ",(0,r.kt)("inlineCode",{parentName:"p"},"ownership.json"),".\nThe JSON in the ",(0,r.kt)("inlineCode",{parentName:"p"},"ownership.json")," file needs to conform to the ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/datahub-project/datahub/blob/master/metadata-models/src/main/pegasus/com/linkedin/common/Ownership.pdl"},(0,r.kt)("inlineCode",{parentName:"a"},"Ownership"))," Aspect model as shown below."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-json"},'{\n  "owners": [\n    {\n      "owner": "urn:li:corpuser:jdoe",\n      "type": "DEVELOPER"\n    },\n    {\n      "owner": "urn:li:corpuser:jdub",\n      "type": "DATAOWNER"\n    }\n  ]\n}\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-console"},'datahub --debug put --urn "urn:li:dataset:(urn:li:dataPlatform:hive,SampleHiveDataset,PROD)" --aspect ownership -d ownership.json\n\n[DATE_TIMESTAMP] DEBUG    {datahub.cli.cli_utils:340} - Attempting to emit to DataHub GMS; using curl equivalent to:\ncurl -X POST -H \'User-Agent: python-requests/2.26.0\' -H \'Accept-Encoding: gzip, deflate\' -H \'Accept: */*\' -H \'Connection: keep-alive\' -H \'X-RestLi-Protocol-Version: 2.0.0\' -H \'Content-Type: application/json\' --data \'{"proposal": {"entityType": "dataset", "entityUrn": "urn:li:dataset:(urn:li:dataPlatform:hive,SampleHiveDataset,PROD)", "aspectName": "ownership", "changeType": "UPSERT", "aspect": {"contentType": "application/json", "value": "{\\"owners\\": [{\\"owner\\": \\"urn:li:corpuser:jdoe\\", \\"type\\": \\"DEVELOPER\\"}, {\\"owner\\": \\"urn:li:corpuser:jdub\\", \\"type\\": \\"DATAOWNER\\"}]}"}}}\' \'http://localhost:8080/aspects/?action=ingestProposal\'\nUpdate succeeded with status 200\n')),(0,r.kt)("h4",{id:"put-platform"},"put platform"),(0,r.kt)("p",null,"The ",(0,r.kt)("strong",{parentName:"p"},"put platform")," command (available in version>0.8.44.4) instructs ",(0,r.kt)("inlineCode",{parentName:"p"},"datahub")," to create or update metadata about a data platform. This is very useful if you are using a custom data platform, to set up its logo and display name for a native UI experience."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},'datahub put platform --name longtail_schemas --display_name "Long Tail Schemas" --logo "https://flink.apache.org/img/logo/png/50/color_50.png"\n\u2705 Successfully wrote data platform metadata for urn:li:dataPlatform:longtail_schemas to DataHub (DataHubRestEmitter: configured to talk to https://longtailcompanions.acryl.io/api/gms with token: eyJh**********Cics)\n')),(0,r.kt)("h3",{id:"timeline"},"timeline"),(0,r.kt)("p",null,"The ",(0,r.kt)("inlineCode",{parentName:"p"},"timeline")," command allows you to view a version history for entities. Currently only supported for Datasets. For example,\nthe following command will show you the modifications to tags for a dataset for the past week. The output includes a computed semantic version,\nrelevant for schema changes only currently, the target of the modification, and a description of the change including a timestamp.\nThe default output is sanitized to be more readable, but the full API output can be obtained by passing the ",(0,r.kt)("inlineCode",{parentName:"p"},"--verbose")," flag and\nto get the raw JSON difference in addition to the API output you can add the ",(0,r.kt)("inlineCode",{parentName:"p"},"--raw")," flag. For more details about the feature please see ",(0,r.kt)("a",{parentName:"p",href:"/datahub-project-forked/docs/dev-guides/timeline"},"the main feature page")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-console"},'datahub timeline --urn "urn:li:dataset:(urn:li:dataPlatform:mysql,User.UserAccount,PROD)" --category TAG --start 7daysago\n2022-02-17 14:03:42 - 0.0.0-computed\n MODIFY TAG dataset:mysql:User.UserAccount : A change in aspect editableSchemaMetadata happened at time 2022-02-17 20:03:42.0\n2022-02-17 14:17:30 - 0.0.0-computed\n MODIFY TAG dataset:mysql:User.UserAccount : A change in aspect editableSchemaMetadata happened at time 2022-02-17 20:17:30.118\n')),(0,r.kt)("h3",{id:"telemetry"},"telemetry"),(0,r.kt)("p",null,"To help us understand how people are using DataHub, we collect anonymous usage statistics on actions such as command invocations via Mixpanel.\nWe do not collect private information such as IP addresses, contents of ingestions, or credentials.\nThe code responsible for collecting and broadcasting these events is open-source and can be found ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/datahub-project/datahub/blob/master/metadata-ingestion/src/datahub/telemetry/telemetry.py"},"within our GitHub"),"."),(0,r.kt)("p",null,"Telemetry is enabled by default, and the ",(0,r.kt)("inlineCode",{parentName:"p"},"telemetry")," command lets you toggle the sending of these statistics via ",(0,r.kt)("inlineCode",{parentName:"p"},"telemetry enable/disable"),"."),(0,r.kt)("h3",{id:"migrate"},"migrate"),(0,r.kt)("p",null,"The ",(0,r.kt)("inlineCode",{parentName:"p"},"migrate")," group of commands allows you to perform certain kinds of migrations."),(0,r.kt)("h4",{id:"dataplatform2instance"},"dataplatform2instance"),(0,r.kt)("p",null,"The ",(0,r.kt)("inlineCode",{parentName:"p"},"dataplatform2instance")," migration command allows you to migrate your entities from an instance-agnostic platform identifier to an instance-specific platform identifier. If you have ingested metadata in the past for this platform and would like to transfer any important metadata over to the new instance-specific entities, then you should use this command. For example, if your users have added documentation or added tags or terms to your datasets, then you should run this command to transfer this metadata over to the new entities. For further context, read the Platform Instance Guide ",(0,r.kt)("a",{parentName:"p",href:"/datahub-project-forked/docs/platform-instances"},"here"),"."),(0,r.kt)("p",null,"A few important options worth calling out:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"--dry-run / -n : Use this to get a report for what will be migrated before running"),(0,r.kt)("li",{parentName:"ul"},"--force / -F : Use this if you know what you are doing and do not want to get a confirmation prompt before migration is started"),(0,r.kt)("li",{parentName:"ul"},"--keep : When enabled, will preserve the old entities and not delete them. Default behavior is to soft-delete old entities."),(0,r.kt)("li",{parentName:"ul"},"--hard : When enabled, will hard-delete the old entities.")),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("em",{parentName:"strong"},"Note")),": Timeseries aspects such as Usage Statistics and Dataset Profiles are not migrated over to the new entity instances, you will get new data points created when you re-run ingestion using the ",(0,r.kt)("inlineCode",{parentName:"p"},"usage")," or sources with profiling turned on."),(0,r.kt)("h5",{id:"dry-run"},"Dry Run"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-console"},"datahub migrate dataplatform2instance --platform elasticsearch --instance prod_index --dry-run\nStarting migration: platform:elasticsearch, instance=prod_index, force=False, dry-run=True\n100% (25 of 25) |####################################################################################################################################################################################| Elapsed Time: 0:00:00 Time:  0:00:00\n[Dry Run] Migration Report:\n--------------\n[Dry Run] Migration Run Id: migrate-5710349c-1ec7-4b83-a7d3-47d71b7e972e\n[Dry Run] Num entities created = 25\n[Dry Run] Num entities affected = 0\n[Dry Run] Num entities migrated = 25\n[Dry Run] Details:\n[Dry Run] New Entities Created: {'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,prod_index.datahubretentionindex_v2,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,prod_index.schemafieldindex_v2,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,prod_index.system_metadata_service_v1,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,prod_index.tagindex_v2,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,prod_index.dataset_datasetprofileaspect_v1,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,prod_index.mlmodelindex_v2,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,prod_index.mlfeaturetableindex_v2,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,prod_index.datajob_datahubingestioncheckpointaspect_v1,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,prod_index.datahub_usage_event,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,prod_index.dataset_operationaspect_v1,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,prod_index.datajobindex_v2,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,prod_index.dataprocessindex_v2,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,prod_index.glossarytermindex_v2,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,prod_index.dataplatformindex_v2,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,prod_index.mlmodeldeploymentindex_v2,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,prod_index.datajob_datahubingestionrunsummaryaspect_v1,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,prod_index.graph_service_v1,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,prod_index.datahubpolicyindex_v2,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,prod_index.dataset_datasetusagestatisticsaspect_v1,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,prod_index.dashboardindex_v2,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,prod_index.glossarynodeindex_v2,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,prod_index.mlfeatureindex_v2,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,prod_index.dataflowindex_v2,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,prod_index.mlprimarykeyindex_v2,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,prod_index.chartindex_v2,PROD)'}\n[Dry Run] External Entities Affected: None\n[Dry Run] Old Entities Migrated = {'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,dataset_datasetusagestatisticsaspect_v1,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,mlmodelindex_v2,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,mlmodeldeploymentindex_v2,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,datajob_datahubingestionrunsummaryaspect_v1,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,datahubretentionindex_v2,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,datahubpolicyindex_v2,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,dataset_datasetprofileaspect_v1,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,glossarynodeindex_v2,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,dataset_operationaspect_v1,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,graph_service_v1,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,datajobindex_v2,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,mlprimarykeyindex_v2,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,dashboardindex_v2,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,datajob_datahubingestioncheckpointaspect_v1,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,tagindex_v2,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,datahub_usage_event,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,schemafieldindex_v2,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,mlfeatureindex_v2,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,dataprocessindex_v2,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,dataplatformindex_v2,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,mlfeaturetableindex_v2,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,glossarytermindex_v2,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,dataflowindex_v2,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,chartindex_v2,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:elasticsearch,system_metadata_service_v1,PROD)'}\n")),(0,r.kt)("h5",{id:"real-migration-with-soft-delete"},"Real Migration (with soft-delete)"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"> datahub migrate dataplatform2instance --platform hive --instance\ndatahub migrate dataplatform2instance --platform hive --instance warehouse\nStarting migration: platform:hive, instance=warehouse, force=False, dry-run=False\nWill migrate 4 urns such as ['urn:li:dataset:(urn:li:dataPlatform:hive,SampleHiveDataset,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:hive,SampleHiveDataset,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:hive,fct_users_deleted,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:hive,logging_events,PROD)']\nNew urns will look like ['urn:li:dataset:(urn:li:dataPlatform:hive,warehouse.logging_events,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:hive,warehouse.fct_users_created,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:hive,warehouse.SampleHiveDataset,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:hive,warehouse.fct_users_deleted,PROD)']\n\nOk to proceed? [y/N]:\n...\nMigration Report:\n--------------\nMigration Run Id: migrate-f5ae7201-4548-4bee-aed4-35758bb78c89\nNum entities created = 4\nNum entities affected = 0\nNum entities migrated = 4\nDetails:\nNew Entities Created: {'urn:li:dataset:(urn:li:dataPlatform:hive,warehouse.SampleHiveDataset,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:hive,warehouse.fct_users_deleted,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:hive,warehouse.logging_events,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:hive,warehouse.fct_users_created,PROD)'}\nExternal Entities Affected: None\nOld Entities Migrated = {'urn:li:dataset:(urn:li:dataPlatform:hive,logging_events,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:hive,SampleHiveDataset,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:hive,fct_users_deleted,PROD)', 'urn:li:dataset:(urn:li:dataPlatform:hive,fct_users_created,PROD)'}\n")),(0,r.kt)("h2",{id:"alternate-installation-options"},"Alternate Installation Options"),(0,r.kt)("h3",{id:"using-docker"},"Using docker"),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://hub.docker.com/r/linkedin/datahub-ingestion"},(0,r.kt)("img",{parentName:"a",src:"https://img.shields.io/docker/pulls/linkedin/datahub-ingestion?style=plastic",alt:"Docker Hub"})),"\n",(0,r.kt)("a",{parentName:"p",href:"https://github.com/datahub-project/datahub/actions/workflows/docker-ingestion.yml"},(0,r.kt)("img",{parentName:"a",src:"https://github.com/datahub-project/datahub/actions/workflows/docker-ingestion.yml/badge.svg",alt:"datahub-ingestion docker"}))),(0,r.kt)("p",null,"If you don't want to install locally, you can alternatively run metadata ingestion within a Docker container.\nWe have prebuilt images available on ",(0,r.kt)("a",{parentName:"p",href:"https://hub.docker.com/r/linkedin/datahub-ingestion"},"Docker hub"),". All plugins will be installed and enabled automatically."),(0,r.kt)("p",null,"You can use the ",(0,r.kt)("inlineCode",{parentName:"p"},"datahub-ingestion")," docker image as explained in ",(0,r.kt)("a",{parentName:"p",href:"/datahub-project-forked/docs/docker"},"Docker Images"),". In case you are using Kubernetes you can start a pod with the ",(0,r.kt)("inlineCode",{parentName:"p"},"datahub-ingestion")," docker image, log onto a shell on the pod and you should have the access to datahub CLI in your kubernetes cluster."),(0,r.kt)("p",null,(0,r.kt)("em",{parentName:"p"},"Limitation: the datahub_docker.sh convenience script assumes that the recipe and any input/output files are accessible in the current working directory or its subdirectories. Files outside the current working directory will not be found, and you'll need to invoke the Docker image directly.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},"# Assumes the DataHub repo is cloned locally.\n./metadata-ingestion/scripts/datahub_docker.sh ingest -c ./examples/recipes/example_to_datahub_rest.yml\n")),(0,r.kt)("h3",{id:"install-from-source"},"Install from source"),(0,r.kt)("p",null,"If you'd like to install from source, see the ",(0,r.kt)("a",{parentName:"p",href:"/datahub-project-forked/docs/metadata-ingestion/developing"},"developer guide"),"."),(0,r.kt)("h2",{id:"installing-plugins"},"Installing Plugins"),(0,r.kt)("p",null,"We use a plugin architecture so that you can install only the dependencies you actually need. Click the plugin name to learn more about the specific source recipe and any FAQs!"),(0,r.kt)("h3",{id:"sources"},"Sources"),(0,r.kt)("p",null,"Please see our ",(0,r.kt)("a",{parentName:"p",href:"/integrations"},"Integrations page")," if you want to filter on the features offered by each source."),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Plugin Name"),(0,r.kt)("th",{parentName:"tr",align:null},"Install Command"),(0,r.kt)("th",{parentName:"tr",align:null},"Provides"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("a",{parentName:"td",href:"/datahub-project-forked/docs/generated/ingestion/sources/file"},"file")),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("em",{parentName:"td"},"included by default")),(0,r.kt)("td",{parentName:"tr",align:null},"File source and sink")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("a",{parentName:"td",href:"/datahub-project-forked/docs/generated/ingestion/sources/athena"},"athena")),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[athena]'")),(0,r.kt)("td",{parentName:"tr",align:null},"AWS Athena source")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("a",{parentName:"td",href:"/datahub-project-forked/docs/generated/ingestion/sources/bigquery"},"bigquery")),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[bigquery]'")),(0,r.kt)("td",{parentName:"tr",align:null},"BigQuery source")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("a",{parentName:"td",href:"/datahub-project-forked/docs/generated/ingestion/sources/file-based-lineage"},"datahub-lineage-file")),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("em",{parentName:"td"},"no additional dependencies")),(0,r.kt)("td",{parentName:"tr",align:null},"Lineage File source")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("a",{parentName:"td",href:"/datahub-project-forked/docs/generated/ingestion/sources/business-glossary"},"datahub-business-glossary")),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("em",{parentName:"td"},"no additional dependencies")),(0,r.kt)("td",{parentName:"tr",align:null},"Business Glossary File source")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("a",{parentName:"td",href:"/datahub-project-forked/docs/generated/ingestion/sources/dbt"},"dbt")),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("em",{parentName:"td"},"no additional dependencies")),(0,r.kt)("td",{parentName:"tr",align:null},"dbt source")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("a",{parentName:"td",href:"/datahub-project-forked/docs/generated/ingestion/sources/druid"},"druid")),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[druid]'")),(0,r.kt)("td",{parentName:"tr",align:null},"Druid Source")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("a",{parentName:"td",href:"/datahub-project-forked/docs/generated/ingestion/sources/feast"},"feast")),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[feast]'")),(0,r.kt)("td",{parentName:"tr",align:null},"Feast source (0.26.0)")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("a",{parentName:"td",href:"/datahub-project-forked/docs/generated/ingestion/sources/glue"},"glue")),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[glue]'")),(0,r.kt)("td",{parentName:"tr",align:null},"AWS Glue source")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("a",{parentName:"td",href:"/datahub-project-forked/docs/generated/ingestion/sources/hana"},"hana")),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[hana]'")),(0,r.kt)("td",{parentName:"tr",align:null},"SAP HANA source")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("a",{parentName:"td",href:"/datahub-project-forked/docs/generated/ingestion/sources/hive"},"hive")),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[hive]'")),(0,r.kt)("td",{parentName:"tr",align:null},"Hive source")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("a",{parentName:"td",href:"/datahub-project-forked/docs/generated/ingestion/sources/kafka"},"kafka")),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[kafka]'")),(0,r.kt)("td",{parentName:"tr",align:null},"Kafka source")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("a",{parentName:"td",href:"/datahub-project-forked/docs/generated/ingestion/sources/kafka-connect"},"kafka-connect")),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[kafka-connect]'")),(0,r.kt)("td",{parentName:"tr",align:null},"Kafka connect source")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("a",{parentName:"td",href:"/datahub-project-forked/docs/generated/ingestion/sources/ldap"},"ldap")),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[ldap]'")," (",(0,r.kt)("a",{parentName:"td",href:"https://www.python-ldap.org/en/python-ldap-3.3.0/installing.html#build-prerequisites"},"extra requirements"),")"),(0,r.kt)("td",{parentName:"tr",align:null},"LDAP source")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("a",{parentName:"td",href:"/datahub-project-forked/docs/generated/ingestion/sources/looker"},"looker")),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[looker]'")),(0,r.kt)("td",{parentName:"tr",align:null},"Looker source")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("a",{parentName:"td",href:"/datahub-project-forked/docs/generated/ingestion/sources/looker#module-lookml"},"lookml")),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[lookml]'")),(0,r.kt)("td",{parentName:"tr",align:null},"LookML source, requires Python 3.7+")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("a",{parentName:"td",href:"/datahub-project-forked/docs/generated/ingestion/sources/metabase"},"metabase")),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[metabase]'")),(0,r.kt)("td",{parentName:"tr",align:null},"Metabase source")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("a",{parentName:"td",href:"/datahub-project-forked/docs/generated/ingestion/sources/mode"},"mode")),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[mode]'")),(0,r.kt)("td",{parentName:"tr",align:null},"Mode Analytics source")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("a",{parentName:"td",href:"/datahub-project-forked/docs/generated/ingestion/sources/mongodb"},"mongodb")),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[mongodb]'")),(0,r.kt)("td",{parentName:"tr",align:null},"MongoDB source")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("a",{parentName:"td",href:"/datahub-project-forked/docs/generated/ingestion/sources/mssql"},"mssql")),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[mssql]'")),(0,r.kt)("td",{parentName:"tr",align:null},"SQL Server source")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("a",{parentName:"td",href:"/datahub-project-forked/docs/generated/ingestion/sources/mysql"},"mysql")),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[mysql]'")),(0,r.kt)("td",{parentName:"tr",align:null},"MySQL source")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("a",{parentName:"td",href:"/datahub-project-forked/docs/generated/ingestion/sources/mariadb"},"mariadb")),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[mariadb]'")),(0,r.kt)("td",{parentName:"tr",align:null},"MariaDB source")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("a",{parentName:"td",href:"/datahub-project-forked/docs/generated/ingestion/sources/openapi"},"openapi")),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[openapi]'")),(0,r.kt)("td",{parentName:"tr",align:null},"OpenApi Source")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("a",{parentName:"td",href:"/datahub-project-forked/docs/generated/ingestion/sources/oracle"},"oracle")),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[oracle]'")),(0,r.kt)("td",{parentName:"tr",align:null},"Oracle source")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("a",{parentName:"td",href:"/datahub-project-forked/docs/generated/ingestion/sources/postgres"},"postgres")),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[postgres]'")),(0,r.kt)("td",{parentName:"tr",align:null},"Postgres source")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("a",{parentName:"td",href:"/datahub-project-forked/docs/generated/ingestion/sources/redash"},"redash")),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[redash]'")),(0,r.kt)("td",{parentName:"tr",align:null},"Redash source")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("a",{parentName:"td",href:"/datahub-project-forked/docs/generated/ingestion/sources/redshift"},"redshift")),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[redshift]'")),(0,r.kt)("td",{parentName:"tr",align:null},"Redshift source")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("a",{parentName:"td",href:"/datahub-project-forked/docs/generated/ingestion/sources/sagemaker"},"sagemaker")),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[sagemaker]'")),(0,r.kt)("td",{parentName:"tr",align:null},"AWS SageMaker source")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("a",{parentName:"td",href:"/datahub-project-forked/docs/generated/ingestion/sources/snowflake"},"snowflake")),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[snowflake]'")),(0,r.kt)("td",{parentName:"tr",align:null},"Snowflake source")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("a",{parentName:"td",href:"/datahub-project-forked/docs/generated/ingestion/sources/sqlalchemy"},"sqlalchemy")),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[sqlalchemy]'")),(0,r.kt)("td",{parentName:"tr",align:null},"Generic SQLAlchemy source")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("a",{parentName:"td",href:"/datahub-project-forked/docs/generated/ingestion/sources/superset"},"superset")),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[superset]'")),(0,r.kt)("td",{parentName:"tr",align:null},"Superset source")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("a",{parentName:"td",href:"/datahub-project-forked/docs/generated/ingestion/sources/tableau"},"tableau")),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[tableau]'")),(0,r.kt)("td",{parentName:"tr",align:null},"Tableau source")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("a",{parentName:"td",href:"/datahub-project-forked/docs/generated/ingestion/sources/trino"},"trino")),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[trino]'")),(0,r.kt)("td",{parentName:"tr",align:null},"Trino source")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("a",{parentName:"td",href:"/datahub-project-forked/docs/generated/ingestion/sources/trino"},"starburst-trino-usage")),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[starburst-trino-usage]'")),(0,r.kt)("td",{parentName:"tr",align:null},"Starburst Trino usage statistics source")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("a",{parentName:"td",href:"/datahub-project-forked/docs/generated/ingestion/sources/nifi"},"nifi")),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[nifi]'")),(0,r.kt)("td",{parentName:"tr",align:null},"NiFi source")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("a",{parentName:"td",href:"/datahub-project-forked/docs/generated/ingestion/sources/powerbi#module-powerbi"},"powerbi")),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[powerbi]'")),(0,r.kt)("td",{parentName:"tr",align:null},"Microsoft Power BI source")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("a",{parentName:"td",href:"/datahub-project-forked/docs/generated/ingestion/sources/powerbi#module-powerbi-report-server"},"powerbi-report-server")),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[powerbi-report-server]'")),(0,r.kt)("td",{parentName:"tr",align:null},"Microsoft Power BI Report Server source")))),(0,r.kt)("h3",{id:"sinks"},"Sinks"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Plugin Name"),(0,r.kt)("th",{parentName:"tr",align:null},"Install Command"),(0,r.kt)("th",{parentName:"tr",align:null},"Provides"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("a",{parentName:"td",href:"/datahub-project-forked/docs/metadata-ingestion/sink_docs/file"},"file")),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("em",{parentName:"td"},"included by default")),(0,r.kt)("td",{parentName:"tr",align:null},"File source and sink")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("a",{parentName:"td",href:"/datahub-project-forked/docs/metadata-ingestion/sink_docs/console"},"console")),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("em",{parentName:"td"},"included by default")),(0,r.kt)("td",{parentName:"tr",align:null},"Console sink")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("a",{parentName:"td",href:"/datahub-project-forked/docs/metadata-ingestion/sink_docs/datahub"},"datahub-rest")),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[datahub-rest]'")),(0,r.kt)("td",{parentName:"tr",align:null},"DataHub sink over REST API")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("a",{parentName:"td",href:"/datahub-project-forked/docs/metadata-ingestion/sink_docs/datahub"},"datahub-kafka")),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"pip install 'acryl-datahub[datahub-kafka]'")),(0,r.kt)("td",{parentName:"tr",align:null},"DataHub sink over Kafka")))),(0,r.kt)("p",null,"These plugins can be mixed and matched as desired. For example:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},"pip install 'acryl-datahub[bigquery,datahub-rest]'\n")),(0,r.kt)("h3",{id:"check-the-active-plugins"},"Check the active plugins"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},"datahub check plugins\n")),(0,r.kt)("h2",{id:"release-notes-and-cli-versions"},"Release Notes and CLI versions"),(0,r.kt)("p",null,"The server release notes can be found in ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/datahub-project/datahub/releases"},"github releases"),". These releases are done approximately every week on a regular cadence unless a blocking issue or regression is discovered."),(0,r.kt)("p",null,"CLI release is made through a different repository and release notes can be found in ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/acryldata/datahub/releases"},"acryldata releases"),". At least one release which is tied to the server release is always made alongwith the server release. Multiple other bigfix releases are made in between based on amount of fixes that are merged between the server release mentioned above."),(0,r.kt)("p",null,"If server with version ",(0,r.kt)("inlineCode",{parentName:"p"},"0.8.28")," is being used then CLI used to connect to it should be ",(0,r.kt)("inlineCode",{parentName:"p"},"0.8.28.x"),". Tests of new CLI are not ran with older server versions so it is not recommended to update the CLI if the server is not updated."))}p.isMDXComponent=!0}}]);
"use strict";(self.webpackChunkdocs_website=self.webpackChunkdocs_website||[]).push([[53],{1109:e=>{e.exports=JSON.parse('{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"overviewSidebar":[{"type":"category","label":"What is DataHub?","items":[{"type":"link","label":"Features","href":"/datahub-project-forked/docs/features","docId":"docs/features"},{"type":"link","label":"Concepts","href":"/datahub-project-forked/docs/what-is-datahub/datahub-concepts","docId":"docs/what-is-datahub/datahub-concepts"},{"type":"category","label":"Architecture","items":[{"type":"link","label":"Overview","href":"/datahub-project-forked/docs/architecture/architecture","docId":"docs/architecture/architecture"},{"type":"link","label":"Components","href":"/datahub-project-forked/docs/components","docId":"docs/components"},{"type":"link","label":"Ingestion Framework","href":"/datahub-project-forked/docs/architecture/metadata-ingestion","docId":"docs/architecture/metadata-ingestion"},{"type":"link","label":"Serving Tier","href":"/datahub-project-forked/docs/architecture/metadata-serving","docId":"docs/architecture/metadata-serving"}],"collapsed":true,"collapsible":true},{"type":"link","label":"See DataHub in Action","href":"/datahub-project-forked/docs/demo","docId":"docs/demo"},{"type":"link","label":"Managed DataHub","href":"/datahub-project-forked/docs/saas","docId":"docs/saas"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Get Started","collapsed":false,"items":[{"type":"category","label":"Self-Hosted DataHub","items":[{"type":"link","label":"Quickstart Guide","href":"/datahub-project-forked/docs/quickstart","docId":"docs/quickstart"},{"type":"link","label":"Onboarding Users to DataHub","href":"/datahub-project-forked/docs/authentication/guides/add-users","docId":"docs/authentication/guides/add-users"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Managed DataHub","items":[{"type":"link","label":"Onboarding Users to DataHub","href":"/datahub-project-forked/docs/authentication/guides/add-users","docId":"docs/authentication/guides/add-users"},{"type":"link","label":"Configure Slack Notifications","href":"/datahub-project-forked/docs/managed-datahub/saas-slack-setup","className":"saasOnly","docId":"docs/managed-datahub/saas-slack-setup"},{"type":"link","label":"Approval Workflows","href":"/datahub-project-forked/docs/managed-datahub/approval-workflows","className":"saasOnly","docId":"docs/managed-datahub/approval-workflows"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Ingestion Quickstart Guides","items":[{"type":"category","label":"BigQuery","items":[{"type":"link","label":"Overview","href":"/datahub-project-forked/docs/quick-ingestion-guides/bigquery/overview","docId":"docs/quick-ingestion-guides/bigquery/overview"},{"type":"link","label":"Setup","href":"/datahub-project-forked/docs/quick-ingestion-guides/bigquery/setup","docId":"docs/quick-ingestion-guides/bigquery/setup"},{"type":"link","label":"Configuration","href":"/datahub-project-forked/docs/quick-ingestion-guides/bigquery/configuration","docId":"docs/quick-ingestion-guides/bigquery/configuration"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Snowflake","items":[{"type":"link","label":"Overview","href":"/datahub-project-forked/docs/quick-ingestion-guides/snowflake/overview","docId":"docs/quick-ingestion-guides/snowflake/overview"},{"type":"link","label":"Setup","href":"/datahub-project-forked/docs/quick-ingestion-guides/snowflake/setup","docId":"docs/quick-ingestion-guides/snowflake/setup"},{"type":"link","label":"Configuration","href":"/datahub-project-forked/docs/quick-ingestion-guides/snowflake/configuration","docId":"docs/quick-ingestion-guides/snowflake/configuration"}],"collapsed":true,"collapsible":true}],"collapsed":true,"collapsible":true}],"collapsible":true,"href":"/datahub-project-forked/docs/get-started-with-datahub"},{"type":"category","label":"Ingest Metadata","items":[{"type":"category","label":"Overview","items":[{"type":"link","label":"Introduction to Metadata Ingestion","href":"/datahub-project-forked/docs/metadata-ingestion","docId":"metadata-ingestion/README"},{"type":"link","label":"UI Ingestion Guide","href":"/datahub-project-forked/docs/ui-ingestion","docId":"docs/ui-ingestion"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Sources","items":[{"type":"link","label":"Airflow","href":"/datahub-project-forked/docs/lineage/airflow","docId":"docs/lineage/airflow"},{"type":"link","label":"Spark","href":"/datahub-project-forked/docs/metadata-integration/java/spark-lineage","docId":"metadata-integration/java/spark-lineage/README"},{"type":"link","label":"Great Expectations","href":"/datahub-project-forked/docs/metadata-ingestion/integration_docs/great-expectations","docId":"metadata-ingestion/integration_docs/great-expectations"},{"type":"link","label":"Protobuf Schemas","href":"/datahub-project-forked/docs/metadata-integration/java/datahub-protobuf","docId":"metadata-integration/java/datahub-protobuf/README"},{"type":"link","label":"Athena","href":"/datahub-project-forked/docs/generated/ingestion/sources/athena","docId":"docs/generated/ingestion/sources/athena"},{"type":"link","label":"Azure AD","href":"/datahub-project-forked/docs/generated/ingestion/sources/azure-ad","docId":"docs/generated/ingestion/sources/azure-ad"},{"type":"link","label":"BigQuery","href":"/datahub-project-forked/docs/generated/ingestion/sources/bigquery","docId":"docs/generated/ingestion/sources/bigquery"},{"type":"link","label":"Business Glossary","href":"/datahub-project-forked/docs/generated/ingestion/sources/business-glossary","docId":"docs/generated/ingestion/sources/business-glossary"},{"type":"link","label":"ClickHouse","href":"/datahub-project-forked/docs/generated/ingestion/sources/clickhouse","docId":"docs/generated/ingestion/sources/clickhouse"},{"type":"link","label":"CSV","href":"/datahub-project-forked/docs/generated/ingestion/sources/csv","docId":"docs/generated/ingestion/sources/csv"},{"type":"link","label":"Databricks","href":"/datahub-project-forked/docs/generated/ingestion/sources/databricks","docId":"docs/generated/ingestion/sources/databricks"},{"type":"link","label":"dbt","href":"/datahub-project-forked/docs/generated/ingestion/sources/dbt","docId":"docs/generated/ingestion/sources/dbt"},{"type":"link","label":"Delta Lake","href":"/datahub-project-forked/docs/generated/ingestion/sources/delta-lake","docId":"docs/generated/ingestion/sources/delta-lake"},{"type":"link","label":"Demo Data","href":"/datahub-project-forked/docs/generated/ingestion/sources/demo-data","docId":"docs/generated/ingestion/sources/demo-data"},{"type":"link","label":"Druid","href":"/datahub-project-forked/docs/generated/ingestion/sources/druid","docId":"docs/generated/ingestion/sources/druid"},{"type":"link","label":"Elasticsearch","href":"/datahub-project-forked/docs/generated/ingestion/sources/elasticsearch","docId":"docs/generated/ingestion/sources/elasticsearch"},{"type":"link","label":"Feast","href":"/datahub-project-forked/docs/generated/ingestion/sources/feast","docId":"docs/generated/ingestion/sources/feast"},{"type":"link","label":"File","href":"/datahub-project-forked/docs/generated/ingestion/sources/file","docId":"docs/generated/ingestion/sources/file"},{"type":"link","label":"File Based Lineage","href":"/datahub-project-forked/docs/generated/ingestion/sources/file-based-lineage","docId":"docs/generated/ingestion/sources/file-based-lineage"},{"type":"link","label":"Glue","href":"/datahub-project-forked/docs/generated/ingestion/sources/glue","docId":"docs/generated/ingestion/sources/glue"},{"type":"link","label":"Hive","href":"/datahub-project-forked/docs/generated/ingestion/sources/hive","docId":"docs/generated/ingestion/sources/hive"},{"type":"link","label":"Iceberg","href":"/datahub-project-forked/docs/generated/ingestion/sources/iceberg","docId":"docs/generated/ingestion/sources/iceberg"},{"type":"link","label":"JSON Schemas","href":"/datahub-project-forked/docs/generated/ingestion/sources/json-schema","docId":"docs/generated/ingestion/sources/json-schema"},{"type":"link","label":"Kafka","href":"/datahub-project-forked/docs/generated/ingestion/sources/kafka","docId":"docs/generated/ingestion/sources/kafka"},{"type":"link","label":"Kafka Connect","href":"/datahub-project-forked/docs/generated/ingestion/sources/kafka-connect","docId":"docs/generated/ingestion/sources/kafka-connect"},{"type":"link","label":"LDAP","href":"/datahub-project-forked/docs/generated/ingestion/sources/ldap","docId":"docs/generated/ingestion/sources/ldap"},{"type":"link","label":"Looker","href":"/datahub-project-forked/docs/generated/ingestion/sources/looker","docId":"docs/generated/ingestion/sources/looker"},{"type":"link","label":"MariaDB","href":"/datahub-project-forked/docs/generated/ingestion/sources/mariadb","docId":"docs/generated/ingestion/sources/mariadb"},{"type":"link","label":"Metabase","href":"/datahub-project-forked/docs/generated/ingestion/sources/metabase","docId":"docs/generated/ingestion/sources/metabase"},{"type":"link","label":"Microsoft SQL Server","href":"/datahub-project-forked/docs/generated/ingestion/sources/mssql","docId":"docs/generated/ingestion/sources/mssql"},{"type":"link","label":"Mode","href":"/datahub-project-forked/docs/generated/ingestion/sources/mode","docId":"docs/generated/ingestion/sources/mode"},{"type":"link","label":"MongoDB","href":"/datahub-project-forked/docs/generated/ingestion/sources/mongodb","docId":"docs/generated/ingestion/sources/mongodb"},{"type":"link","label":"MySQL","href":"/datahub-project-forked/docs/generated/ingestion/sources/mysql","docId":"docs/generated/ingestion/sources/mysql"},{"type":"link","label":"NiFi","href":"/datahub-project-forked/docs/generated/ingestion/sources/nifi","docId":"docs/generated/ingestion/sources/nifi"},{"type":"link","label":"Okta","href":"/datahub-project-forked/docs/generated/ingestion/sources/okta","docId":"docs/generated/ingestion/sources/okta"},{"type":"link","label":"OpenAPI","href":"/datahub-project-forked/docs/generated/ingestion/sources/openapi","docId":"docs/generated/ingestion/sources/openapi"},{"type":"link","label":"Oracle","href":"/datahub-project-forked/docs/generated/ingestion/sources/oracle","docId":"docs/generated/ingestion/sources/oracle"},{"type":"link","label":"Postgres","href":"/datahub-project-forked/docs/generated/ingestion/sources/postgres","docId":"docs/generated/ingestion/sources/postgres"},{"type":"link","label":"PowerBI","href":"/datahub-project-forked/docs/generated/ingestion/sources/powerbi","docId":"docs/generated/ingestion/sources/powerbi"},{"type":"link","label":"Presto","href":"/datahub-project-forked/docs/generated/ingestion/sources/presto","docId":"docs/generated/ingestion/sources/presto"},{"type":"link","label":"Presto on Hive","href":"/datahub-project-forked/docs/generated/ingestion/sources/presto-on-hive","docId":"docs/generated/ingestion/sources/presto-on-hive"},{"type":"link","label":"Pulsar","href":"/datahub-project-forked/docs/generated/ingestion/sources/pulsar","docId":"docs/generated/ingestion/sources/pulsar"},{"type":"link","label":"Redash","href":"/datahub-project-forked/docs/generated/ingestion/sources/redash","docId":"docs/generated/ingestion/sources/redash"},{"type":"link","label":"Redshift","href":"/datahub-project-forked/docs/generated/ingestion/sources/redshift","docId":"docs/generated/ingestion/sources/redshift"},{"type":"link","label":"S3 Data Lake","href":"/datahub-project-forked/docs/generated/ingestion/sources/s3","docId":"docs/generated/ingestion/sources/s3"},{"type":"link","label":"SageMaker","href":"/datahub-project-forked/docs/generated/ingestion/sources/sagemaker","docId":"docs/generated/ingestion/sources/sagemaker"},{"type":"link","label":"Salesforce","href":"/datahub-project-forked/docs/generated/ingestion/sources/salesforce","docId":"docs/generated/ingestion/sources/salesforce"},{"type":"link","label":"SAP HANA","href":"/datahub-project-forked/docs/generated/ingestion/sources/hana","docId":"docs/generated/ingestion/sources/hana"},{"type":"link","label":"Snowflake","href":"/datahub-project-forked/docs/generated/ingestion/sources/snowflake","docId":"docs/generated/ingestion/sources/snowflake"},{"type":"link","label":"SQLAlchemy","href":"/datahub-project-forked/docs/generated/ingestion/sources/sqlalchemy","docId":"docs/generated/ingestion/sources/sqlalchemy"},{"type":"link","label":"Superset","href":"/datahub-project-forked/docs/generated/ingestion/sources/superset","docId":"docs/generated/ingestion/sources/superset"},{"type":"link","label":"Tableau","href":"/datahub-project-forked/docs/generated/ingestion/sources/tableau","docId":"docs/generated/ingestion/sources/tableau"},{"type":"link","label":"Trino","href":"/datahub-project-forked/docs/generated/ingestion/sources/trino","docId":"docs/generated/ingestion/sources/trino"},{"type":"link","label":"Vertica","href":"/datahub-project-forked/docs/generated/ingestion/sources/vertica","docId":"docs/generated/ingestion/sources/vertica"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Sinks","items":[{"type":"link","label":"Console","href":"/datahub-project-forked/docs/metadata-ingestion/sink_docs/console","docId":"metadata-ingestion/sink_docs/console"},{"type":"link","label":"DataHub","href":"/datahub-project-forked/docs/metadata-ingestion/sink_docs/datahub","docId":"metadata-ingestion/sink_docs/datahub"},{"type":"link","label":"File","href":"/datahub-project-forked/docs/metadata-ingestion/sink_docs/file","docId":"metadata-ingestion/sink_docs/file"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Transformers","items":[{"type":"link","label":"Introduction","href":"/datahub-project-forked/docs/metadata-ingestion/docs/transformer/intro","docId":"metadata-ingestion/docs/transformer/intro"},{"type":"link","label":"Dataset","href":"/datahub-project-forked/docs/metadata-ingestion/docs/transformer/dataset_transformer","docId":"metadata-ingestion/docs/transformer/dataset_transformer"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Advanced Guides","items":[{"type":"category","label":"Scheduling Ingestion","items":[{"type":"link","label":"Introduction to Scheduling Metadata Ingestion","href":"/datahub-project-forked/docs/metadata-ingestion/schedule_docs/intro","docId":"metadata-ingestion/schedule_docs/intro"},{"type":"link","label":"Using Cron","href":"/datahub-project-forked/docs/metadata-ingestion/schedule_docs/cron","docId":"metadata-ingestion/schedule_docs/cron"},{"type":"link","label":"Using Airflow","href":"/datahub-project-forked/docs/metadata-ingestion/schedule_docs/airflow","docId":"metadata-ingestion/schedule_docs/airflow"},{"type":"link","label":"Using Kubernetes","href":"/datahub-project-forked/docs/metadata-ingestion/schedule_docs/kubernetes","docId":"metadata-ingestion/schedule_docs/kubernetes"}],"collapsed":true,"collapsible":true},{"type":"link","label":"Working With Platform Instances","href":"/datahub-project-forked/docs/platform-instances","docId":"docs/platform-instances"},{"type":"link","label":"Stateful Ingestion","href":"/datahub-project-forked/docs/metadata-ingestion/docs/dev_guides/stateful","docId":"metadata-ingestion/docs/dev_guides/stateful"},{"type":"link","label":"Classification","href":"/datahub-project-forked/docs/metadata-ingestion/docs/dev_guides/classification","docId":"metadata-ingestion/docs/dev_guides/classification"},{"type":"link","label":"Adding Stateful Ingestion to a Source","href":"/datahub-project-forked/docs/metadata-ingestion/docs/dev_guides/add_stateful_ingestion_to_source","docId":"metadata-ingestion/docs/dev_guides/add_stateful_ingestion_to_source"},{"type":"link","label":"SQL Profiling","href":"/datahub-project-forked/docs/metadata-ingestion/docs/dev_guides/sql_profiles","docId":"metadata-ingestion/docs/dev_guides/sql_profiles"}],"collapsed":true,"collapsible":true}],"collapsed":true,"collapsible":true},{"type":"category","label":"Enrich Metadata","items":[{"type":"link","label":"Domains","href":"/datahub-project-forked/docs/domains","docId":"docs/domains"},{"type":"link","label":"Business Glossary","href":"/datahub-project-forked/docs/glossary/business-glossary","docId":"docs/glossary/business-glossary"},{"type":"link","label":"Tags","href":"/datahub-project-forked/docs/tags","docId":"docs/tags"},{"type":"link","label":"Lineage","href":"/datahub-project-forked/docs/lineage/lineage-feature-guide","docId":"docs/lineage/lineage-feature-guide"}],"collapsed":true,"collapsible":true,"href":"/datahub-project-forked/docs/enrich-metadata"},{"type":"category","label":"Act on Metadata","items":[{"type":"category","label":"Actions Framework","items":[{"type":"link","label":"Introduction","href":"/datahub-project-forked/docs/actions","docId":"docs/actions/README"},{"type":"link","label":"Quickstart","href":"/datahub-project-forked/docs/actions/quickstart","docId":"docs/actions/quickstart"},{"type":"link","label":"Concepts","href":"/datahub-project-forked/docs/actions/concepts","docId":"docs/actions/concepts"},{"type":"category","label":"Sources","items":[{"type":"link","label":"Kafka Event Source","href":"/datahub-project-forked/docs/actions/sources/kafka-event-source","docId":"docs/actions/sources/kafka-event-source"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Events","items":[{"type":"link","label":"Entity Change Event V1","href":"/datahub-project-forked/docs/actions/events/entity-change-event","docId":"docs/actions/events/entity-change-event"},{"type":"link","label":"Metadata Change Log Event V1","href":"/datahub-project-forked/docs/actions/events/metadata-change-log-event","docId":"docs/actions/events/metadata-change-log-event"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Actions","items":[{"type":"link","label":"Ingestion Executor","href":"/datahub-project-forked/docs/actions/actions/executor","docId":"docs/actions/actions/executor"},{"type":"link","label":"Hello World","href":"/datahub-project-forked/docs/actions/actions/hello_world","docId":"docs/actions/actions/hello_world"},{"type":"link","label":"Slack","href":"/datahub-project-forked/docs/actions/actions/slack","docId":"docs/actions/actions/slack"},{"type":"link","label":"Microsoft Teams","href":"/datahub-project-forked/docs/actions/actions/teams","docId":"docs/actions/actions/teams"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Guides","items":[{"type":"link","label":"Developing a Transformer","href":"/datahub-project-forked/docs/actions/guides/developing-a-transformer","docId":"docs/actions/guides/developing-a-transformer"},{"type":"link","label":"Developing an Action","href":"/datahub-project-forked/docs/actions/guides/developing-an-action","docId":"docs/actions/guides/developing-an-action"}],"collapsed":true,"collapsible":true}],"collapsed":true,"collapsible":true},{"type":"link","label":"About Metadata Tests","href":"/datahub-project-forked/docs/tests/metadata-tests","className":"saasOnly","docId":"docs/tests/metadata-tests"},{"type":"link","label":"Lineage Impact Analysis","href":"/datahub-project-forked/docs/act-on-metadata/impact-analysis","docId":"docs/act-on-metadata/impact-analysis"}],"collapsed":true,"collapsible":true,"href":"/datahub-project-forked/docs/act-on-metadata"},{"type":"category","label":"Deploy DataHub","items":[{"type":"link","label":"Deploying to AWS","href":"/datahub-project-forked/docs/deploy/aws","docId":"docs/deploy/aws"},{"type":"link","label":"Deploying to GCP","href":"/datahub-project-forked/docs/deploy/gcp","docId":"docs/deploy/gcp"},{"type":"link","label":"Deploying with Docker","href":"/datahub-project-forked/docs/docker","docId":"docker/README"},{"type":"link","label":"Deploying with Kubernetes","href":"/datahub-project-forked/docs/deploy/kubernetes","docId":"docs/deploy/kubernetes"},{"type":"category","label":"Authentication","items":[{"type":"link","label":"Overview","href":"/datahub-project-forked/docs/authentication","docId":"docs/authentication/README"},{"type":"link","label":"Concepts & Key Components","href":"/datahub-project-forked/docs/authentication/concepts","docId":"docs/authentication/concepts"},{"type":"category","label":"Frontend Authentication","items":[{"type":"link","label":"JaaS Authentication","href":"/datahub-project-forked/docs/authentication/guides/jaas","docId":"docs/authentication/guides/jaas"},{"type":"category","label":"OIDC Authentication","items":[{"type":"link","label":"Overview","href":"/datahub-project-forked/docs/authentication/guides/sso/configure-oidc-react","docId":"docs/authentication/guides/sso/configure-oidc-react"},{"type":"link","label":"Configuring Google Authentication for React App (OIDC)","href":"/datahub-project-forked/docs/authentication/guides/sso/configure-oidc-react-google","docId":"docs/authentication/guides/sso/configure-oidc-react-google"},{"type":"link","label":"Configuring Okta Authentication for React App (OIDC)","href":"/datahub-project-forked/docs/authentication/guides/sso/configure-oidc-react-okta","docId":"docs/authentication/guides/sso/configure-oidc-react-okta"},{"type":"link","label":"Configuring Azure Authentication for React App (OIDC)","href":"/datahub-project-forked/docs/authentication/guides/sso/configure-oidc-react-azure","docId":"docs/authentication/guides/sso/configure-oidc-react-azure"}],"collapsed":true,"collapsible":true}],"collapsed":true,"collapsible":true},{"type":"link","label":"Metadata Service Authentication","href":"/datahub-project-forked/docs/authentication/introducing-metadata-service-authentication","docId":"docs/authentication/introducing-metadata-service-authentication"},{"type":"link","label":"Personal Access Tokens","href":"/datahub-project-forked/docs/authentication/personal-access-tokens","docId":"docs/authentication/personal-access-tokens"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Authorization","items":[{"type":"link","label":"Overview","href":"/datahub-project-forked/docs/authorization","docId":"docs/authorization/README"},{"type":"link","label":"Roles","href":"/datahub-project-forked/docs/authorization/roles","docId":"docs/authorization/roles"},{"type":"link","label":"Policies Guide","href":"/datahub-project-forked/docs/authorization/policies","docId":"docs/authorization/policies"},{"type":"link","label":"Authorization using Groups","href":"/datahub-project-forked/docs/authorization/groups","docId":"docs/authorization/groups"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Advanced Guides","items":[{"type":"link","label":"Removing Metadata from DataHub","href":"/datahub-project-forked/docs/how/delete-metadata","docId":"docs/how/delete-metadata"},{"type":"link","label":"Configuring Authorization with Apache Ranger","href":"/datahub-project-forked/docs/how/configuring-authorization-with-apache-ranger","docId":"docs/how/configuring-authorization-with-apache-ranger"},{"type":"link","label":"Taking backup of DataHub","href":"/datahub-project-forked/docs/how/backup-datahub","docId":"docs/how/backup-datahub"},{"type":"link","label":"Restoring Search and Graph Indices from Local Database","href":"/datahub-project-forked/docs/how/restore-indices","docId":"docs/how/restore-indices"},{"type":"link","label":"Configuring Database Retention","href":"/datahub-project-forked/docs/advanced/db-retention","docId":"docs/advanced/db-retention"},{"type":"link","label":"Monitoring DataHub","href":"/datahub-project-forked/docs/advanced/monitoring","docId":"docs/advanced/monitoring"},{"type":"link","label":"How to Extract Logs from DataHub Containers","href":"/datahub-project-forked/docs/how/extract-container-logs","docId":"docs/how/extract-container-logs"},{"type":"link","label":"Telemetry","href":"/datahub-project-forked/docs/deploy/telemetry","docId":"docs/deploy/telemetry"},{"type":"link","label":"Configuring Kafka","href":"/datahub-project-forked/docs/how/kafka-config","docId":"docs/how/kafka-config"},{"type":"link","label":"Integrating with Confluent Cloud","href":"/datahub-project-forked/docs/deploy/confluent-cloud","docId":"docs/deploy/confluent-cloud"},{"type":"link","label":"No Code Upgrade (In-Place Migration Guide)","href":"/datahub-project-forked/docs/advanced/no-code-upgrade","docId":"docs/advanced/no-code-upgrade"}],"collapsed":true,"collapsible":true},{"type":"link","label":"Updating DataHub","href":"/datahub-project-forked/docs/how/updating-datahub","docId":"docs/how/updating-datahub"}],"collapsed":true,"collapsible":true},{"type":"category","label":"DataHub API","items":[{"type":"link","label":"Which DataHub API is for me?","href":"/datahub-project-forked/docs/api/datahub-apis","docId":"docs/api/datahub-apis"},{"type":"category","label":"GraphQL API","items":[{"type":"link","label":"Overview","href":"/datahub-project-forked/docs/api/graphql/overview","docId":"docs/api/graphql/overview"},{"type":"category","label":"Reference","items":[{"type":"link","label":"Queries","href":"/datahub-project-forked/docs/graphql/queries","docId":"graphql/queries"},{"type":"link","label":"Mutations","href":"/datahub-project-forked/docs/graphql/mutations","docId":"graphql/mutations"},{"type":"link","label":"Objects","href":"/datahub-project-forked/docs/graphql/objects","docId":"graphql/objects"},{"type":"link","label":"Inputs","href":"/datahub-project-forked/docs/graphql/inputObjects","docId":"graphql/inputObjects"},{"type":"link","label":"Interfaces","href":"/datahub-project-forked/docs/graphql/interfaces","docId":"graphql/interfaces"},{"type":"link","label":"Unions","href":"/datahub-project-forked/docs/graphql/unions","docId":"graphql/unions"},{"type":"link","label":"Enums","href":"/datahub-project-forked/docs/graphql/enums","docId":"graphql/enums"},{"type":"link","label":"Scalars","href":"/datahub-project-forked/docs/graphql/scalars","docId":"graphql/scalars"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Guides","items":[{"type":"link","label":"Getting Started","href":"/datahub-project-forked/docs/api/graphql/getting-started","docId":"docs/api/graphql/getting-started"},{"type":"link","label":"Working with Metadata Entities","href":"/datahub-project-forked/docs/api/graphql/querying-entities","docId":"docs/api/graphql/querying-entities"},{"type":"link","label":"Access Token Management","href":"/datahub-project-forked/docs/api/graphql/token-management","docId":"docs/api/graphql/token-management"}],"collapsed":true,"collapsible":true}],"collapsed":true,"collapsible":true},{"type":"category","label":"OpenAPI","items":[{"type":"link","label":"OpenAPI Guide","href":"/datahub-project-forked/docs/api/openapi/openapi-usage-guide","docId":"docs/api/openapi/openapi-usage-guide"},{"type":"link","label":"Timeline API","href":"/datahub-project-forked/docs/dev-guides/timeline","docId":"docs/dev-guides/timeline"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Rest.li","items":[{"type":"link","label":"Rest.li API","href":"/datahub-project-forked/docs/api/restli/restli-overview","docId":"docs/api/restli/restli-overview"},{"type":"link","label":"Restore Indices","href":"/datahub-project-forked/docs/api/restli/restore-indices","docId":"docs/api/restli/restore-indices"},{"type":"link","label":"Aspect Versioning and Rest.li Modeling","href":"/datahub-project-forked/docs/advanced/aspect-versioning","docId":"docs/advanced/aspect-versioning"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Getting Started: APIs & SDKs","items":[{"type":"category","label":"Creating Entities","items":[{"type":"link","label":"Creating Datasets","href":"/datahub-project-forked/docs/api/tutorials/creating-datasets","docId":"docs/api/tutorials/creating-datasets"},{"type":"link","label":"Creating Tags","href":"/datahub-project-forked/docs/api/tutorials/creating-tags","docId":"docs/api/tutorials/creating-tags"},{"type":"link","label":"Creating Terms","href":"/datahub-project-forked/docs/api/tutorials/creating-terms","docId":"docs/api/tutorials/creating-terms"},{"type":"link","label":"Creating or Updating Users And Groups","href":"/datahub-project-forked/docs/api/tutorials/creating-users-and-groups","docId":"docs/api/tutorials/creating-users-and-groups"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Enriching Entities","items":[{"type":"link","label":"Adding Tags On Datasets/Columns","href":"/datahub-project-forked/docs/api/tutorials/adding-tags","docId":"docs/api/tutorials/adding-tags"},{"type":"link","label":"Adding Terms On Datasets/Columns","href":"/datahub-project-forked/docs/api/tutorials/adding-terms","docId":"docs/api/tutorials/adding-terms"},{"type":"link","label":"Adding Owners On Datasets/Columns","href":"/datahub-project-forked/docs/api/tutorials/adding-ownerships","docId":"docs/api/tutorials/adding-ownerships"},{"type":"link","label":"Adding Description on Datasets","href":"/datahub-project-forked/docs/api/tutorials/adding-dataset-description","docId":"docs/api/tutorials/adding-dataset-description"},{"type":"link","label":"Adding Description on Columns","href":"/datahub-project-forked/docs/api/tutorials/adding-column-description","docId":"docs/api/tutorials/adding-column-description"},{"type":"link","label":"Adding Lineage","href":"/datahub-project-forked/docs/api/tutorials/adding-lineage","docId":"docs/api/tutorials/adding-lineage"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Deleting Entities","items":[{"type":"link","label":"Deleting Entities By Urn","href":"/datahub-project-forked/docs/api/tutorials/deleting-entities-by-urn","docId":"docs/api/tutorials/deleting-entities-by-urn"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Reference","items":[{"type":"link","label":"Generate Access Token","href":"/datahub-project-forked/docs/api/tutorials/references/generate-access-token","docId":"docs/api/tutorials/references/generate-access-token"},{"type":"link","label":"Preparing Your Local DataHub Environment","href":"/datahub-project-forked/docs/api/tutorials/references/prepare-datahub","docId":"docs/api/tutorials/references/prepare-datahub"}],"collapsed":true,"collapsible":true}],"collapsed":true,"collapsible":true}],"collapsed":true,"collapsible":true},{"type":"category","label":"Tools","items":[{"type":"link","label":"CLI","href":"/datahub-project-forked/docs/cli","docId":"docs/cli"},{"type":"category","label":"SDKs","items":[{"type":"link","label":"Python Emitter","href":"/datahub-project-forked/docs/metadata-ingestion/as-a-library","docId":"metadata-ingestion/as-a-library"},{"type":"link","label":"Java Emitter","href":"/datahub-project-forked/docs/metadata-integration/java/as-a-library","docId":"metadata-integration/java/as-a-library"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Python SDK Reference","items":[{"type":"link","label":"Builder","href":"/datahub-project-forked/docs/python-sdk/builder","docId":"python-sdk/builder"},{"type":"link","label":"Client","href":"/datahub-project-forked/docs/python-sdk/clients","docId":"python-sdk/clients"},{"type":"link","label":"Models","href":"/datahub-project-forked/docs/python-sdk/models","docId":"python-sdk/models"}],"collapsed":true,"collapsible":true},{"type":"link","label":"Lite (Experimental)","href":"/datahub-project-forked/docs/datahub_lite","docId":"docs/datahub_lite"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Developer Guides","items":[{"type":"category","label":"DataHub Metadata Model","items":[{"type":"link","label":"The Metadata Model","href":"/datahub-project-forked/docs/metadata-modeling/metadata-model","docId":"docs/modeling/metadata-model"},{"type":"link","label":"Extending the Metadata Model","href":"/datahub-project-forked/docs/metadata-modeling/extending-the-metadata-model","docId":"docs/modeling/extending-the-metadata-model"},{"type":"link","label":"Metadata Events","href":"/datahub-project-forked/docs/what/mxe","docId":"docs/what/mxe"},{"type":"category","label":"Entities","items":[{"type":"link","label":"Data Platform","href":"/datahub-project-forked/docs/generated/metamodel/entities/dataplatform","docId":"docs/generated/metamodel/entities/dataPlatform"},{"type":"link","label":"Dataset","href":"/datahub-project-forked/docs/generated/metamodel/entities/dataset","docId":"docs/generated/metamodel/entities/dataset"},{"type":"link","label":"DataJob","href":"/datahub-project-forked/docs/generated/metamodel/entities/datajob","docId":"docs/generated/metamodel/entities/dataJob"},{"type":"link","label":"DataFlow","href":"/datahub-project-forked/docs/generated/metamodel/entities/dataflow","docId":"docs/generated/metamodel/entities/dataFlow"},{"type":"link","label":"DataProcessInstance","href":"/datahub-project-forked/docs/generated/metamodel/entities/dataprocessinstance","docId":"docs/generated/metamodel/entities/dataProcessInstance"},{"type":"link","label":"Chart","href":"/datahub-project-forked/docs/generated/metamodel/entities/chart","docId":"docs/generated/metamodel/entities/chart"},{"type":"link","label":"Dashboard","href":"/datahub-project-forked/docs/generated/metamodel/entities/dashboard","docId":"docs/generated/metamodel/entities/dashboard"},{"type":"link","label":"Notebook","href":"/datahub-project-forked/docs/generated/metamodel/entities/notebook","docId":"docs/generated/metamodel/entities/notebook"},{"type":"link","label":"Corpuser","href":"/datahub-project-forked/docs/generated/metamodel/entities/corpuser","docId":"docs/generated/metamodel/entities/corpuser"},{"type":"link","label":"CorpGroup","href":"/datahub-project-forked/docs/generated/metamodel/entities/corpgroup","docId":"docs/generated/metamodel/entities/corpGroup"},{"type":"link","label":"Domain","href":"/datahub-project-forked/docs/generated/metamodel/entities/domain","docId":"docs/generated/metamodel/entities/domain"},{"type":"link","label":"Container","href":"/datahub-project-forked/docs/generated/metamodel/entities/container","docId":"docs/generated/metamodel/entities/container"},{"type":"link","label":"Tag","href":"/datahub-project-forked/docs/generated/metamodel/entities/tag","docId":"docs/generated/metamodel/entities/tag"},{"type":"link","label":"GlossaryTerm","href":"/datahub-project-forked/docs/generated/metamodel/entities/glossaryterm","docId":"docs/generated/metamodel/entities/glossaryTerm"},{"type":"link","label":"GlossaryNode","href":"/datahub-project-forked/docs/generated/metamodel/entities/glossarynode","docId":"docs/generated/metamodel/entities/glossaryNode"},{"type":"link","label":"Assertion","href":"/datahub-project-forked/docs/generated/metamodel/entities/assertion","docId":"docs/generated/metamodel/entities/assertion"},{"type":"link","label":"MlModel","href":"/datahub-project-forked/docs/generated/metamodel/entities/mlmodel","docId":"docs/generated/metamodel/entities/mlModel"},{"type":"link","label":"MlModelGroup","href":"/datahub-project-forked/docs/generated/metamodel/entities/mlmodelgroup","docId":"docs/generated/metamodel/entities/mlModelGroup"},{"type":"link","label":"MlFeatureTable","href":"/datahub-project-forked/docs/generated/metamodel/entities/mlfeaturetable","docId":"docs/generated/metamodel/entities/mlFeatureTable"},{"type":"link","label":"MlFeature","href":"/datahub-project-forked/docs/generated/metamodel/entities/mlfeature","docId":"docs/generated/metamodel/entities/mlFeature"},{"type":"link","label":"MlPrimaryKey","href":"/datahub-project-forked/docs/generated/metamodel/entities/mlprimarykey","docId":"docs/generated/metamodel/entities/mlPrimaryKey"},{"type":"link","label":"Test","href":"/datahub-project-forked/docs/generated/metamodel/entities/test","docId":"docs/generated/metamodel/entities/test"},{"type":"link","label":"InviteToken","href":"/datahub-project-forked/docs/generated/metamodel/entities/invitetoken","docId":"docs/generated/metamodel/entities/inviteToken"},{"type":"link","label":"SchemaField","href":"/datahub-project-forked/docs/generated/metamodel/entities/schemafield","docId":"docs/generated/metamodel/entities/schemaField"},{"type":"link","label":"DataHubRole","href":"/datahub-project-forked/docs/generated/metamodel/entities/datahubrole","docId":"docs/generated/metamodel/entities/dataHubRole"},{"type":"link","label":"Post","href":"/datahub-project-forked/docs/generated/metamodel/entities/post","docId":"docs/generated/metamodel/entities/post"},{"type":"link","label":"DataHubStepState","href":"/datahub-project-forked/docs/generated/metamodel/entities/datahubstepstate","docId":"docs/generated/metamodel/entities/dataHubStepState"},{"type":"link","label":"DataHubView","href":"/datahub-project-forked/docs/generated/metamodel/entities/datahubview","docId":"docs/generated/metamodel/entities/dataHubView"},{"type":"link","label":"Query","href":"/datahub-project-forked/docs/generated/metamodel/entities/query","docId":"docs/generated/metamodel/entities/query"},{"type":"link","label":"DataProcess","href":"/datahub-project-forked/docs/generated/metamodel/entities/dataprocess","docId":"docs/generated/metamodel/entities/dataProcess"},{"type":"link","label":"MlModelDeployment","href":"/datahub-project-forked/docs/generated/metamodel/entities/mlmodeldeployment","docId":"docs/generated/metamodel/entities/mlModelDeployment"},{"type":"link","label":"DataHubPolicy","href":"/datahub-project-forked/docs/generated/metamodel/entities/datahubpolicy","docId":"docs/generated/metamodel/entities/dataHubPolicy"},{"type":"link","label":"DataHubIngestionSource","href":"/datahub-project-forked/docs/generated/metamodel/entities/datahubingestionsource","docId":"docs/generated/metamodel/entities/dataHubIngestionSource"},{"type":"link","label":"DataHubSecret","href":"/datahub-project-forked/docs/generated/metamodel/entities/datahubsecret","docId":"docs/generated/metamodel/entities/dataHubSecret"},{"type":"link","label":"DataHubExecutionRequest","href":"/datahub-project-forked/docs/generated/metamodel/entities/datahubexecutionrequest","docId":"docs/generated/metamodel/entities/dataHubExecutionRequest"},{"type":"link","label":"DataHubRetention","href":"/datahub-project-forked/docs/generated/metamodel/entities/datahubretention","docId":"docs/generated/metamodel/entities/dataHubRetention"},{"type":"link","label":"DataPlatformInstance","href":"/datahub-project-forked/docs/generated/metamodel/entities/dataplatforminstance","docId":"docs/generated/metamodel/entities/dataPlatformInstance"},{"type":"link","label":"Telemetry","href":"/datahub-project-forked/docs/generated/metamodel/entities/telemetry","docId":"docs/generated/metamodel/entities/telemetry"},{"type":"link","label":"DataHubAccessToken","href":"/datahub-project-forked/docs/generated/metamodel/entities/datahubaccesstoken","docId":"docs/generated/metamodel/entities/dataHubAccessToken"},{"type":"link","label":"DataHubUpgrade","href":"/datahub-project-forked/docs/generated/metamodel/entities/datahubupgrade","docId":"docs/generated/metamodel/entities/dataHubUpgrade"},{"type":"link","label":"GlobalSettings","href":"/datahub-project-forked/docs/generated/metamodel/entities/globalsettings","docId":"docs/generated/metamodel/entities/globalSettings"}],"collapsed":true,"collapsible":true}],"collapsed":true,"collapsible":true},{"type":"category","label":"Developing on DataHub","items":[{"type":"link","label":"Local Development","href":"/datahub-project-forked/docs/developers","docId":"docs/developers"},{"type":"link","label":"Using Docker Images During Development","href":"/datahub-project-forked/docs/docker/development","docId":"docs/docker/development"},{"type":"link","label":"Developing on Metadata Ingestion","href":"/datahub-project-forked/docs/metadata-ingestion/developing","docId":"metadata-ingestion/developing"},{"type":"category","label":"Modules","items":[{"type":"link","label":"datahub-web-react","href":"/datahub-project-forked/docs/datahub-web-react","docId":"datahub-web-react/README"},{"type":"link","label":"datahub-frontend","href":"/datahub-project-forked/docs/datahub-frontend","docId":"datahub-frontend/README"},{"type":"link","label":"datahub-graphql-core","href":"/datahub-project-forked/docs/datahub-graphql-core","docId":"datahub-graphql-core/README"},{"type":"link","label":"metadata-service","href":"/datahub-project-forked/docs/metadata-service","docId":"metadata-service/README"},{"type":"link","label":"metadata-jobs:mae-consumer-job","href":"/datahub-project-forked/docs/metadata-jobs/mae-consumer-job","docId":"metadata-jobs/mae-consumer-job/README"},{"type":"link","label":"metadata-jobs:mce-consumer-job","href":"/datahub-project-forked/docs/metadata-jobs/mce-consumer-job","docId":"metadata-jobs/mce-consumer-job/README"}],"collapsed":true,"collapsible":true}],"collapsed":true,"collapsible":true},{"type":"link","label":"Quickstart Debugging Guide","href":"/datahub-project-forked/docs/debugging","docId":"docs/debugging"},{"type":"link","label":"FAQ Using DataHub","href":"/datahub-project-forked/docs/faq-using-datahub","docId":"docs/faq-using-datahub"},{"type":"link","label":"Plugins Guide","href":"/datahub-project-forked/docs/plugins","docId":"docs/plugins"},{"type":"category","label":"Advanced","items":[{"type":"link","label":"Datahub\'s Reporting Framework for Ingestion Job Telemetry","href":"/datahub-project-forked/docs/metadata-ingestion/docs/dev_guides/reporting_telemetry","docId":"metadata-ingestion/docs/dev_guides/reporting_telemetry"},{"type":"link","label":"MetadataChangeProposal & MetadataChangeLog Events","href":"/datahub-project-forked/docs/advanced/mcp-mcl","docId":"docs/advanced/mcp-mcl"},{"type":"link","label":"Upgrade Docker Image","href":"/datahub-project-forked/docs/docker/datahub-upgrade","docId":"docker/datahub-upgrade/README"},{"type":"link","label":"No Code Metadata","href":"/datahub-project-forked/docs/advanced/no-code-modeling","docId":"docs/advanced/no-code-modeling"},{"type":"link","label":"React Analytics","href":"/datahub-project-forked/docs/datahub-web-react/src/app/analytics","docId":"datahub-web-react/src/app/analytics/README"},{"type":"link","label":"Elasticsearch upgrade from 5.6.8 to 7.9.3","href":"/datahub-project-forked/docs/advanced/es-7-upgrade","docId":"docs/advanced/es-7-upgrade"},{"type":"link","label":"Migrate Graph Service Implementation to Elasticsearch","href":"/datahub-project-forked/docs/how/migrating-graph-service-implementation","docId":"docs/how/migrating-graph-service-implementation"},{"type":"link","label":"SchemaFieldPath Specification (Version 2)","href":"/datahub-project-forked/docs/advanced/field-path-spec-v2","docId":"docs/advanced/field-path-spec-v2"},{"type":"link","label":"Adding a Metadata Ingestion Source","href":"/datahub-project-forked/docs/metadata-ingestion/adding-source","docId":"metadata-ingestion/adding-source"},{"type":"link","label":"Using a Custom Ingestion Source","href":"/datahub-project-forked/docs/how/add-custom-ingestion-source","docId":"docs/how/add-custom-ingestion-source"},{"type":"link","label":"Adding a custom Dataset Data Platform","href":"/datahub-project-forked/docs/how/add-custom-data-platform","docId":"docs/how/add-custom-data-platform"},{"type":"link","label":"Browse Paths Upgrade (August 2022)","href":"/datahub-project-forked/docs/advanced/browse-paths-upgrade","docId":"docs/advanced/browse-paths-upgrade"}],"collapsed":true,"collapsible":true}],"collapsed":true,"collapsible":true},{"type":"category","label":"Feature Guides","items":[{"type":"link","label":"Search","href":"/datahub-project-forked/docs/how/search","docId":"docs/how/search"},{"type":"link","label":"Schema History","href":"/datahub-project-forked/docs/schema-history","docId":"docs/schema-history"},{"type":"link","label":"Domains","href":"/datahub-project-forked/docs/domains","docId":"docs/domains"},{"type":"link","label":"Business Glossary","href":"/datahub-project-forked/docs/glossary/business-glossary","docId":"docs/glossary/business-glossary"},{"type":"link","label":"Tags","href":"/datahub-project-forked/docs/tags","docId":"docs/tags"},{"type":"link","label":"Browse","href":"/datahub-project-forked/docs/browse","docId":"docs/browse"},{"type":"link","label":"Access Policies","href":"/datahub-project-forked/docs/authorization/access-policies-guide","docId":"docs/authorization/access-policies-guide"},{"type":"link","label":"Dataset Usage & Query History","href":"/datahub-project-forked/docs/features/dataset-usage-and-query-history","docId":"docs/features/dataset-usage-and-query-history"},{"type":"link","label":"Posts","href":"/datahub-project-forked/docs/posts","docId":"docs/posts"},{"type":"link","label":"Sync Status","href":"/datahub-project-forked/docs/sync-status","docId":"docs/sync-status"},{"type":"link","label":"[Stemming and Synonyms Support]","href":"/datahub-project-forked/docs/architecture/stemming_and_synonyms","docId":"docs/architecture/stemming_and_synonyms"},{"type":"link","label":"Lineage","href":"/datahub-project-forked/docs/lineage/lineage-feature-guide","docId":"docs/lineage/lineage-feature-guide"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Join the Community","items":[{"type":"link","label":"Slack","href":"/datahub-project-forked/docs/slack","docId":"docs/slack"},{"type":"link","label":"Town Halls","href":"/datahub-project-forked/docs/townhalls","docId":"docs/townhalls"},{"type":"link","label":"Town Hall History","href":"/datahub-project-forked/docs/townhall-history","docId":"docs/townhall-history"},{"type":"link","label":"Contributor Covenant Code of Conduct","href":"/datahub-project-forked/docs/code_of_conduct","docId":"docs/CODE_OF_CONDUCT"},{"type":"link","label":"Contributing","href":"/datahub-project-forked/docs/contributing","docId":"docs/CONTRIBUTING"},{"type":"link","label":"Articles & Talks","href":"/datahub-project-forked/docs/links","docId":"docs/links"},{"type":"link","label":"RFC Process","href":"/datahub-project-forked/docs/rfc","docId":"docs/rfc"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Managed DataHub","items":[{"type":"link","label":"Managed DataHub Exclusives","href":"/datahub-project-forked/docs/managed-datahub/managed-datahub-overview","docId":"docs/managed-datahub/managed-datahub-overview"},{"type":"link","label":"Getting Started with Acryl DataHub","href":"/datahub-project-forked/docs/managed-datahub/welcome-acryl","docId":"docs/managed-datahub/welcome-acryl"},{"type":"category","label":"Metadata Ingestion With Acryl","items":[{"type":"link","label":"Ingestion","href":"/datahub-project-forked/docs/managed-datahub/metadata-ingestion-with-acryl/ingestion","docId":"docs/managed-datahub/metadata-ingestion-with-acryl/ingestion"}],"collapsed":true,"collapsible":true},{"type":"category","label":"DataHub API","items":[{"type":"link","label":"Entity Events API","href":"/datahub-project-forked/docs/managed-datahub/datahub-api/entity-events-api","className":"saasOnly","docId":"docs/managed-datahub/datahub-api/entity-events-api"},{"type":"category","label":"GraphQL API","items":[{"type":"link","label":"Getting Started","href":"/datahub-project-forked/docs/managed-datahub/datahub-api/graphql-api/getting-started","docId":"docs/managed-datahub/datahub-api/graphql-api/getting-started"},{"type":"link","label":"Incidents API (Beta)","href":"/datahub-project-forked/docs/managed-datahub/datahub-api/graphql-api/incidents-api-beta","className":"saasOnly","docId":"docs/managed-datahub/datahub-api/graphql-api/incidents-api-beta"}],"collapsed":true,"collapsible":true}],"collapsed":true,"collapsible":true},{"type":"category","label":"Integrations","items":[{"type":"link","label":"AWS PrivateLink","href":"/datahub-project-forked/docs/managed-datahub/integrations/aws-privatelink","className":"saasOnly","docId":"docs/managed-datahub/integrations/aws-privatelink"},{"type":"link","label":"OIDC SSO Integration","href":"/datahub-project-forked/docs/managed-datahub/integrations/oidc-sso-integration","className":"saasOnly","docId":"docs/managed-datahub/integrations/oidc-sso-integration"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Operator Guide","items":[{"type":"link","label":"Setting up Remote Ingestion Executor on AWS","href":"/datahub-project-forked/docs/managed-datahub/operator-guide/setting-up-remote-ingestion-executor-on-aws","className":"saasOnly","docId":"docs/managed-datahub/operator-guide/setting-up-remote-ingestion-executor-on-aws"},{"type":"link","label":"Setting up Events API on AWS EventBridge","href":"/datahub-project-forked/docs/managed-datahub/operator-guide/setting-up-events-api-on-aws-eventbridge","className":"saasOnly","docId":"docs/managed-datahub/operator-guide/setting-up-events-api-on-aws-eventbridge"}],"collapsed":true,"collapsible":true},{"type":"link","label":"Local Chrome Extension","href":"/datahub-project-forked/docs/managed-datahub/chrome-extension","className":"saasOnly","docId":"docs/managed-datahub/chrome-extension"},{"type":"category","label":"Managed DataHub Release History","items":[{"type":"link","label":"v0.2.4","href":"/datahub-project-forked/docs/managed-datahub/release-notes/v_0_2_4","docId":"docs/managed-datahub/release-notes/v_0_2_4"},{"type":"link","label":"v0.2.3","href":"/datahub-project-forked/docs/managed-datahub/release-notes/v_0_2_3","docId":"docs/managed-datahub/release-notes/v_0_2_3"},{"type":"link","label":"v0.2.2","href":"/datahub-project-forked/docs/managed-datahub/release-notes/v_0_2_2","docId":"docs/managed-datahub/release-notes/v_0_2_2"},{"type":"link","label":"v0.2.1","href":"/datahub-project-forked/docs/managed-datahub/release-notes/v_0_2_1","docId":"docs/managed-datahub/release-notes/v_0_2_1"},{"type":"link","label":"v0.2.0","href":"/datahub-project-forked/docs/managed-datahub/release-notes/v_0_2_0","docId":"docs/managed-datahub/release-notes/v_0_2_0"},{"type":"link","label":"v0.1.73","href":"/datahub-project-forked/docs/managed-datahub/release-notes/v_0_1_73","docId":"docs/managed-datahub/release-notes/v_0_1_73"},{"type":"link","label":"v0.1.72","href":"/datahub-project-forked/docs/managed-datahub/release-notes/v_0_1_72","docId":"docs/managed-datahub/release-notes/v_0_1_72"},{"type":"link","label":"v0.1.70","href":"/datahub-project-forked/docs/managed-datahub/release-notes/v_0_1_70","docId":"docs/managed-datahub/release-notes/v_0_1_70"},{"type":"link","label":"v0.1.69","href":"/datahub-project-forked/docs/managed-datahub/release-notes/v_0_1_69","docId":"docs/managed-datahub/release-notes/v_0_1_69"}],"collapsed":true,"collapsible":true}],"collapsed":true,"collapsible":true},{"type":"category","label":"Release History","items":[{"type":"link","label":"Releases","href":"/datahub-project-forked/docs/releases","docId":"releases"}],"collapsed":true,"collapsible":true}]},"docs":{"datahub-frontend/README":{"id":"datahub-frontend/README","title":"datahub-frontend","description":"DataHub frontend is a Play service written in Java. It is served as a mid-tier","sidebar":"overviewSidebar"},"datahub-graphql-core/README":{"id":"datahub-graphql-core/README","title":"datahub-graphql-core","description":"DataHub GraphQL API is a shared lib module containing a GraphQL API on top of the GMS service layer. It exposes a graph-based representation","sidebar":"overviewSidebar"},"datahub-web-react/README":{"id":"datahub-web-react/README","title":"datahub-web-react","description":"About","sidebar":"overviewSidebar"},"datahub-web-react/src/app/analytics/README":{"id":"datahub-web-react/src/app/analytics/README","title":"DataHub React Analytics","description":"About","sidebar":"overviewSidebar"},"docker/airflow/local_airflow":{"id":"docker/airflow/local_airflow","title":"Running Airflow locally with DataHub","description":"This feature is currently unmaintained. As of 0.10.0 the container described is not published alongside the DataHub CLI. If you\'d like to use it, please reach out to us on the community slack."},"docker/datahub-upgrade/README":{"id":"docker/datahub-upgrade/README","title":"DataHub Upgrade Docker Image","description":"This container is used to automatically apply upgrades from one version of DataHub to another.","sidebar":"overviewSidebar"},"docker/README":{"id":"docker/README","title":"Deploying with Docker","description":"Prerequisites","sidebar":"overviewSidebar"},"docs/act-on-metadata":{"id":"docs/act-on-metadata","title":"Act on Metadata Overview","description":"DataHub\'s metadata infrastructure is stream-oriented, meaning that all changes in metadata are communicated and reflected within the platform within seconds.","sidebar":"overviewSidebar"},"docs/act-on-metadata/impact-analysis":{"id":"docs/act-on-metadata/impact-analysis","title":"About DataHub Lineage Impact Analysis","description":"Lineage Impact Analysis is a powerful workflow for understanding the complete set of upstream and downstream dependencies of a Dataset, Dashboard, Chart, and many other DataHub Entities.","sidebar":"overviewSidebar"},"docs/actions/actions/executor":{"id":"docs/actions/actions/executor","title":"Ingestion Executor","description":"Certified","sidebar":"overviewSidebar"},"docs/actions/actions/hello_world":{"id":"docs/actions/actions/hello_world","title":"Hello World","description":"Certified","sidebar":"overviewSidebar"},"docs/actions/actions/slack":{"id":"docs/actions/actions/slack","title":"Slack","description":"|  |  |","sidebar":"overviewSidebar"},"docs/actions/actions/teams":{"id":"docs/actions/actions/teams","title":"Microsoft Teams","description":"|  |  |","sidebar":"overviewSidebar"},"docs/actions/concepts":{"id":"docs/actions/concepts","title":"Concepts","description":"The Actions framework includes pluggable components for filtering, transforming, and reacting to important DataHub, such as","sidebar":"overviewSidebar"},"docs/actions/events/entity-change-event":{"id":"docs/actions/events/entity-change-event","title":"Entity Change Event V1","description":"Event Type","sidebar":"overviewSidebar"},"docs/actions/events/metadata-change-log-event":{"id":"docs/actions/events/metadata-change-log-event","title":"Metadata Change Log Event V1","description":"Event Type","sidebar":"overviewSidebar"},"docs/actions/guides/developing-a-transformer":{"id":"docs/actions/guides/developing-a-transformer","title":"Developing a Transformer","description":"In this guide, we will outline each step to developing a custom Transformer for the DataHub Actions Framework.","sidebar":"overviewSidebar"},"docs/actions/guides/developing-an-action":{"id":"docs/actions/guides/developing-an-action","title":"Developing an Action","description":"In this guide, we will outline each step to developing a Action for the DataHub Actions Framework.","sidebar":"overviewSidebar"},"docs/actions/quickstart":{"id":"docs/actions/quickstart","title":"Quickstart","description":"Prerequisites","sidebar":"overviewSidebar"},"docs/actions/README":{"id":"docs/actions/README","title":"Introduction","description":"Welcome to DataHub Actions! The Actions framework makes responding to realtime changes in your Metadata Graph easy, enabling you to seamlessly integrate DataHub into a broader events-based architecture.","sidebar":"overviewSidebar"},"docs/actions/sources/kafka-event-source":{"id":"docs/actions/sources/kafka-event-source","title":"Kafka Event Source","description":"Overview","sidebar":"overviewSidebar"},"docs/advanced/aspect-versioning":{"id":"docs/advanced/aspect-versioning","title":"Aspect Versioning","description":"As each version of metadata aspect is immutable, any update to an existing aspect results in the creation of a new version. Typically one would expect the version number increases sequentially with the largest version number being the latest version, i.e. v1 (oldest), v2 (second oldest), ..., vN (latest). However, this approach results in major challenges in both rest.li modeling & transaction isolation and therefore requires a rethinking.","sidebar":"overviewSidebar"},"docs/advanced/backfilling":{"id":"docs/advanced/backfilling","title":"Backfilling Search Index & Graph DB","description":"WIP"},"docs/advanced/browse-paths-upgrade":{"id":"docs/advanced/browse-paths-upgrade","title":"Browse Paths Upgrade (August 2022)","description":"Background","sidebar":"overviewSidebar"},"docs/advanced/db-retention":{"id":"docs/advanced/db-retention","title":"Configuring Database Retention","description":"Goal","sidebar":"overviewSidebar"},"docs/advanced/derived-aspects":{"id":"docs/advanced/derived-aspects","title":"Derived Aspects","description":"WIP"},"docs/advanced/entity-hierarchy":{"id":"docs/advanced/entity-hierarchy","title":"Entity Hierarchy","description":"WIP"},"docs/advanced/es-7-upgrade":{"id":"docs/advanced/es-7-upgrade","title":"Elasticsearch upgrade from 5.6.8 to 7.9.3","description":"Summary of changes","sidebar":"overviewSidebar"},"docs/advanced/field-path-spec-v2":{"id":"docs/advanced/field-path-spec-v2","title":"SchemaFieldPath Specification (Version 2)","description":"This document outlines the formal specification for the fieldPath member of","sidebar":"overviewSidebar"},"docs/advanced/high-cardinality":{"id":"docs/advanced/high-cardinality","title":"High Cardinality Relationships","description":"As explained in What is a Relationship, the raw metadata for forming relationships is captured directly inside of a Metadata Aspect. The most natural way to model this is using an array, e.g. a group membership aspect contains an array of user URNs. However, this poses some challenges when the cardinality of the relationship is expected to be large (say, greater than 10,000). The aspect becomes large in size, which leads to slow update and retrieval. It may even exceed the underlying limit of the document store, which is often in the range of a few MBs. Furthermore, sending large messages (> 1MB) over Kafka requires special tuning and is generally discouraged."},"docs/advanced/mcp-mcl":{"id":"docs/advanced/mcp-mcl","title":"MetadataChangeProposal & MetadataChangeLog Events","description":"Overview & Vision","sidebar":"overviewSidebar"},"docs/advanced/monitoring":{"id":"docs/advanced/monitoring","title":"Monitoring DataHub","description":"Monitoring DataHub\'s system components is critical for operating and improving DataHub. This doc explains how to add","sidebar":"overviewSidebar"},"docs/advanced/no-code-modeling":{"id":"docs/advanced/no-code-modeling","title":"No Code Metadata","description":"Summary of changes","sidebar":"overviewSidebar"},"docs/advanced/no-code-upgrade":{"id":"docs/advanced/no-code-upgrade","title":"No Code Upgrade (In-Place Migration Guide)","description":"Summary of changes","sidebar":"overviewSidebar"},"docs/advanced/partial-update":{"id":"docs/advanced/partial-update","title":"Supporting Partial Aspect Update","description":"WIP"},"docs/advanced/pdl-best-practices":{"id":"docs/advanced/pdl-best-practices","title":"PDL Best Practices","description":"WIP"},"docs/api/datahub-apis":{"id":"docs/api/datahub-apis","title":"Which DataHub API is for me?","description":"DataHub supplys several APIs to manipulate metadata on the platform. These are our most-to-least recommended approaches:","sidebar":"overviewSidebar"},"docs/api/graphql/getting-started":{"id":"docs/api/graphql/getting-started","title":"Getting Started","description":"Get started using the DataHub GraphQL API.","sidebar":"overviewSidebar"},"docs/api/graphql/overview":{"id":"docs/api/graphql/overview","title":"DataHub GraphQL API","description":"DataHub provides a rich GraphQL API for programmatically interacting with the Entities & Relationships comprising your organization\'s Metadata Graph.","sidebar":"overviewSidebar"},"docs/api/graphql/querying-entities":{"id":"docs/api/graphql/querying-entities","title":"Working with Metadata Entities","description":"Learn how to find, retrieve & update entities comprising your Metadata Graph programmatically.","sidebar":"overviewSidebar"},"docs/api/graphql/token-management":{"id":"docs/api/graphql/token-management","title":"Access Token Management","description":"DataHub provides the following GraphQL endpoints for managing Access Tokens. In this page you will see examples as well","sidebar":"overviewSidebar"},"docs/api/openapi/openapi-usage-guide":{"id":"docs/api/openapi/openapi-usage-guide","title":"DataHub OpenAPI Guide","description":"Why OpenAPI","sidebar":"overviewSidebar"},"docs/api/restli/restli-overview":{"id":"docs/api/restli/restli-overview","title":"Rest.li API","description":"You can access basic documentation on the API endpoints by opening the /restli/docs endpoint in the browser.","sidebar":"overviewSidebar"},"docs/api/restli/restore-indices":{"id":"docs/api/restli/restore-indices","title":"Restore Indices Endpoint","description":"You can do a HTTP POST request to /gms/aspects?action=restoreIndices endpoint with the urn as part of JSON Payload to restore indices for the particular URN, or with the urnLike regex to restore for batchSize URNs matching the pattern starting from start.","sidebar":"overviewSidebar"},"docs/api/tutorials/adding-column-description":{"id":"docs/api/tutorials/adding-column-description","title":"Adding Description on Columns","description":"Why Would You Add Description on Columns?","sidebar":"overviewSidebar"},"docs/api/tutorials/adding-dataset-description":{"id":"docs/api/tutorials/adding-dataset-description","title":"Adding Description on Datasets","description":"Why Would You Add Description on Dataset?","sidebar":"overviewSidebar"},"docs/api/tutorials/adding-lineage":{"id":"docs/api/tutorials/adding-lineage","title":"Adding Lineage","description":"Why Would You Add Lineage?","sidebar":"overviewSidebar"},"docs/api/tutorials/adding-ownerships":{"id":"docs/api/tutorials/adding-ownerships","title":"Adding Owners On Datasets/Columns","description":"Why Would You Add Owners?","sidebar":"overviewSidebar"},"docs/api/tutorials/adding-tags":{"id":"docs/api/tutorials/adding-tags","title":"Adding Tags On Datasets/Columns","description":"Why Would You Add Tags?","sidebar":"overviewSidebar"},"docs/api/tutorials/adding-terms":{"id":"docs/api/tutorials/adding-terms","title":"Adding Terms On Datasets/Columns","description":"Why Would You Add Terms?","sidebar":"overviewSidebar"},"docs/api/tutorials/creating-datasets":{"id":"docs/api/tutorials/creating-datasets","title":"Creating Datasets","description":"Why Would You Create Datasets?","sidebar":"overviewSidebar"},"docs/api/tutorials/creating-tags":{"id":"docs/api/tutorials/creating-tags","title":"Creating Tags","description":"Why Would You Create Tags?","sidebar":"overviewSidebar"},"docs/api/tutorials/creating-terms":{"id":"docs/api/tutorials/creating-terms","title":"Creating Terms","description":"Why Would You Create Terms?","sidebar":"overviewSidebar"},"docs/api/tutorials/creating-users-and-groups":{"id":"docs/api/tutorials/creating-users-and-groups","title":"Creating or Updating Users And Groups","description":"Why Would You Create or Update Users and Groups?","sidebar":"overviewSidebar"},"docs/api/tutorials/deleting-entities-by-urn":{"id":"docs/api/tutorials/deleting-entities-by-urn","title":"Deleting Entities By Urn","description":"Why Would You Delete Entities?","sidebar":"overviewSidebar"},"docs/api/tutorials/references/generate-access-token":{"id":"docs/api/tutorials/references/generate-access-token","title":"Generate Access Token","description":"With CURL, you need to provide tokens. To generate token, run the following comand.","sidebar":"overviewSidebar"},"docs/api/tutorials/references/prepare-datahub":{"id":"docs/api/tutorials/references/prepare-datahub","title":"Preparing Your Local DataHub Environment","description":"Deploy DataHub Quickstart","sidebar":"overviewSidebar"},"docs/architecture/architecture":{"id":"docs/architecture/architecture","title":"Overview","description":"DataHub is a 3rd generation Metadata Platform that enables Data Discovery, Collaboration, Governance, and end-to-end Observability","sidebar":"overviewSidebar"},"docs/architecture/metadata-ingestion":{"id":"docs/architecture/metadata-ingestion","title":"Ingestion Framework","description":"DataHub supports an extremely flexible ingestion architecture that can support push, pull, asynchronous and synchronous models.","sidebar":"overviewSidebar"},"docs/architecture/metadata-serving":{"id":"docs/architecture/metadata-serving","title":"Serving Tier","description":"The figure below shows the high-level system diagram for DataHub\'s Serving Tier.","sidebar":"overviewSidebar"},"docs/architecture/stemming_and_synonyms":{"id":"docs/architecture/stemming_and_synonyms","title":"About DataHub [Stemming and Synonyms Support]","description":"This feature adds features to our current search implementation in an effort to make search results more relevant. Included improvements are:","sidebar":"overviewSidebar"},"docs/authentication/concepts":{"id":"docs/authentication/concepts","title":"Concepts & Key Components","description":"We introduced a few important concepts to the Metadata Service to make authentication work:","sidebar":"overviewSidebar"},"docs/authentication/guides/add-users":{"id":"docs/authentication/guides/add-users","title":"Onboarding Users to DataHub","description":"New user accounts can be provisioned on DataHub in 3 ways:","sidebar":"overviewSidebar"},"docs/authentication/guides/jaas":{"id":"docs/authentication/guides/jaas","title":"JaaS Authentication","description":"Overview","sidebar":"overviewSidebar"},"docs/authentication/guides/sso/configure-oidc-react":{"id":"docs/authentication/guides/sso/configure-oidc-react","title":"Overview","description":"The DataHub React application supports OIDC authentication built on top of the Pac4j Play library.","sidebar":"overviewSidebar"},"docs/authentication/guides/sso/configure-oidc-react-azure":{"id":"docs/authentication/guides/sso/configure-oidc-react-azure","title":"Configuring Azure Authentication for React App (OIDC)","description":"Authored on 21/12/2021","sidebar":"overviewSidebar"},"docs/authentication/guides/sso/configure-oidc-react-google":{"id":"docs/authentication/guides/sso/configure-oidc-react-google","title":"Configuring Google Authentication for React App (OIDC)","description":"Authored on 3/10/2021","sidebar":"overviewSidebar"},"docs/authentication/guides/sso/configure-oidc-react-okta":{"id":"docs/authentication/guides/sso/configure-oidc-react-okta","title":"Configuring Okta Authentication for React App (OIDC)","description":"Authored on 3/10/2021","sidebar":"overviewSidebar"},"docs/authentication/introducing-metadata-service-authentication":{"id":"docs/authentication/introducing-metadata-service-authentication","title":"Metadata Service Authentication","description":"Introduction","sidebar":"overviewSidebar"},"docs/authentication/personal-access-tokens":{"id":"docs/authentication/personal-access-tokens","title":"About DataHub Personal Access Tokens","description":"Personal Access Tokens, or PATs for short, allow users to represent themselves in code and programmatically use DataHub\'s APIs in deployments where security is a concern.","sidebar":"overviewSidebar"},"docs/authentication/README":{"id":"docs/authentication/README","title":"Overview","description":"Authentication is the process of verifying the identity of a user or service. There are two","sidebar":"overviewSidebar"},"docs/authorization/access-policies-guide":{"id":"docs/authorization/access-policies-guide","title":"About DataHub Access Policies","description":"Access Policies define who can do what to which resources. In conjunction with Roles, Access Policies determine what users are allowed to do on DataHub.","sidebar":"overviewSidebar"},"docs/authorization/groups":{"id":"docs/authorization/groups","title":"Authorization using Groups","description":"Introduction","sidebar":"overviewSidebar"},"docs/authorization/policies":{"id":"docs/authorization/policies","title":"Policies Guide","description":"Introduction","sidebar":"overviewSidebar"},"docs/authorization/README":{"id":"docs/authorization/README","title":"Overview","description":"Authorization specifies what accesses an authenticated user has within a system.","sidebar":"overviewSidebar"},"docs/authorization/roles":{"id":"docs/authorization/roles","title":"About DataHub Roles","description":"DataHub provides the ability to use Roles to manage permissions.","sidebar":"overviewSidebar"},"docs/browse":{"id":"docs/browse","title":"About DataHub Browse","description":"Browse is one of the primary entrypoints for discovering different Datasets, Dashboards, Charts and other DataHub Entities.","sidebar":"overviewSidebar"},"docs/cli":{"id":"docs/cli","title":"DataHub CLI","description":"DataHub comes with a friendly cli called datahub that allows you to perform a lot of common operations using just the command line. Acryl Data maintains the pypi package for datahub.","sidebar":"overviewSidebar"},"docs/CODE_OF_CONDUCT":{"id":"docs/CODE_OF_CONDUCT","title":"Contributor Covenant Code of Conduct","description":"Our Pledge","sidebar":"overviewSidebar"},"docs/components":{"id":"docs/components","title":"Components","description":"The DataHub platform consists of the components shown in the following diagram.","sidebar":"overviewSidebar"},"docs/CONTRIBUTING":{"id":"docs/CONTRIBUTING","title":"Contributing","description":"We always welcome contributions to help make DataHub better. Take a moment to read this document if you would like to contribute.","sidebar":"overviewSidebar"},"docs/datahub_lite":{"id":"docs/datahub_lite","title":"DataHub Lite (Experimental)","description":"What is it?","sidebar":"overviewSidebar"},"docs/debugging":{"id":"docs/debugging","title":"Quickstart Debugging Guide","description":"For when Quickstart did not work out smoothly.","sidebar":"overviewSidebar"},"docs/demo":{"id":"docs/demo","title":"See DataHub in Action","description":"We have a hosted demo environment available, kindly provided by Acryl Data.","sidebar":"overviewSidebar"},"docs/deploy/aws":{"id":"docs/deploy/aws","title":"Deploying to AWS","description":"The following is a set of instructions to quickstart DataHub on AWS Elastic Kubernetes Service (EKS). Note, the guide","sidebar":"overviewSidebar"},"docs/deploy/confluent-cloud":{"id":"docs/deploy/confluent-cloud","title":"Integrating with Confluent Cloud","description":"DataHub provides the ability to easily leverage Confluent Cloud as your Kafka provider. To do so, you\'ll need to configure DataHub to talk to a broker and schema registry hosted by Confluent.","sidebar":"overviewSidebar"},"docs/deploy/gcp":{"id":"docs/deploy/gcp","title":"Deploying to GCP","description":"The following is a set of instructions to quickstart DataHub on GCP Google Kubernetes Engine (GKE). Note, the guide","sidebar":"overviewSidebar"},"docs/deploy/kubernetes":{"id":"docs/deploy/kubernetes","title":"Deploying with Kubernetes","description":"Introduction","sidebar":"overviewSidebar"},"docs/deploy/telemetry":{"id":"docs/deploy/telemetry","title":"DataHub Telemetry","description":"Overview of DataHub Telemetry","sidebar":"overviewSidebar"},"docs/dev-guides/timeline":{"id":"docs/dev-guides/timeline","title":"Timeline API","description":"The Timeline API supports viewing version history of schemas, documentation, tags, glossary terms, and other updates","sidebar":"overviewSidebar"},"docs/developers":{"id":"docs/developers","title":"Local Development","description":"Pre-requirements","sidebar":"overviewSidebar"},"docs/docker/development":{"id":"docs/docker/development","title":"Using Docker Images During Development","description":"We\'ve created a special docker-compose.dev.yml override file that should configure docker images to be easier to use","sidebar":"overviewSidebar"},"docs/domains":{"id":"docs/domains","title":"About DataHub Domains","description":"Starting in version 0.8.25, DataHub supports grouping data assets into logical collections called Domains. Domains are curated, top-level folders or categories where related assets can be explicitly grouped. Management of Domains can be centralized, or distributed out to Domain owners Currently, an asset can belong to only one Domain at a time.","sidebar":"overviewSidebar"},"docs/enrich-metadata":{"id":"docs/enrich-metadata","title":"Enriching Metadata in DataHub","description":"Metadata Enrichment is a powerful way to annotate entities within DataHub, supercharging data discoverability and ensuring end-users have quick access to critical context for a given entity, such as:","sidebar":"overviewSidebar"},"docs/faq-using-datahub":{"id":"docs/faq-using-datahub","title":"FAQ Using DataHub","description":"Logo for my platform is not appearing on the Home Page or search results","sidebar":"overviewSidebar"},"docs/features":{"id":"docs/features","title":"Features","description":"DataHub is a modern data catalog built to enable end-to-end data discovery, data observability, and data governance. This extensible metadata platform is built for developers to tame the complexity of their rapidly evolving data ecosystems and for data practitioners to leverage the total value of data within their organization.","sidebar":"overviewSidebar"},"docs/features/dataset-usage-and-query-history":{"id":"docs/features/dataset-usage-and-query-history","title":"About DataHub Dataset Usage & Query History","description":"Dataset Usage & Query History can give dataset-level information about the top queries which referenced a dataset.","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/athena":{"id":"docs/generated/ingestion/sources/athena","title":"Athena","description":"Module athena","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/azure-ad":{"id":"docs/generated/ingestion/sources/azure-ad","title":"Azure AD","description":"Extracting DataHub Users","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/bigquery":{"id":"docs/generated/ingestion/sources/bigquery","title":"BigQuery","description":"Ingesting metadata from Bigquery requires using the bigquery module.","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/business-glossary":{"id":"docs/generated/ingestion/sources/business-glossary","title":"Business Glossary","description":"Module datahub-business-glossary","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/clickhouse":{"id":"docs/generated/ingestion/sources/clickhouse","title":"ClickHouse","description":"There are 2 sources that provide integration with ClickHouse","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/csv":{"id":"docs/generated/ingestion/sources/csv","title":"CSV","description":"Module csv-enricher","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/databricks":{"id":"docs/generated/ingestion/sources/databricks","title":"Databricks","description":"DataHub supports integration with Databricks ecosystem using a multitude of connectors, depending on your exact setup.","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/dbt":{"id":"docs/generated/ingestion/sources/dbt","title":"dbt","description":"There are 2 sources that provide integration with dbt","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/delta-lake":{"id":"docs/generated/ingestion/sources/delta-lake","title":"Delta Lake","description":"Module delta-lake","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/demo-data":{"id":"docs/generated/ingestion/sources/demo-data","title":"Demo Data","description":"Module demo-data","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/druid":{"id":"docs/generated/ingestion/sources/druid","title":"Druid","description":"Module druid","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/elasticsearch":{"id":"docs/generated/ingestion/sources/elasticsearch","title":"Elasticsearch","description":"Module elasticsearch","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/feast":{"id":"docs/generated/ingestion/sources/feast","title":"Feast","description":"Module feast","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/file":{"id":"docs/generated/ingestion/sources/file","title":"File","description":"Module file","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/file-based-lineage":{"id":"docs/generated/ingestion/sources/file-based-lineage","title":"File Based Lineage","description":"Module datahub-lineage-file","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/glue":{"id":"docs/generated/ingestion/sources/glue","title":"Glue","description":"Module glue","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/hana":{"id":"docs/generated/ingestion/sources/hana","title":"SAP HANA","description":"Module hana","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/hive":{"id":"docs/generated/ingestion/sources/hive","title":"Hive","description":"Module hive","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/iceberg":{"id":"docs/generated/ingestion/sources/iceberg","title":"Iceberg","description":"Module iceberg","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/json-schema":{"id":"docs/generated/ingestion/sources/json-schema","title":"JSON Schemas","description":"Module json-schema","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/kafka":{"id":"docs/generated/ingestion/sources/kafka","title":"Kafka","description":"Extract Topics & Schemas from Apache Kafka or Confluent Cloud.","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/kafka-connect":{"id":"docs/generated/ingestion/sources/kafka-connect","title":"Kafka Connect","description":"Integration Details","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/ldap":{"id":"docs/generated/ingestion/sources/ldap","title":"LDAP","description":"Module ldap","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/looker":{"id":"docs/generated/ingestion/sources/looker","title":"Looker","description":"There are 2 sources that provide integration with Looker","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/mariadb":{"id":"docs/generated/ingestion/sources/mariadb","title":"MariaDB","description":"Module mariadb","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/metabase":{"id":"docs/generated/ingestion/sources/metabase","title":"Metabase","description":"Module metabase","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/mode":{"id":"docs/generated/ingestion/sources/mode","title":"Mode","description":"Module mode","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/mongodb":{"id":"docs/generated/ingestion/sources/mongodb","title":"MongoDB","description":"Module mongodb","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/mssql":{"id":"docs/generated/ingestion/sources/mssql","title":"Microsoft SQL Server","description":"Module mssql","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/mysql":{"id":"docs/generated/ingestion/sources/mysql","title":"MySQL","description":"Module mysql","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/nifi":{"id":"docs/generated/ingestion/sources/nifi","title":"NiFi","description":"Module nifi","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/okta":{"id":"docs/generated/ingestion/sources/okta","title":"Okta","description":"Module okta","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/openapi":{"id":"docs/generated/ingestion/sources/openapi","title":"OpenAPI","description":"Module openapi","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/oracle":{"id":"docs/generated/ingestion/sources/oracle","title":"Oracle","description":"Module oracle","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/postgres":{"id":"docs/generated/ingestion/sources/postgres","title":"Postgres","description":"Module postgres","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/powerbi":{"id":"docs/generated/ingestion/sources/powerbi","title":"PowerBI","description":"There are 2 sources that provide integration with PowerBI","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/presto":{"id":"docs/generated/ingestion/sources/presto","title":"Presto","description":"Module presto","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/presto-on-hive":{"id":"docs/generated/ingestion/sources/presto-on-hive","title":"Presto on Hive","description":"Module presto-on-hive","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/pulsar":{"id":"docs/generated/ingestion/sources/pulsar","title":"Pulsar","description":"Integration Details","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/redash":{"id":"docs/generated/ingestion/sources/redash","title":"Redash","description":"Module redash","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/redshift":{"id":"docs/generated/ingestion/sources/redshift","title":"Redshift","description":"There are 2 sources that provide integration with Redshift","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/s3":{"id":"docs/generated/ingestion/sources/s3","title":"S3 Data Lake","description":"Module s3","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/sagemaker":{"id":"docs/generated/ingestion/sources/sagemaker","title":"SageMaker","description":"Module sagemaker","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/salesforce":{"id":"docs/generated/ingestion/sources/salesforce","title":"Salesforce","description":"Module salesforce","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/snowflake":{"id":"docs/generated/ingestion/sources/snowflake","title":"Snowflake","description":"Snowflake Ingestion through the UI","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/sqlalchemy":{"id":"docs/generated/ingestion/sources/sqlalchemy","title":"SQLAlchemy","description":"Module sqlalchemy","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/superset":{"id":"docs/generated/ingestion/sources/superset","title":"Superset","description":"Module superset","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/tableau":{"id":"docs/generated/ingestion/sources/tableau","title":"Tableau","description":"Module tableau","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/trino":{"id":"docs/generated/ingestion/sources/trino","title":"Trino","description":"There are 2 sources that provide integration with Trino","sidebar":"overviewSidebar"},"docs/generated/ingestion/sources/vertica":{"id":"docs/generated/ingestion/sources/vertica","title":"Vertica","description":"Integration Details","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/assertion":{"id":"docs/generated/metamodel/entities/assertion","title":"Assertion","description":"Assertion entity represents a data quality rule applied on dataset.","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/chart":{"id":"docs/generated/metamodel/entities/chart","title":"Chart","description":"Aspects","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/container":{"id":"docs/generated/metamodel/entities/container","title":"Container","description":"A container of related data assets.","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/corpGroup":{"id":"docs/generated/metamodel/entities/corpGroup","title":"CorpGroup","description":"CorpGroup represents an identity of a group of users in the enterprise.","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/corpuser":{"id":"docs/generated/metamodel/entities/corpuser","title":"Corpuser","description":"CorpUser represents an identity of a person (or an account) in the enterprise.","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/dashboard":{"id":"docs/generated/metamodel/entities/dashboard","title":"Dashboard","description":"Aspects","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/dataFlow":{"id":"docs/generated/metamodel/entities/dataFlow","title":"DataFlow","description":"Aspects","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/dataHubAccessToken":{"id":"docs/generated/metamodel/entities/dataHubAccessToken","title":"DataHubAccessToken","description":"Aspects","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/dataHubExecutionRequest":{"id":"docs/generated/metamodel/entities/dataHubExecutionRequest","title":"DataHubExecutionRequest","description":"Aspects","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/dataHubIngestionSource":{"id":"docs/generated/metamodel/entities/dataHubIngestionSource","title":"DataHubIngestionSource","description":"Aspects","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/dataHubPolicy":{"id":"docs/generated/metamodel/entities/dataHubPolicy","title":"DataHubPolicy","description":"DataHub Policies represent access policies granted to users or groups on metadata operations like edit, view etc.","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/dataHubRetention":{"id":"docs/generated/metamodel/entities/dataHubRetention","title":"DataHubRetention","description":"Aspects","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/dataHubRole":{"id":"docs/generated/metamodel/entities/dataHubRole","title":"DataHubRole","description":"Aspects","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/dataHubSecret":{"id":"docs/generated/metamodel/entities/dataHubSecret","title":"DataHubSecret","description":"Aspects","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/dataHubStepState":{"id":"docs/generated/metamodel/entities/dataHubStepState","title":"DataHubStepState","description":"Aspects","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/dataHubUpgrade":{"id":"docs/generated/metamodel/entities/dataHubUpgrade","title":"DataHubUpgrade","description":"Aspects","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/dataHubView":{"id":"docs/generated/metamodel/entities/dataHubView","title":"DataHubView","description":"Aspects","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/dataJob":{"id":"docs/generated/metamodel/entities/dataJob","title":"DataJob","description":"Aspects","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/dataPlatform":{"id":"docs/generated/metamodel/entities/dataPlatform","title":"Data Platform","description":"Data Platforms are systems or tools that contain Datasets, Dashboards, Charts, and all other kinds of data assets modeled in the metadata graph.","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/dataPlatformInstance":{"id":"docs/generated/metamodel/entities/dataPlatformInstance","title":"DataPlatformInstance","description":"Aspects","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/dataProcess":{"id":"docs/generated/metamodel/entities/dataProcess","title":"DataProcess","description":"Aspects","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/dataProcessInstance":{"id":"docs/generated/metamodel/entities/dataProcessInstance","title":"DataProcessInstance","description":"DataProcessInstance represents an instance of a datajob/jobflow run","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/dataset":{"id":"docs/generated/metamodel/entities/dataset","title":"Dataset","description":"The dataset entity is one the most important entities in the metadata model. They represent collections of data that are typically represented as Tables or Views in a database (e.g. BigQuery, Snowflake, Redshift etc.), Streams in a stream-processing environment (Kafka, Pulsar etc.), bundles of data found as Files or Folders in data lake systems (S3, ADLS, etc.).","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/domain":{"id":"docs/generated/metamodel/entities/domain","title":"Domain","description":"A data domain within an organization.","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/globalSettings":{"id":"docs/generated/metamodel/entities/globalSettings","title":"GlobalSettings","description":"Global settings for an the platform","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/glossaryNode":{"id":"docs/generated/metamodel/entities/glossaryNode","title":"GlossaryNode","description":"Aspects","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/glossaryTerm":{"id":"docs/generated/metamodel/entities/glossaryTerm","title":"GlossaryTerm","description":"Aspects","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/inviteToken":{"id":"docs/generated/metamodel/entities/inviteToken","title":"InviteToken","description":"Aspects","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/mlFeature":{"id":"docs/generated/metamodel/entities/mlFeature","title":"MlFeature","description":"Aspects","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/mlFeatureTable":{"id":"docs/generated/metamodel/entities/mlFeatureTable","title":"MlFeatureTable","description":"Aspects","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/mlModel":{"id":"docs/generated/metamodel/entities/mlModel","title":"MlModel","description":"Aspects","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/mlModelDeployment":{"id":"docs/generated/metamodel/entities/mlModelDeployment","title":"MlModelDeployment","description":"Aspects","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/mlModelGroup":{"id":"docs/generated/metamodel/entities/mlModelGroup","title":"MlModelGroup","description":"Aspects","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/mlPrimaryKey":{"id":"docs/generated/metamodel/entities/mlPrimaryKey","title":"MlPrimaryKey","description":"Aspects","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/notebook":{"id":"docs/generated/metamodel/entities/notebook","title":"Notebook","description":"\u26a0\ufe0f Notice: The Notebook entity is under active community development and IS NOT YET fully supported on the DataHub web application.","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/post":{"id":"docs/generated/metamodel/entities/post","title":"Post","description":"Aspects","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/query":{"id":"docs/generated/metamodel/entities/query","title":"Query","description":"Aspects","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/schemaField":{"id":"docs/generated/metamodel/entities/schemaField","title":"SchemaField","description":"Aspects","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/tag":{"id":"docs/generated/metamodel/entities/tag","title":"Tag","description":"Aspects","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/telemetry":{"id":"docs/generated/metamodel/entities/telemetry","title":"Telemetry","description":"Aspects","sidebar":"overviewSidebar"},"docs/generated/metamodel/entities/test":{"id":"docs/generated/metamodel/entities/test","title":"Test","description":"A DataHub test","sidebar":"overviewSidebar"},"docs/get-started-with-datahub":{"id":"docs/get-started-with-datahub","title":"Get Started With DataHub","description":"These guides are focused on helping you get up and running with DataHub as quickly as possible.","sidebar":"overviewSidebar"},"docs/glossary/business-glossary":{"id":"docs/glossary/business-glossary","title":"Business Glossary","description":"Introduction","sidebar":"overviewSidebar"},"docs/how/add-custom-data-platform":{"id":"docs/how/add-custom-data-platform","title":"Adding a custom Dataset Data Platform","description":"A Data Platform represents a 3rd party system from which Metadata Entities are ingested from. Each Dataset that is ingested is associated with a single platform, for example MySQL, Snowflake, Redshift, or BigQuery.","sidebar":"overviewSidebar"},"docs/how/add-custom-ingestion-source":{"id":"docs/how/add-custom-ingestion-source","title":"Using a Custom Ingestion Source","description":"Adding a custom ingestion source is the easiest way to extend Datahubs ingestion framework to support source systems","sidebar":"overviewSidebar"},"docs/how/add-new-aspect":{"id":"docs/how/add-new-aspect","title":"How to add a new metadata aspect?","description":"Adding a new metadata aspect is one of the most common ways to extend an existing entity."},"docs/how/add-user-data":{"id":"docs/how/add-user-data","title":"Adding user metadata in DataHub","description":"This guide shares how you can add user metadata in DataHub. Usually you would want to use one of our sources for ingesting user metadata. But if there is no connector for your use case then you would want to use this guide."},"docs/how/backup-datahub":{"id":"docs/how/backup-datahub","title":"Taking backup of DataHub","description":"Production","sidebar":"overviewSidebar"},"docs/how/configuring-authorization-with-apache-ranger":{"id":"docs/how/configuring-authorization-with-apache-ranger","title":"Configuring Authorization with Apache Ranger","description":"DataHub integration with Apache Ranger allows DataHub Authorization policies to be controlled inside Apache Ranger.","sidebar":"overviewSidebar"},"docs/how/delete-metadata":{"id":"docs/how/delete-metadata","title":"Removing Metadata from DataHub","description":"There are a two ways to delete metadata from DataHub:","sidebar":"overviewSidebar"},"docs/how/extract-container-logs":{"id":"docs/how/extract-container-logs","title":"How to Extract Logs from DataHub Containers","description":"DataHub containers, datahub GMS (backend server) and datahub frontend (UI server), write log files to the local container filesystem. To extract these logs, you\'ll need to get them from inside the container where the services are running.","sidebar":"overviewSidebar"},"docs/how/kafka-config":{"id":"docs/how/kafka-config","title":"Configuring Kafka","description":"DataHub requires Kafka to operate. Kafka is used as a durable log that can be used to store inbound","sidebar":"overviewSidebar"},"docs/how/migrating-graph-service-implementation":{"id":"docs/how/migrating-graph-service-implementation","title":"Migrate Graph Service Implementation to Elasticsearch","description":"We currently support either Elasticsearch or Neo4j as backend implementations for the graph service. We recommend","sidebar":"overviewSidebar"},"docs/how/restore-indices":{"id":"docs/how/restore-indices","title":"Restoring Search and Graph Indices from Local Database","description":"If search or graph services go down or you have made changes to them that require reindexing, you can restore them from","sidebar":"overviewSidebar"},"docs/how/search":{"id":"docs/how/search","title":"About DataHub Search","description":"\x3c!--","sidebar":"overviewSidebar"},"docs/how/ui-tabs-guide":{"id":"docs/how/ui-tabs-guide","title":"UI Tabs Guide","description":"Some of the tabs in the UI might not be enabled by default. This guide is supposed to tell Admins of DataHub how to enable those UI tabs."},"docs/how/updating-datahub":{"id":"docs/how/updating-datahub","title":"Updating DataHub","description":"This file documents any backwards-incompatible changes in DataHub and assists people when migrating to a new version.","sidebar":"overviewSidebar"},"docs/lineage/airflow":{"id":"docs/lineage/airflow","title":"Airflow Integration","description":"DataHub supports integration of","sidebar":"overviewSidebar"},"docs/lineage/lineage-feature-guide":{"id":"docs/lineage/lineage-feature-guide","title":"About DataHub Lineage","description":"Lineage is used to capture data dependencies within an organization. It allows you to track the inputs from which a data asset is derived, along with the data assets that depend on it downstream.","sidebar":"overviewSidebar"},"docs/links":{"id":"docs/links","title":"Articles & Talks","description":"Overviews","sidebar":"overviewSidebar"},"docs/managed-datahub/approval-workflows":{"id":"docs/managed-datahub/approval-workflows","title":"About DataHub Approval Workflows","description":"Overview","sidebar":"overviewSidebar"},"docs/managed-datahub/chrome-extension":{"id":"docs/managed-datahub/chrome-extension","title":"Local Chrome Extension","description":"Learn how to upload and use the Acryl DataHub Chrome extension (beta) locally before it\'s available on the Chrome store.","sidebar":"overviewSidebar"},"docs/managed-datahub/datahub-api/entity-events-api":{"id":"docs/managed-datahub/datahub-api/entity-events-api","title":"Entity Events API","description":"This guide details the Entity Events API, which allows you to take action when things change on DataHub.","sidebar":"overviewSidebar"},"docs/managed-datahub/datahub-api/graphql-api/getting-started":{"id":"docs/managed-datahub/datahub-api/graphql-api/getting-started","title":"Getting Started","description":"Getting started with the DataHub GraphQL API.","sidebar":"overviewSidebar"},"docs/managed-datahub/datahub-api/graphql-api/incidents-api-beta":{"id":"docs/managed-datahub/datahub-api/graphql-api/incidents-api-beta","title":"Incidents API (Beta)","description":"This page provides an overview of working with the DataHub Incidents API.","sidebar":"overviewSidebar"},"docs/managed-datahub/integrations/aws-privatelink":{"id":"docs/managed-datahub/integrations/aws-privatelink","title":"AWS PrivateLink","description":"If you require a private connection between the provisioned DataHub instance and your own existing AWS account, Acryl supports using AWS PrivateLink in order to complete this private connection.","sidebar":"overviewSidebar"},"docs/managed-datahub/integrations/oidc-sso-integration":{"id":"docs/managed-datahub/integrations/oidc-sso-integration","title":"OIDC SSO Integration","description":"This page will help you set up OIDC SSO with your identity provider to log into Acryl Data","sidebar":"overviewSidebar"},"docs/managed-datahub/managed-datahub-overview":{"id":"docs/managed-datahub/managed-datahub-overview","title":"Managed DataHub Exclusives","description":"Acryl DataHub offers a slew of additional features on top of the normal OSS project.","sidebar":"overviewSidebar"},"docs/managed-datahub/metadata-ingestion-with-acryl/ingestion":{"id":"docs/managed-datahub/metadata-ingestion-with-acryl/ingestion","title":"Ingestion","description":"Acryl Metadata Ingestion functions similarly to that in open source DataHub. Sources are configured via the UI Ingestion or via a Recipe, ingestion recipes can be scheduled using your system of choice, and metadata can be pushed from anywhere.","sidebar":"overviewSidebar"},"docs/managed-datahub/operator-guide/setting-up-events-api-on-aws-eventbridge":{"id":"docs/managed-datahub/operator-guide/setting-up-events-api-on-aws-eventbridge","title":"Setting up Events API on AWS EventBridge","description":"This guide will walk through the configuration required to start receiving Acryl DataHub events via AWS EventBridge.","sidebar":"overviewSidebar"},"docs/managed-datahub/operator-guide/setting-up-remote-ingestion-executor-on-aws":{"id":"docs/managed-datahub/operator-guide/setting-up-remote-ingestion-executor-on-aws","title":"Setting up Remote Ingestion Executor on AWS","description":"This page describes the steps required to configure a remote ingestion executor, which allows you to ingest metadata from private metadata sources using private credentials via the DataHub UI.","sidebar":"overviewSidebar"},"docs/managed-datahub/release-notes/v_0_1_69":{"id":"docs/managed-datahub/release-notes/v_0_1_69","title":"v0.1.69","description":"---","sidebar":"overviewSidebar"},"docs/managed-datahub/release-notes/v_0_1_70":{"id":"docs/managed-datahub/release-notes/v_0_1_70","title":"v0.1.70","description":"---","sidebar":"overviewSidebar"},"docs/managed-datahub/release-notes/v_0_1_72":{"id":"docs/managed-datahub/release-notes/v_0_1_72","title":"v0.1.72","description":"---","sidebar":"overviewSidebar"},"docs/managed-datahub/release-notes/v_0_1_73":{"id":"docs/managed-datahub/release-notes/v_0_1_73","title":"v0.1.73","description":"---","sidebar":"overviewSidebar"},"docs/managed-datahub/release-notes/v_0_2_0":{"id":"docs/managed-datahub/release-notes/v_0_2_0","title":"v0.2.0","description":"---","sidebar":"overviewSidebar"},"docs/managed-datahub/release-notes/v_0_2_1":{"id":"docs/managed-datahub/release-notes/v_0_2_1","title":"v0.2.1","description":"---","sidebar":"overviewSidebar"},"docs/managed-datahub/release-notes/v_0_2_2":{"id":"docs/managed-datahub/release-notes/v_0_2_2","title":"v0.2.2","description":"---","sidebar":"overviewSidebar"},"docs/managed-datahub/release-notes/v_0_2_3":{"id":"docs/managed-datahub/release-notes/v_0_2_3","title":"v0.2.3","description":"---","sidebar":"overviewSidebar"},"docs/managed-datahub/release-notes/v_0_2_4":{"id":"docs/managed-datahub/release-notes/v_0_2_4","title":"v0.2.4","description":"---","sidebar":"overviewSidebar"},"docs/managed-datahub/saas-slack-setup":{"id":"docs/managed-datahub/saas-slack-setup","title":"Configure Slack Notifications","description":"Install the DataHub Slack App into your Slack workspace","sidebar":"overviewSidebar"},"docs/managed-datahub/welcome-acryl":{"id":"docs/managed-datahub/welcome-acryl","title":"Getting Started with Acryl DataHub","description":"Welcome to the Acryl DataHub! We at Acryl are on a mission to make data reliable by bringing clarity to the who, what, when, & how of your data ecosystem. We\'re thrilled to be on this journey with you; and cannot wait to see what we build together!","sidebar":"overviewSidebar"},"docs/modeling/extending-the-metadata-model":{"id":"docs/modeling/extending-the-metadata-model","title":"Extending the Metadata Model","description":"You can extend the metadata model by either creating a new Entity or extending an existing one. Unsure if you need to","sidebar":"overviewSidebar"},"docs/modeling/metadata-model":{"id":"docs/modeling/metadata-model","title":"The Metadata Model","description":"DataHub takes a schema-first approach to modeling metadata. We use the open-source Pegasus schema language (PDL) extended with a custom set of annotations to model metadata. The DataHub storage, serving, indexing and ingestion layer operates directly on top of the metadata model and supports strong types all the way from the client to the storage layer.","sidebar":"overviewSidebar"},"docs/platform-instances":{"id":"docs/platform-instances","title":"Working With Platform Instances","description":"DataHub\'s metadata model for Datasets supports a three-part key currently:","sidebar":"overviewSidebar"},"docs/plugins":{"id":"docs/plugins","title":"Plugins Guide","description":"Plugins are way to enhance the basic DataHub functionality in a custom manner.","sidebar":"overviewSidebar"},"docs/posts":{"id":"docs/posts","title":"About DataHub Posts","description":"DataHub allows users to make Posts that can be displayed on the app. Currently, Posts are only supported on the Home Page, but may be extended to other surfaces of the app in the future. Posts can be used to accomplish the following:","sidebar":"overviewSidebar"},"docs/quick-ingestion-guides/bigquery/configuration":{"id":"docs/quick-ingestion-guides/bigquery/configuration","title":"Configuration","description":"Now that you have created a Service Account and Service Account Key in BigQuery in the prior step, it\'s now time to set up a connection via the DataHub UI.","sidebar":"overviewSidebar"},"docs/quick-ingestion-guides/bigquery/overview":{"id":"docs/quick-ingestion-guides/bigquery/overview","title":"Overview","description":"What You Will Get Out of This Guide","sidebar":"overviewSidebar"},"docs/quick-ingestion-guides/bigquery/setup":{"id":"docs/quick-ingestion-guides/bigquery/setup","title":"Setup","description":"To configure ingestion from BigQuery, you\'ll need a Service Account configured with the proper permission sets, and an associated Service Account Key.","sidebar":"overviewSidebar"},"docs/quick-ingestion-guides/snowflake/configuration":{"id":"docs/quick-ingestion-guides/snowflake/configuration","title":"Configuration","description":"Now that you have created a DataHub-specific user with the relevant roles in Snowflake in the prior step, it\'s now time to set up a connection via the DataHub UI.","sidebar":"overviewSidebar"},"docs/quick-ingestion-guides/snowflake/overview":{"id":"docs/quick-ingestion-guides/snowflake/overview","title":"Overview","description":"What You Will Get Out of This Guide","sidebar":"overviewSidebar"},"docs/quick-ingestion-guides/snowflake/setup":{"id":"docs/quick-ingestion-guides/snowflake/setup","title":"Setup","description":"In order to configure ingestion from Snowflake, you\'ll first have to ensure you have a Snowflake user with the ACCOUNTADMIN role or MANAGE GRANTS privilege.","sidebar":"overviewSidebar"},"docs/quickstart":{"id":"docs/quickstart","title":"DataHub Quickstart Guide","description":"Deploying DataHub","sidebar":"overviewSidebar"},"docs/rfc":{"id":"docs/rfc","title":"DataHub RFC Process","description":"What is an RFC?","sidebar":"overviewSidebar"},"docs/roadmap":{"id":"docs/roadmap","title":"DataHub Roadmap","description":"The DataHub Roadmap has a new home!"},"docs/saas":{"id":"docs/saas","title":"Managed DataHub","description":"Sign up for fully managed, hassle-free and secure SaaS service for DataHub, provided by Acryl Data.","sidebar":"overviewSidebar"},"docs/schema-history":{"id":"docs/schema-history","title":"About DataHub Schema History","description":"Schema History is a valuable tool for understanding how a Dataset changes over time and gives insight into the following cases,","sidebar":"overviewSidebar"},"docs/slack":{"id":"docs/slack","title":"Slack","description":"The DataHub Slack is a thriving and rapidly growing community - we can\'t wait for you to join us!","sidebar":"overviewSidebar"},"docs/sync-status":{"id":"docs/sync-status","title":"About DataHub Sync Status","description":"When looking at metadata in DataHub, it\'s useful to know if the information you\'re looking at is relevant.","sidebar":"overviewSidebar"},"docs/tags":{"id":"docs/tags","title":"About DataHub Tags","description":"Tags are informal, loosely controlled labels that help in search & discovery. They can be added to datasets, dataset schemas, or containers, for an easy way to label or categorize entities \u2013 without having to associate them to a broader business glossary or vocabulary.","sidebar":"overviewSidebar"},"docs/tests/metadata-tests":{"id":"docs/tests/metadata-tests","title":"About Metadata Tests","description":"DataHub includes a highly configurable, no-code framework that allows you to configure broad-spanning monitors & continuous actions","sidebar":"overviewSidebar"},"docs/townhall-history":{"id":"docs/townhall-history","title":"Town Hall History","description":"A list of previous Town Halls, their planned schedule, and the recording of the meeting.","sidebar":"overviewSidebar"},"docs/townhalls":{"id":"docs/townhalls","title":"DataHub Town Halls","description":"We hold regular virtual town hall meetings to meet with DataHub community.","sidebar":"overviewSidebar"},"docs/ui-ingestion":{"id":"docs/ui-ingestion","title":"UI Ingestion Guide","description":"Introduction","sidebar":"overviewSidebar"},"docs/what-is-datahub/datahub-concepts":{"id":"docs/what-is-datahub/datahub-concepts","title":"DataHub Concepts","description":"Explore key concepts of DataHub to take full advantage of its capabilities in managing your data.","sidebar":"overviewSidebar"},"docs/what/aspect":{"id":"docs/what/aspect","title":"What is a metadata aspect?","description":"A metadata aspect is a structured document, or more precisely a record in PDL,"},"docs/what/delta":{"id":"docs/what/delta","title":"What is a metadata delta?","description":"Rest.li supports partial update natively without needing explicitly defined models."},"docs/what/entity":{"id":"docs/what/entity","title":"Entities","description":"This page has been moved. Please refer to The Metadata Model for details on"},"docs/what/gma":{"id":"docs/what/gma","title":"What is Generalized Metadata Architecture (GMA)?","description":"GMA is the backend infrastructure for DataHub. Unlike existing architectures, GMA leverages multiple storage technologies to efficiently service the four most commonly used query patterns"},"docs/what/gms":{"id":"docs/what/gms","title":"What is Generalized Metadata Service (GMS)?","description":"Metadata for entities onboarded to GMA is served through microservices known as Generalized Metadata Service (GMS). GMS typically provides a Rest.li API and must access the metadata using GMA DAOs."},"docs/what/graph":{"id":"docs/what/graph","title":"What is GMA graph?","description":"All the entities and relationships are stored in a graph database, Neo4j."},"docs/what/mxe":{"id":"docs/what/mxe","title":"Metadata Events","description":"DataHub makes use a few important Kafka events for operation. The most notable of these include","sidebar":"overviewSidebar"},"docs/what/relationship":{"id":"docs/what/relationship","title":"What is a relationship?","description":"A relationship is a named associate between exactly two entities, a source and a destination."},"docs/what/search-document":{"id":"docs/what/search-document","title":"What is a search document?","description":"Search documents are also modeled using PDL explicitly."},"docs/what/search-index":{"id":"docs/what/search-index","title":"What is GMA search index?","description":"Each search document type (or entity type) will be mapped to an independent search index in Elasticsearch."},"docs/what/snapshot":{"id":"docs/what/snapshot","title":"What is a snapshot?","description":"A metadata snapshot models the current state of one or multiple metadata aspects associated with a particular entity."},"docs/what/urn":{"id":"docs/what/urn","title":"What is URN?","description":"URN (Uniform Resource Name) is the chosen scheme of URI to uniquely define any resource in DataHub. It has the following form"},"graphql/enums":{"id":"graphql/enums","title":"Enums","description":"AccessLevel","sidebar":"overviewSidebar"},"graphql/inputObjects":{"id":"graphql/inputObjects","title":"Input objects","description":"AcceptRoleInput","sidebar":"overviewSidebar"},"graphql/interfaces":{"id":"graphql/interfaces","title":"Interfaces","description":"Aspect","sidebar":"overviewSidebar"},"graphql/mutations":{"id":"graphql/mutations","title":"Mutations","description":"acceptRole","sidebar":"overviewSidebar"},"graphql/objects":{"id":"graphql/objects","title":"Objects","description":"AccessToken","sidebar":"overviewSidebar"},"graphql/queries":{"id":"graphql/queries","title":"Queries","description":"appConfig","sidebar":"overviewSidebar"},"graphql/scalars":{"id":"graphql/scalars","title":"Scalars","description":"Boolean","sidebar":"overviewSidebar"},"graphql/unions":{"id":"graphql/unions","title":"Unions","description":"AnalyticsChart","sidebar":"overviewSidebar"},"metadata-ingestion-modules/airflow-plugin/README":{"id":"metadata-ingestion-modules/airflow-plugin/README","title":"Datahub Airflow Plugin","description":"See the DataHub Airflow docs for details."},"metadata-ingestion/adding-source":{"id":"metadata-ingestion/adding-source","title":"Adding a Metadata Ingestion Source","description":"There are two ways of adding a metadata ingestion source.","sidebar":"overviewSidebar"},"metadata-ingestion/as-a-library":{"id":"metadata-ingestion/as-a-library","title":"Python Emitter","description":"In some cases, you might want to construct Metadata events directly and use programmatic ways to emit that metadata to DataHub. Use-cases are typically push-based and include emitting metadata events from CI/CD pipelines, custom orchestrators etc.","sidebar":"overviewSidebar"},"metadata-ingestion/developing":{"id":"metadata-ingestion/developing","title":"Developing on Metadata Ingestion","description":"If you just want to use metadata ingestion, check the user-centric guide.","sidebar":"overviewSidebar"},"metadata-ingestion/docs/dev_guides/add_stateful_ingestion_to_source":{"id":"metadata-ingestion/docs/dev_guides/add_stateful_ingestion_to_source","title":"Adding Stateful Ingestion to a Source","description":"Currently, datahub supports the Stale Metadata Removal and","sidebar":"overviewSidebar"},"metadata-ingestion/docs/dev_guides/classification":{"id":"metadata-ingestion/docs/dev_guides/classification","title":"Classification","description":"The classification feature enables sources to be configured to automatically predict info types for columns and use them as glossary terms. This is an explicit opt-in feature and is not enabled by default.","sidebar":"overviewSidebar"},"metadata-ingestion/docs/dev_guides/reporting_telemetry":{"id":"metadata-ingestion/docs/dev_guides/reporting_telemetry","title":"Datahub\'s Reporting Framework for Ingestion Job Telemetry","description":"The Datahub\'s reporting framework allows for configuring reporting providers with the ingestion pipelines to send","sidebar":"overviewSidebar"},"metadata-ingestion/docs/dev_guides/sql_profiles":{"id":"metadata-ingestion/docs/dev_guides/sql_profiles","title":"SQL Profiling","description":"SQL Profiling collects table level and column level statistics.","sidebar":"overviewSidebar"},"metadata-ingestion/docs/dev_guides/stateful":{"id":"metadata-ingestion/docs/dev_guides/stateful","title":"Stateful Ingestion","description":"The stateful ingestion feature enables sources to be configured to save custom checkpoint states from their","sidebar":"overviewSidebar"},"metadata-ingestion/docs/transformer/dataset_transformer":{"id":"metadata-ingestion/docs/transformer/dataset_transformer","title":"Dataset","description":"The below table shows transformer which can transform aspects of entity Dataset.","sidebar":"overviewSidebar"},"metadata-ingestion/docs/transformer/intro":{"id":"metadata-ingestion/docs/transformer/intro","title":"Introduction","description":"What\u2019s a transformer?","sidebar":"overviewSidebar"},"metadata-ingestion/examples/transforms/README":{"id":"metadata-ingestion/examples/transforms/README","title":"Custom transformer script","description":"This script sets up a transformer that reads in a list of owner URNs from a JSON file specified via owners_json and appends these owners to every MCE."},"metadata-ingestion/integration_docs/great-expectations":{"id":"metadata-ingestion/integration_docs/great-expectations","title":"Great Expectations","description":"This guide helps to setup and configure DataHubValidationAction in Great Expectations to send assertions(expectations) and their results to DataHub using DataHub\'s Python Rest emitter.","sidebar":"overviewSidebar"},"metadata-ingestion/README":{"id":"metadata-ingestion/README","title":"Introduction to Metadata Ingestion","description":"Integration Options","sidebar":"overviewSidebar"},"metadata-ingestion/schedule_docs/airflow":{"id":"metadata-ingestion/schedule_docs/airflow","title":"Using Airflow","description":"If you are using Apache Airflow for your scheduling then you might want to also use it for scheduling your ingestion recipes. For any Airflow specific questions you can go through Airflow docs for more details.","sidebar":"overviewSidebar"},"metadata-ingestion/schedule_docs/cron":{"id":"metadata-ingestion/schedule_docs/cron","title":"Using Cron","description":"Assume you have a recipe file /home/ubuntu/datahubingest/mysqlto_datahub.yml on your machine","sidebar":"overviewSidebar"},"metadata-ingestion/schedule_docs/datahub":{"id":"metadata-ingestion/schedule_docs/datahub","title":"Using DataHub","description":"UI Ingestion can be used to schedule metadata ingestion through DataHub."},"metadata-ingestion/schedule_docs/intro":{"id":"metadata-ingestion/schedule_docs/intro","title":"Introduction to Scheduling Metadata Ingestion","description":"Given a recipe file /home/ubuntu/datahubingest/mysqlto_datahub.yml.","sidebar":"overviewSidebar"},"metadata-ingestion/schedule_docs/kubernetes":{"id":"metadata-ingestion/schedule_docs/kubernetes","title":"Using Kubernetes","description":"If you have deployed DataHub using our official helm charts you can use the","sidebar":"overviewSidebar"},"metadata-ingestion/sink_docs/console":{"id":"metadata-ingestion/sink_docs/console","title":"Console","description":"For context on getting started with ingestion, check out our metadata ingestion guide.","sidebar":"overviewSidebar"},"metadata-ingestion/sink_docs/datahub":{"id":"metadata-ingestion/sink_docs/datahub","title":"DataHub","description":"DataHub Rest","sidebar":"overviewSidebar"},"metadata-ingestion/sink_docs/file":{"id":"metadata-ingestion/sink_docs/file","title":"File","description":"For context on getting started with ingestion, check out our metadata ingestion guide.","sidebar":"overviewSidebar"},"metadata-ingestion/source-docs-template":{"id":"metadata-ingestion/source-docs-template","title":"Source Name","description":"Certified"},"metadata-integration/java/as-a-library":{"id":"metadata-integration/java/as-a-library","title":"Java Emitter","description":"In some cases, you might want to construct Metadata events directly and use programmatic ways to emit that metadata to DataHub. Use-cases are typically push-based and include emitting metadata events from CI/CD pipelines, custom orchestrators etc.","sidebar":"overviewSidebar"},"metadata-integration/java/datahub-protobuf/README":{"id":"metadata-integration/java/datahub-protobuf/README","title":"Protobuf Schemas","description":"The datahub-protobuf module is designed to be used with the Java Emitter, the input is a compiled protobuf binary .protoc files and optionally the corresponding .proto source code. In addition, you can supply the root message in cases where a single protobuf source file includes multiple non-nested messages.","sidebar":"overviewSidebar"},"metadata-integration/java/spark-lineage/README":{"id":"metadata-integration/java/spark-lineage/README","title":"Spark","description":"To integrate Spark with DataHub, we provide a lightweight Java agent that listens for Spark application and job events and pushes metadata out to DataHub in real-time. The agent listens to events such application start/end, and SQLExecution start/end to create pipelines (i.e. DataJob) and tasks (i.e. DataFlow) in Datahub along with lineage to datasets that are being read from and written to. Read on to learn how to configure this for different Spark scenarios.","sidebar":"overviewSidebar"},"metadata-jobs/mae-consumer-job/README":{"id":"metadata-jobs/mae-consumer-job/README","title":"metadata-jobs:mae-consumer-job","description":"The Metadata Audit Event Consumer is a Spring job which can be deployed by itself, or as part of the Metadata Service.","sidebar":"overviewSidebar"},"metadata-jobs/mce-consumer-job/README":{"id":"metadata-jobs/mce-consumer-job/README","title":"metadata-jobs:mce-consumer-job","description":"The Metadata Change Event Consumer is a Spring job which can be deployed by itself, or as part of the Metadata Service.","sidebar":"overviewSidebar"},"metadata-jobs/README":{"id":"metadata-jobs/README","title":"MXE Processing Jobs","description":"DataHub uses Kafka as the pub-sub message queue in the backend. There are 2 Kafka topics used by DataHub which are"},"metadata-models-custom/README":{"id":"metadata-models-custom/README","title":"A Custom Metadata Model","description":"This module hosts a gradle project where you can store your custom metadata model. It contains an example extension for you to follow."},"metadata-service/README":{"id":"metadata-service/README","title":"metadata-service","description":"DataHub Metadata Service is a service written in Java consisting of multiple servlets:","sidebar":"overviewSidebar"},"perf-test/README":{"id":"perf-test/README","title":"Load testing with Locust","description":"Locust is an open-source, python-based, easy-to-use load testing tool. It provides an interface to"},"python-sdk/builder":{"id":"python-sdk/builder","title":"Builder","description":"\\\\n\\\\n\\\\nThese classes and methods make it easier to construct MetadataChangeProposals and MetadataChangeEvents.\\\\n\\\\n\\\\nclass datahub.emitter.mcp.MetadataChangeProposalWrapper(entityType=\'ENTITYTYPEUNSET\', changeType=\'UPSERT\', entityUrn=None, entityKeyAspect=None, auditHeader=None, aspectName=None, aspect=None, systemMetadata=None)\\\\nBases\\\\n\\\\nentityType (str)\\\\nchangeType (Union[str, ChangeTypeClass])\\\\nentityUrn (Optional[str])\\\\nentityKeyAspect (Optional[Aspect])\\\\nauditHeader (Optional[KafkaAuditHeaderClass])\\\\naspectName (Optional[str])\\\\naspect (Optional[Aspect])\\\\nsystemMetadata (Optional[SystemMetadataClass])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nentityType Union[str, ChangeTypeClass] = \'UPSERT\'\\\\n\\\\n\\\\n\\\\nentityUrn Optional[Aspect] = None\\\\n\\\\n\\\\n\\\\nauditHeader Optional[str] = None\\\\n\\\\n\\\\n\\\\naspect Optional[SystemMetadataClass] = None\\\\n\\\\n\\\\n\\\\nclassmethod constructmany(entityUrn, aspects)\\\\n\\\\nParameters\\\\nList[MetadataChangeProposalWrapper]\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nmakemcp()\\\\n\\\\nReturn type\\\\nbool\\\\n\\\\n\\\\n\\\\n\\\\n\\\\ntoobj(tuples=False, simplifiedstructure=False)\\\\n\\\\nParameters\\\\ndict\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nclassmethod fromobj(obj, tuples=False)\\\\nAttempt to deserialize into an MCPW, but fall back\\\\nto a standard MCP if we\\\\u2019re missing codegen\\\\u2019d classes for the\\\\nentity key or aspect.\\\\n\\\\nParameters\\\\nUnion[MetadataChangeProposalWrapper, MetadataChangeProposalClass]\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nclassmethod fromobjrequirewrapper(obj, tuples=False)\\\\n\\\\nParameters\\\\nMetadataChangeProposalWrapper\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nasworkunit()\\\\n\\\\nReturn type\\\\nvalue (bool)\\\\n\\\\nReturn type Enum\\\\nAn enumeration.\\\\n\\\\n\\\\nUSER = \'corpuser\'\\\\n\\\\n\\\\n\\\\nGROUP = \'corpGroup\'\\\\n\\\\n\\\\n\\\\n\\\\ndatahub.emitter.mcebuilder.getsystime()\\\\n\\\\nReturn type\\\\nplatform (str)\\\\n\\\\nReturn type\\\\n\\\\nplatform (str)\\\\nname (str)\\\\nenv (str)\\\\n\\\\n\\\\nReturn type\\\\n\\\\nplatform (str)\\\\ninstance (str)\\\\n\\\\n\\\\nReturn type\\\\n\\\\nplatform (str)\\\\nname (str)\\\\nplatforminstance (Optional[str])\\\\nenv (str)\\\\n\\\\n\\\\nReturn type\\\\n\\\\nparenturn (str)\\\\nfieldpath (str)\\\\n\\\\n\\\\nReturn type\\\\nschemafieldurn (str)\\\\n\\\\nReturn type\\\\ndataseturn (str)\\\\n\\\\nReturn type\\\\nkey (DatasetKeyClass)\\\\n\\\\nReturn type\\\\nguid (str)\\\\n\\\\nReturn type\\\\nguid (str)\\\\n\\\\nReturn type\\\\nobj (dict)\\\\n\\\\nReturn type\\\\nassertionid (str)\\\\n\\\\nReturn type\\\\nassertionurn (str)\\\\n\\\\nReturn type\\\\nusername (str)\\\\n\\\\nReturn type\\\\ngroupname (str)\\\\n\\\\nReturn type\\\\ntag (str)\\\\n\\\\nReturn type\\\\n\\\\nowner (str)\\\\nownertype (OwnerType)\\\\n\\\\n\\\\nReturn type\\\\nterm (str)\\\\n\\\\nReturn type\\\\n\\\\norchestrator (str)\\\\nflowid (str)\\\\ncluster (str)\\\\nplatforminstance (Optional[str])\\\\n\\\\n\\\\nReturn type\\\\n\\\\nflowurn (str)\\\\njobid (str)\\\\n\\\\n\\\\nReturn type\\\\ndataProcessInstanceId (str)\\\\n\\\\nReturn type\\\\n\\\\norchestrator (str)\\\\nflowid (str)\\\\njobid (str)\\\\ncluster (str)\\\\nplatforminstance (Optional[str])\\\\n\\\\n\\\\nReturn type\\\\n\\\\nplatform (str)\\\\nname (str)\\\\nplatforminstance (Optional[str])\\\\n\\\\n\\\\nReturn type\\\\ndashboardurn (str)\\\\n\\\\nReturn type\\\\n\\\\nplatform (str)\\\\nname (str)\\\\nplatforminstance (Optional[str])\\\\n\\\\n\\\\nReturn type\\\\ncharturn (str)\\\\n\\\\nReturn type\\\\ndomain (str)\\\\n\\\\nReturn type\\\\n\\\\nfeaturetablename (str)\\\\nprimarykeyname (str)\\\\n\\\\n\\\\nReturn type\\\\n\\\\nfeaturetablename (str)\\\\nfeaturename (str)\\\\n\\\\n\\\\nReturn type\\\\n\\\\nplatform (str)\\\\nfeaturetablename (str)\\\\n\\\\n\\\\nReturn type\\\\n\\\\nplatform (str)\\\\nmodelname (str)\\\\nenv (str)\\\\n\\\\n\\\\nReturn type\\\\n\\\\nplatform (str)\\\\ndeploymentname (str)\\\\nenv (str)\\\\n\\\\n\\\\nReturn type\\\\n\\\\nplatform (str)\\\\ngroupname (str)\\\\nenv (str)\\\\n\\\\n\\\\nReturn type\\\\nownershiptype (Optional[str])\\\\n\\\\nReturn type\\\\nownershiptype (Optional[str])\\\\n\\\\nReturn type\\\\n\\\\nupstreamurns (List[str])\\\\ndownstreamurn (str)\\\\nlineagetype (str)\\\\n\\\\n\\\\nReturn type\\\\n\\\\nmce (MetadataChangeEventClass)\\\\nAspectType (Type[TypeVar(Aspect, bound= Aspect)])\\\\n\\\\n\\\\nReturn type\\\\n\\\\nmce (MetadataChangeEventClass)\\\\nAspectType (Type[TypeVar(Aspect, bound= Aspect)])\\\\n\\\\n\\\\nReturn type\\\\n\\\\nmce (MetadataChangeEventClass)\\\\nAspectType (Type[TypeVar(Aspect, bound= Aspect)])\\\\n\\\\n\\\\nReturn type\\\\n\\\\nmce (MetadataChangeEventClass)\\\\naspecttype (Type[TypeVar(Aspect, bound= Aspect)])\\\\n\\\\n\\\\nReturn type\\\\n\\\\nmce (MetadataChangeEventClass)\\\\ndefault (TypeVar(Aspect, bound= Aspect))\\\\n\\\\n\\\\nReturn type\\\\ntags (List[str])\\\\n\\\\nReturn type\\\\n\\\\nownerurns (List[str])\\\\nsourcetype (Union[str, OwnershipSourceTypeClass, None])\\\\nownertype (Union[str, OwnershipTypeClass])\\\\n\\\\n\\\\nReturn type\\\\ntermurns (List[str])\\\\n\\\\nReturn type\\\\n\\\\nmce (MetadataChangeEventClass)\\\\naspect (Optional[TypeVar(Aspect, bound= Aspect)])\\\\naspecttype (Type[TypeVar(Aspect, bound= Aspect)])\\\\n\\\\n\\\\nReturn type BaseModel\\\\n\\\\nParameters\\\\nDict[str, str]\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nguid()\\\\n\\\\nReturn type DatahubKey\\\\n\\\\nParameters str\\\\n\\\\n\\\\n\\\\ninstance Optional[str]\\\\n\\\\n\\\\n\\\\nguiddict()\\\\n\\\\nReturn type PlatformKey\\\\n\\\\nParameters str\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.emitter.mcp_builder.SchemaKey(data)\\\\nBases\\\\n\\\\ndata (Any)\\\\nplatform (str)\\\\ninstance (str | None)\\\\nbackcompatinstanceforguid (str | None)\\\\ndatabase (str)\\\\nschema (str)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\ndbschema PlatformKey\\\\n\\\\nParameters str\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.emitter.mcpbuilder.MetastoreKey(**data)\\\\nBases\\\\n\\\\ndata (Any)\\\\nplatform (str)\\\\ninstance (str | None)\\\\nbackcompatinstanceforguid (str | None)\\\\nmetastore (str)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nmetastore MetastoreKey\\\\n\\\\nParameters str\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.emitter.mcpbuilder.UnitySchemaKey(**data)\\\\nBases\\\\n\\\\ndata (Any)\\\\nplatform (str)\\\\ninstance (str | None)\\\\nbackcompatinstanceforguid (str | None)\\\\nmetastore (str)\\\\ncatalog (str)\\\\nunityschema (str)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nunityschema ProjectIdKey\\\\n\\\\nParameters str\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.emitter.mcpbuilder.FolderKey(data)\\\\nBases\\\\n\\\\ndata (Any)\\\\nplatform (str)\\\\ninstance (str | None)\\\\nbackcompat_instance_for_guid (str | None)\\\\nfolder_abs_path (str)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nfolder_abs_path PlatformKey\\\\n\\\\nParameters str\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.emitter.mcpbuilder.DatahubKeyJSONEncoder(*, skipkeys=False, ensureascii=True, checkcircular=True, allownan=True, sortkeys=False, indent=None, separators=None, default=None)\\\\nBases\\\\ndef default(self, o)\\\\n        iterable = iter(o)\\\\n    except TypeError\\\\n        return list(iterable)\\\\n    # Let the base class default method raise the TypeError\\\\n    return JSONEncoder.default(self, o)\\\\n\\\\n\\\\n\\\\nParameters\\\\nAny\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\ndatahub.emitter.mcpbuilder.adddomaintoentitywu(entityurn, domainurn)\\\\n\\\\nParameters\\\\nIterable[MetadataWorkUnit]\\\\n\\\\n\\\\n\\\\n\\\\n\\\\ndatahub.emitter.mcpbuilder.addownertoentitywu(entitytype, entityurn, ownerurn)\\\\n\\\\nParameters\\\\nIterable[MetadataWorkUnit]\\\\n\\\\n\\\\n\\\\n\\\\n\\\\ndatahub.emitter.mcpbuilder.addtagstoentitywu(entitytype, entityurn, tags)\\\\n\\\\nParameters\\\\nIterable[MetadataWorkUnit]\\\\n\\\\n\\\\n\\\\n\\\\n\\\\ndatahub.emitter.mcpbuilder.wrapaspectasworkunit(entityName, entityUrn, aspectName, aspect)\\\\n\\\\nParameters\\\\nMetadataWorkUnit\\\\n\\\\n\\\\n\\\\n\\\\n\\\\ndatahub.emitter.mcpbuilder.gencontainers(containerkey, name, subtypes, parentcontainerkey=None, extraproperties=None, domainurn=None, description=None, ownerurn=None, externalurl=None, tags=None, qualifiedname=None, created=None, lastmodified=None)\\\\n\\\\nParameters\\\\nIterable[MetadataWorkUnit]\\\\n\\\\n\\\\n\\\\n\\\\n\\\\ndatahub.emitter.mcpbuilder.adddatasettocontainer(containerkey, dataseturn)\\\\n\\\\nParameters\\\\nIterable[MetadataWorkUnit]\\\\n\\\\n\\\\n\\\\n\\\\n\\\\ndatahub.emitter.mcpbuilder.addentitytocontainer(containerkey, entitytype, entityurn)\\\\n\\\\nParameters\\\\nIterable[MetadataWorkUnit]\\\\n\\\\n\\\\n\\\\n\\\\n\\\\ndatahub.emitter.mcpbuilder.mcpsfrommce(mce)\\\\n\\\\nParameters\\\\nIterable[MetadataChangeProposalWrapper]\\\\n\\\\n\\\\n\\\\n\\\\n\\\\ndatahub.emitter.mcpbuilder.createembedmcp(urn, embedurl)\\\\n\\\\nParameters\\\\nMetadataChangeProposalWrapper\\\\n\\\\n\\\\n\\\\n\\\\n\\"}}>","sidebar":"overviewSidebar"},"python-sdk/clients":{"id":"python-sdk/clients","title":"Client","description":"\\\\n\\\\n\\\\nThe Kafka emitter or Rest emitter can be used to push metadata to DataHub.\\\\nThe DataHub graph client extends the Rest emitter with additional functionality.\\\\n\\\\n\\\\nclass datahub.emitter.restemitter.DataHubRestEmitter(gmsserver, token=None, connecttimeoutsec=None, readtimeoutsec=None, retrystatuscodes=None, retrymethods=None, retrymaxtimes=None, extraheaders=None, cacertificatepath=None, servertelemetryid=None, disablesslverification=False)\\\\nBases\\\\n\\\\ngmsserver (str)\\\\ntoken (Optional[str])\\\\nconnecttimeoutsec (Optional[float])\\\\nreadtimeoutsec (Optional[float])\\\\nretrystatuscodes (Optional[List[int]])\\\\nretrymethods (Optional[List[str]])\\\\nretrymaxtimes (Optional[int])\\\\nextraheaders (Optional[Dict[str, str]])\\\\ncacertificatepath (Optional[str])\\\\nservertelemetryid (Optional[str])\\\\ndisablesslverification (bool)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\ntestconnection()\\\\n\\\\nReturn type\\\\n\\\\nitem (Union[MetadataChangeEventClass, MetadataChangeProposalClass, MetadataChangeProposalWrapper, UsageAggregationClass])\\\\ncallback (Optional[Callable[[Exception, str], None]])\\\\n\\\\n\\\\nReturn type\\\\nmce (MetadataChangeEventClass)\\\\n\\\\nReturn type\\\\nmcp (Union[MetadataChangeProposalClass, MetadataChangeProposalWrapper])\\\\n\\\\nReturn type\\\\nusageStats (UsageAggregationClass)\\\\n\\\\nReturn type\\\\nNone\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\ndatahub.emitter.restemitter.DatahubRestEmitter\\\\nalias of DataHubRestEmitter\\\\n\\\\n\\\\n\\\\nclass datahub.emitter.kafkaemitter.KafkaEmitterConfig(**data)\\\\nBases\\\\n\\\\ndata (Any)\\\\nconnection (KafkaProducerConnectionConfig)\\\\ntopicroutes (Dict[str, str])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nconnection Dict[str, str]\\\\n\\\\n\\\\n\\\\nclassmethod validatetopicroutes(v)\\\\n\\\\nParameters\\\\nDict[str, str]\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.emitter.kafkaemitter.DatahubKafkaEmitter(config)\\\\nBases\\\\nconfig (KafkaEmitterConfig)\\\\n\\\\n\\\\n\\\\n\\\\nemit(item, callback=None)\\\\n\\\\nParameters\\\\nNone\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nemitmceasync(mce, callback)\\\\n\\\\nParameters\\\\nNone\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nemitmcpasync(mcp, callback)\\\\n\\\\nParameters\\\\nNone\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nflush()\\\\n\\\\nReturn type ConfigModel\\\\nConfiguration class for holding connectivity to datahub gms\\\\n\\\\nParameters str\\\\n\\\\n\\\\n\\\\ntoken Optional[int]\\\\n\\\\n\\\\n\\\\nretrystatuscodes Optional[int]\\\\n\\\\n\\\\n\\\\nextraheaders Optional[str]\\\\n\\\\n\\\\n\\\\nmaxthreads bool\\\\n\\\\n\\\\n\\\\n\\\\ndatahub.ingestion.graph.client.DataHubGraphConfig\\\\nalias of DatahubClientConfig\\\\n\\\\n\\\\n\\\\nclass datahub.ingestion.graph.client.DataHubGraph(config)\\\\nBases\\\\nconfig (DatahubClientConfig)\\\\n\\\\n\\\\n\\\\n\\\\ngetaspect(entityurn, aspecttype, version=0)\\\\nGet an aspect for an entity.\\\\n\\\\nParameters\\\\nOptional[TypeVar(Aspect, bound= Aspect)]\\\\n\\\\nReturns\\\\n\\\\nTypeError \\\\u2013 if the aspect type is a timeseries aspect\\\\nHttpError \\\\u2013 if the HTTP response is not a 200 or a 404\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\ngetaspectv2(entityurn, aspecttype, aspect, aspecttypename=None, version=0)\\\\n\\\\nParameters\\\\nOptional[TypeVar(Aspect, bound= Aspect)]\\\\n\\\\n\\\\n\\\\n\\\\n\\\\ngetconfig()\\\\n\\\\nReturn type\\\\nentityurn (str)\\\\n\\\\nReturn type\\\\nentityurn (str)\\\\n\\\\nReturn type\\\\nentityurn (str)\\\\n\\\\nReturn type\\\\nentityurn (str)\\\\n\\\\nReturn type\\\\nentityurn (str)\\\\n\\\\nReturn type\\\\nentityurn (str)\\\\n\\\\nReturn type\\\\nentityurn (str)\\\\n\\\\nReturn type\\\\nentityurn (str)\\\\n\\\\nReturn type\\\\n\\\\nentityurn (str)\\\\nstarttimestamp (int)\\\\nendtimestamp (int)\\\\n\\\\n\\\\nReturn type\\\\n\\\\nentitytype (str)\\\\nstart (int)\\\\ncount (int)\\\\n\\\\n\\\\nReturn type\\\\n\\\\nentityurn (str)\\\\naspecttype (Type[TypeVar(Aspect, bound= Aspect)])\\\\nfiltercriteriamap (Dict[str, str])\\\\n\\\\n\\\\nReturn type Do not use this method to determine if an entity exists!\\\\nThis method will always return an entity, even if it doesn\\\\u2019t exist. This is an issue with how DataHub server\\\\nresponds to these calls, and will be fixed automatically when the server-side issue is fixed.\\\\n\\\\nParameters\\\\nOptional[Dict[str, Optional[TypeVar(Aspect, bound= Aspect)]]]\\\\n\\\\nReturns\\\\nHttpError \\\\u2013 if the HTTP response is not a 200 or a 404\\\\n\\\\n\\\\n\\\\n\\\\n\\\\ngetdomainurnbyname(domainname)\\\\nRetrieve a domain urn based on its name. Returns None if there is no match found\\\\n\\\\nParameters\\\\nOptional[str]\\\\n\\\\n\\\\n\\\\n\\\\n\\\\ngetcontainerurnsbyfilter(env=None, searchquery=\'*\')\\\\nReturn container urns that match based on query\\\\n\\\\nParameters\\\\nIterable[str]\\\\n\\\\n\\\\n\\\\n\\\\n\\\\ngetsearchresults(start=0, count=1, entity=\'dataset\')\\\\n\\\\nParameters\\\\nDict\\\\n\\\\n\\\\n\\\\n\\\\n\\\\ngetaspectcounts(aspect, urnlike=None)\\\\n\\\\nParameters\\\\nint\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nclass RelationshipDirection(value)\\\\nBases object\\\\n\\\\nParameters str\\\\n\\\\n\\\\n\\\\nrelationshiptype\\\\n\\\\nentityurn (str)\\\\nrelationshiptypes (List[str])\\\\ndirection (RelationshipDirection)\\\\n\\\\n\\\\nReturn type\\\\nDataHubGraph\\\\n\\\\n\\\\n\\\\n\\\\n\\"}}>","sidebar":"overviewSidebar"},"python-sdk/models":{"id":"python-sdk/models","title":"Models","description":"\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.AccessLevelClass\\\\nBases DictWrapper\\\\nArray field type.\\\\n\\\\nParameters None | List[str]\\\\nList of types this array holds.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.AssertionInfoClass(type, customProperties=None, externalUrl=None, datasetAssertion=None)\\\\nBases\\\\n\\\\ntype (Union[str, AssertionTypeClass])\\\\ncustomProperties (Optional[Dict[str, str]])\\\\nexternalUrl (Optional[str])\\\\ndatasetAssertion (Optional[DatasetAssertionInfoClass])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty customProperties None | DatasetAssertionInfoClass\\\\nDataset Assertion information when type is DATASET\\\\n\\\\n\\\\n\\\\nproperty externalUrl str | AssertionTypeClass\\\\nType of assertion. Assertion types can evolve to span Datasets, Flows (Pipelines), Models, Features etc.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.AssertionKeyClass(assertionId)\\\\nBases\\\\nassertionId (str)\\\\n\\\\n\\\\n\\\\n\\\\nproperty assertionId DictWrapper\\\\nThe result of running an assertion\\\\n\\\\nParameters None | float\\\\nObserved aggregate value for evaluated batch\\\\n\\\\n\\\\n\\\\nproperty externalUrl None | int\\\\nNumber of rows with missing value for evaluated batch\\\\n\\\\n\\\\n\\\\nproperty nativeResults None | int\\\\nNumber of rows for evaluated batch\\\\n\\\\n\\\\n\\\\nproperty type None | int\\\\nNumber of rows with unexpected value for evaluated batch\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.AssertionResultTypeClass\\\\nBases Aspect\\\\nAn event representing the current status of evaluating an assertion on a batch.\\\\nAssertionRunEvent should be used for reporting the status of a run as an assertion evaluation progresses.\\\\n\\\\nParameters ClassVar[str] = \'timeseries\'\\\\n\\\\n\\\\n\\\\nproperty asserteeUrn str\\\\n\\\\n\\\\n\\\\nproperty batchSpec None | TimeWindowSizeClass\\\\nGranularity of the event if applicable\\\\n\\\\n\\\\n\\\\nproperty messageId PartitionSpecClass | None\\\\nThe optional partition specification.\\\\n\\\\n\\\\n\\\\nproperty result str\\\\nNative (platform-specific) identifier for this run\\\\n\\\\n\\\\n\\\\nproperty runtimeContext str | AssertionRunStatusClass\\\\nThe status of the assertion run as per this timeseries event.\\\\n\\\\n\\\\n\\\\nproperty timestampMillis object\\\\nThe Assertion Run has completed\\\\n\\\\n\\\\nCOMPLETE = \'COMPLETE\'\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.AssertionStdAggregationClass\\\\nBases object\\\\nA boolean operator that is applied on the input to an assertion, after an aggregation function has been applied.\\\\n\\\\n\\\\nBETWEEN = \'BETWEEN\'\\\\nValue being asserted is less than a max value. Requires \\\\u2018value\\\\u2019 parameter.\\\\n\\\\n\\\\n\\\\nCONTAIN = \'CONTAIN\'\\\\nValue being asserted ends with value. Requires \\\\u2018value\\\\u2019 parameter.\\\\n\\\\n\\\\n\\\\nENDWITH = \'ENDWITH\'\\\\nValue being asserted starts with value. Requires \\\\u2018value\\\\u2019 parameter.\\\\n\\\\n\\\\n\\\\nEQUALTO = \'EQUALTO\'\\\\nValue being asserted is not null. Requires no parameters.\\\\n\\\\n\\\\n\\\\nGREATERTHAN = \'GREATERTHAN\'\\\\nValue being asserted is greater than or equal to some value. Requires \\\\u2018value\\\\u2019 parameter.\\\\n\\\\n\\\\n\\\\nGREATERTHANOREQUALTO = \'GREATERTHANOREQUALTO\'\\\\nValue being asserted is equal to value. Requires \\\\u2018value\\\\u2019 parameter.\\\\n\\\\n\\\\n\\\\nIN = \'IN\'\\\\nValue being asserted is not in one of the array values. Requires \\\\u2018value\\\\u2019 parameter.\\\\n\\\\n\\\\n\\\\nLESSTHAN = \'LESSTHAN\'\\\\nValue being asserted is less than or equal to some value. Requires \\\\u2018value\\\\u2019 parameter.\\\\n\\\\n\\\\n\\\\nLESSTHANOREQUALTO = \'LESSTHANOREQUALTO\'\\\\nValue being asserted is greater than some value. Requires \\\\u2018value\\\\u2019 parameter.\\\\n\\\\n\\\\n\\\\nNOTIN = \'NOTIN\'\\\\nOther\\\\n\\\\n\\\\n\\\\nNOTNULL = \'NOTNULL\'\\\\nValue being asserted contains value. Requires \\\\u2018value\\\\u2019 parameter.\\\\n\\\\n\\\\n\\\\nREGEXMATCH = \'REGEXMATCH\'\\\\nValue being asserted is one of the array values. Requires \\\\u2018value\\\\u2019 parameter.\\\\n\\\\n\\\\n\\\\nSTARTWITH = \'STARTWITH\'\\\\nValue being asserted matches the regex value. Requires \\\\u2018value\\\\u2019 parameter.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.AssertionStdParameterClass(value, type)\\\\nBases\\\\n\\\\nvalue (str)\\\\ntype (Union[str, AssertionStdParameterTypeClass])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty type str\\\\nThe parameter value\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.AssertionStdParameterTypeClass\\\\nBases DictWrapper\\\\nParameters for AssertionStdOperators.\\\\n\\\\nParameters None | AssertionStdParameterClass\\\\nThe maxValue parameter of an assertion\\\\n\\\\n\\\\n\\\\nproperty minValue None | AssertionStdParameterClass\\\\nThe value parameter of an assertion\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.AssertionTypeClass\\\\nBases DictWrapper\\\\nData captured on a resource/association/sub-resource level giving insight into when that resource/association/sub-resource moved into a particular lifecycle stage, and who acted to move it into that specific lifecycle stage.\\\\n\\\\nParameters str\\\\nThe entity (e.g. a member URN) which will be credited for moving the resource/association/sub-resource into the specific lifecycle stage. It is also the one used to authorize the change.\\\\n\\\\n\\\\n\\\\nproperty impersonator None | str\\\\nwas the change created by an automated process, or manually.\\\\n\\\\nType int\\\\nWhen did the resource/association/sub-resource move into the specific lifecycle stage represented by this AuditEvent.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.AzkabanJobTypeClass\\\\nBases//azkaban.readthedocs.io/en/latest/jobTypes.html#java-job-type\\\\n\\\\n\\\\n\\\\nGLUE = \'GLUE\'\\\\n\\\\n\\\\n\\\\nHADOOPJAVA = \'HADOOPJAVA\'\\\\nIn large part, this is the same Command type. The difference is its ability to talk to a Hadoop cluster\\\\nsecurely, via Hadoop tokens.\\\\n\\\\n\\\\n\\\\nHADOOPSHELL = \'HADOOPSHELL\'\\\\nHive type is for running Hive jobs.\\\\n\\\\n\\\\n\\\\nHIVE = \'HIVE\'\\\\nPig type is for running Pig jobs.\\\\n\\\\n\\\\n\\\\nPIG = \'PIG\'\\\\nSQL is for running Presto, mysql queries etc\\\\n\\\\n\\\\n\\\\nSQL = \'SQL\'\\\\nGlue type is for running AWS Glue job transforms.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.BaseDataClass(dataset, motivation=None, preProcessing=None)\\\\nBases\\\\n\\\\ndataset (str)\\\\nmotivation (Optional[str])\\\\npreProcessing (Optional[List[str]])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty dataset None | str\\\\nWhy was this dataset chosen?\\\\n\\\\n\\\\n\\\\nproperty preProcessing DictWrapper\\\\nA batch on which certain operations, e.g. data quality evaluation, is done.\\\\n\\\\nParameters Dict[str, str]\\\\nCustom property bag.\\\\n\\\\n\\\\n\\\\nproperty limit None | str\\\\nThe native identifier as specified by the system operating on the batch.\\\\n\\\\n\\\\n\\\\nproperty query DictWrapper\\\\nSchema text of binary JSON schema.\\\\n\\\\nParameters str\\\\nThe native schema text for binary JSON file format.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.BooleanTypeClass\\\\nBases Aspect\\\\nShared aspect containing Browse Paths to be indexed for an entity.\\\\n\\\\nParameters List[str]\\\\nA list of valid browse paths for the entity.\\\\nBrowse paths are expected to be forward slash-separated strings. For example DictWrapper\\\\nBytes field type.\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.CalendarIntervalClass\\\\nBases DictWrapper\\\\nThis section should list additional concerns that were not covered in the previous sections. For example, did the results suggest any further testing? Were there any relevant groups that were not represented in the evaluation dataset? Are there additional recommendations for model use?\\\\n\\\\nParameters None | str\\\\nCaveat Description\\\\nFor ex None | List[str]\\\\nRelevant groups that were not represented in the evaluation dataset?\\\\n\\\\n\\\\n\\\\nproperty needsFurtherTesting Aspect\\\\nThis section should list additional concerns that were not covered in the previous sections. For example, did the results suggest any further testing? Were there any relevant groups that were not represented in the evaluation dataset? Are there additional recommendations for model use?\\\\n\\\\nParameters None | CaveatDetailsClass\\\\nThis section should list additional concerns that were not covered in the previous sections. For example, did the results suggest any further testing? Were there any relevant groups that were not represented in the evaluation dataset?\\\\n\\\\n\\\\n\\\\nproperty idealDatasetCharacteristics None | str\\\\nRecommendations on where this MLModel should be used.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.ChangeAuditStampsClass(created=None, lastModified=None, deleted=None)\\\\nBases\\\\n\\\\ncreated (Optional[AuditStampClass])\\\\nlastModified (Optional[AuditStampClass])\\\\ndeleted (Optional[AuditStampClass])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty created None | AuditStampClass\\\\nAn AuditStamp corresponding to the deletion of this resource/association/sub-resource. Logically, deleted MUST have a later timestamp than creation. It may or may not have the same time as lastModified depending upon the resource/association/sub-resource semantics.\\\\n\\\\n\\\\n\\\\nproperty lastModified object\\\\nDescriptor for a change action\\\\n\\\\n\\\\nCREATE = \'CREATE\'\\\\nNOT SUPPORTED YET\\\\nupdate if exists. otherwise fail\\\\n\\\\n\\\\n\\\\nDELETE = \'DELETE\'\\\\nNOT SUPPORTED YET\\\\npatch the changes instead of full replace\\\\n\\\\n\\\\n\\\\nPATCH = \'PATCH\'\\\\nRestate an aspect, eg. in a index refresh.\\\\n\\\\n\\\\n\\\\nRESTATE = \'RESTATE\'\\\\n\\\\n\\\\n\\\\nUPDATE = \'UPDATE\'\\\\nNOT SUPPORTED YET\\\\ndelete action\\\\n\\\\n\\\\n\\\\nUPSERT = \'UPSERT\'\\\\nNOT SUPPORTED YET\\\\ninsert if not exists. otherwise fail\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.ChartCellClass(cellId, changeAuditStamps, cellTitle=None)\\\\nBases\\\\n\\\\ncellId (str)\\\\nchangeAuditStamps (ChangeAuditStampsClass)\\\\ncellTitle (Optional[str])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty cellId None | str\\\\nTitle of the cell\\\\n\\\\n\\\\n\\\\nproperty changeAuditStamps Aspect\\\\nInformation about a chart\\\\n\\\\nParameters None | str | AccessLevelClass\\\\nAccess level for the chart\\\\n\\\\n\\\\n\\\\nproperty chartUrl Dict[str, str]\\\\nCustom property bag.\\\\n\\\\n\\\\n\\\\nproperty description None | str\\\\nURL where the reference exist\\\\n\\\\n\\\\n\\\\nproperty inputEdges None | List[str]\\\\nData sources for the chart\\\\nDeprecated! Use inputEdges instead.\\\\n\\\\n\\\\n\\\\nproperty lastModified None | int\\\\nThe time when this chart last refreshed\\\\n\\\\n\\\\n\\\\nproperty title None | str | ChartTypeClass\\\\nType of the chart\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.ChartKeyClass(dashboardTool, chartId)\\\\nBases\\\\n\\\\ndashboardTool (str)\\\\nchartId (str)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty chartId str\\\\nThe name of the dashboard tool such as looker, redash etc.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.ChartQueryClass(rawQuery, type)\\\\nBases\\\\n\\\\nrawQuery (str)\\\\ntype (Union[str, ChartQueryTypeClass])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty rawQuery str | ChartQueryTypeClass\\\\nChart query type\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.ChartQueryTypeClass\\\\nBases DictWrapper\\\\nA metadata snapshot for a specific Chart entity.\\\\n\\\\nParameters List[ChartKeyClass | ChartInfoClass | ChartQueryClass | EditableChartPropertiesClass | OwnershipClass | StatusClass | GlobalTagsClass | BrowsePathsClass | GlossaryTermsClass | InstitutionalMemoryClass | DataPlatformInstanceClass]\\\\nThe list of metadata aspects associated with the chart. Depending on the use case, this can either be all, or a selection, of supported aspects.\\\\n\\\\n\\\\n\\\\nproperty urn object\\\\nThe various types of charts\\\\n\\\\n\\\\nAREA = \'AREA\'\\\\n\\\\n\\\\n\\\\nBAR = \'BAR\'\\\\nChart showing a Pie chart\\\\n\\\\n\\\\n\\\\nBOXPLOT = \'BOXPLOT\'\\\\n\\\\n\\\\n\\\\nCOHORT = \'COHORT\'\\\\n\\\\n\\\\n\\\\nHISTOGRAM = \'HISTOGRAM\'\\\\n\\\\n\\\\n\\\\nLINE = \'LINE\'\\\\n\\\\n\\\\n\\\\nPIE = \'PIE\'\\\\nChart showing a Scatter plot\\\\n\\\\n\\\\n\\\\nSCATTER = \'SCATTER\'\\\\nChart showing a table\\\\n\\\\n\\\\n\\\\nTABLE = \'TABLE\'\\\\nChart showing Markdown formatted text\\\\n\\\\n\\\\n\\\\nTEXT = \'TEXT\'\\\\n\\\\n\\\\n\\\\nWORDCLOUD = \'WORDCLOUD\'\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.ChartUsageStatisticsClass(timestampMillis, eventGranularity=None, partitionSpec=None, messageId=None, viewsCount=None, uniqueUserCount=None, userCounts=None)\\\\nBases\\\\n\\\\ntimestampMillis (int)\\\\neventGranularity (Optional[TimeWindowSizeClass])\\\\npartitionSpec (Optional[PartitionSpecClass])\\\\nmessageId (Optional[str])\\\\nviewsCount (Optional[int])\\\\nuniqueUserCount (Optional[int])\\\\nuserCounts (Optional[List[ChartUserUsageCountsClass]])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nASPECTTYPE None | TimeWindowSizeClass\\\\nGranularity of the event if applicable\\\\n\\\\n\\\\n\\\\nproperty messageId PartitionSpecClass | None\\\\nThe optional partition specification.\\\\n\\\\n\\\\n\\\\nproperty timestampMillis None | int\\\\nUnique user count\\\\n\\\\n\\\\n\\\\nproperty userCounts None | int\\\\nThe total number of times chart has been viewed\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.ChartUserUsageCountsClass(user, viewsCount=None)\\\\nBases\\\\n\\\\nuser (str)\\\\nviewsCount (Optional[int])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty user None | int\\\\nThe number of times the user has viewed the chart\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.ConditionClass\\\\nBases\\\\nRepresent the relation\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nENDWITH = \'ENDWITH\'\\\\nfield = value, e.g. platform = hdfs\\\\n\\\\nType\\\\nRepresent the relation\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nGREATERTHAN = \'GREATERTHAN\'\\\\nRepresent the relation greater than or equal to, e.g. ownerCount &gt;= 5\\\\n\\\\n\\\\n\\\\nGREATERTHANOREQUALTO = \'GREATERTHANOREQUALTO\'\\\\nString field is one of the array values to, e.g. name in [\\\\u201cProfile\\\\u201d, \\\\u201cEvent\\\\u201d]\\\\n\\\\nType\\\\nRepresent the relation\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nSTARTWITH = \'STARTWITH\'\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.ConjunctiveCriterionClass(and)\\\\nBases\\\\nand (List[CriterionClass])\\\\n\\\\n\\\\n\\\\n\\\\nproperty and Aspect\\\\nLink from an asset to its parent container\\\\n\\\\nParameters str\\\\nThe parent container of an asset\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.ContainerKeyClass(guid=None)\\\\nBases\\\\nguid (Optional[str])\\\\n\\\\n\\\\n\\\\n\\\\nproperty guid Aspect\\\\nInformation about a Asset Container as received from a 3rd party source system\\\\n\\\\nParameters None | TimeStampClass\\\\nA timestamp documenting when the asset was created in the source Data Platform (not on DataHub)\\\\n\\\\n\\\\n\\\\nproperty customProperties None | str\\\\nDescription of the Asset Container as it exists inside a source system\\\\n\\\\n\\\\n\\\\nproperty externalUrl None | TimeStampClass\\\\nA timestamp documenting when the asset was last modified in the source Data Platform (not on DataHub)\\\\n\\\\n\\\\n\\\\nproperty name None | str\\\\nFully-qualified name of the Container\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.CorpGroupEditableInfoClass(description=None, pictureLink=None, slack=None, email=None)\\\\nBases\\\\n\\\\ndescription (Optional[str])\\\\npictureLink (Optional[str])\\\\nslack (Optional[str])\\\\nemail (Optional[str])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty description None | str\\\\nEmail address to contact the group\\\\n\\\\n\\\\n\\\\nproperty pictureLink None | str\\\\nSlack channel for the group\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.CorpGroupInfoClass(admins, members, groups, displayName=None, email=None, description=None, slack=None, created=None)\\\\nBases\\\\n\\\\nadmins (List[str])\\\\nmembers (List[str])\\\\ngroups (List[str])\\\\ndisplayName (Optional[str])\\\\nemail (Optional[str])\\\\ndescription (Optional[str])\\\\nslack (Optional[str])\\\\ncreated (Optional[AuditStampClass])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty admins None | AuditStampClass\\\\nCreated Audit stamp\\\\n\\\\n\\\\n\\\\nproperty description None | str\\\\nThe name of the group.\\\\n\\\\n\\\\n\\\\nproperty email List[str]\\\\nList of groups in this group.\\\\nDeprecated! This field is unused.\\\\n\\\\n\\\\n\\\\nproperty members None | str\\\\nSlack channel for the group\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.CorpGroupKeyClass(name)\\\\nBases\\\\nname (str)\\\\n\\\\n\\\\n\\\\n\\\\nproperty name DictWrapper\\\\nA metadata snapshot for a specific CorpGroup entity.\\\\n\\\\nParameters List[CorpGroupKeyClass | CorpGroupInfoClass | GlobalTagsClass | StatusClass]\\\\nThe list of metadata aspects associated with the LdapUser. Depending on the use case, this can either be all, or a selection, of supported aspects.\\\\n\\\\n\\\\n\\\\nproperty urn DictWrapper\\\\nSettings for a user around the appearance of their DataHub UI\\\\n\\\\nParameters None | bool\\\\nFlag whether the user should see a homepage with only datasets, charts and dashboards. Intended for users\\\\nwho have less operational use cases for the datahub tool.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.CorpUserCredentialsClass(salt, hashedPassword, passwordResetToken=None, passwordResetTokenExpirationTimeMillis=None)\\\\nBases\\\\n\\\\nsalt (str)\\\\nhashedPassword (str)\\\\npasswordResetToken (Optional[str])\\\\npasswordResetTokenExpirationTimeMillis (Optional[int])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty hashedPassword None | str\\\\nOptional token needed to reset a user\\\\u2019s password. Can only be set by the admin.\\\\n\\\\n\\\\n\\\\nproperty passwordResetTokenExpirationTimeMillis str\\\\nSalt used to hash password\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.CorpUserEditableInfoClass(aboutMe=None, teams=None, skills=None, pictureLink=None, displayName=None, title=None, slack=None, phone=None, email=None)\\\\nBases\\\\n\\\\naboutMe (Optional[str])\\\\nteams (Optional[List[str]])\\\\nskills (Optional[List[str]])\\\\npictureLink (Optional[str])\\\\ndisplayName (Optional[str])\\\\ntitle (Optional[str])\\\\nslack (Optional[str])\\\\nphone (Optional[str])\\\\nemail (Optional[str])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty aboutMe None | str\\\\nDataHub-native display name\\\\n\\\\n\\\\n\\\\nproperty email None | str\\\\nPhone number to contact the user\\\\n\\\\n\\\\n\\\\nproperty pictureLink List[str]\\\\nSkills that the user possesses e.g. Machine Learning\\\\n\\\\n\\\\n\\\\nproperty slack List[str]\\\\nTeams that the user belongs to e.g. Metadata\\\\n\\\\n\\\\n\\\\nproperty title Aspect\\\\nLinkedin corp user information\\\\n\\\\nParameters bool\\\\n//iwww.corp.linkedin.com/wiki/cf/display/GTSD/Accessing+Active+Directory+via+LDAP+tools\\\\n\\\\nType\\\\nhttps\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty countryCode Dict[str, str]\\\\nCustom property bag.\\\\n\\\\n\\\\n\\\\nproperty departmentId None | str\\\\ndepartment name this user belong to\\\\n\\\\n\\\\n\\\\nproperty displayName None | str\\\\nemail address of this user\\\\n\\\\n\\\\n\\\\nproperty firstName None | str\\\\nCommon name of this user, format is firstName + lastName (split by a whitespace)\\\\n\\\\n\\\\n\\\\nproperty lastName None | str\\\\ndirect manager of this user\\\\n\\\\n\\\\n\\\\nproperty title Aspect\\\\nKey for a CorpUser\\\\n\\\\nParameters str\\\\nThe name of the AD/LDAP user.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.CorpUserSettingsClass(appearance, views=None)\\\\nBases\\\\n\\\\nappearance (CorpUserAppearanceSettingsClass)\\\\nviews (Optional[CorpUserViewsSettingsClass])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty appearance None | CorpUserViewsSettingsClass\\\\nUser preferences for the Views feature.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.CorpUserSnapshotClass(urn, aspects)\\\\nBases\\\\n\\\\nurn (str)\\\\naspects (List[Union[CorpUserKeyClass, CorpUserInfoClass, CorpUserEditableInfoClass, CorpUserStatusClass, GroupMembershipClass, GlobalTagsClass, StatusClass]])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty aspects str\\\\nURN for the entity the metadata snapshot is associated with.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.CorpUserStatusClass(status, lastModified)\\\\nBases\\\\n\\\\nstatus (str)\\\\nlastModified (AuditStampClass)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty lastModified str\\\\nStatus of the user, e.g. PROVISIONED / ACTIVE / SUSPENDED\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.CorpUserViewsSettingsClass(defaultView=None)\\\\nBases\\\\ndefaultView (Optional[str])\\\\n\\\\n\\\\n\\\\n\\\\nproperty defaultView Aspect\\\\n\\\\nParameters CostCostClass\\\\n\\\\n\\\\n\\\\nproperty costType DictWrapper\\\\n\\\\nParameters None | str\\\\n\\\\n\\\\n\\\\nproperty costId str | CostCostDiscriminatorClass\\\\nContains the name of the field that has its value set.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.CostCostDiscriminatorClass\\\\nBases object\\\\nType of Cost Code\\\\n\\\\n\\\\nORGCOSTTYPE = \'ORGCOSTTYPE\'\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.CriterionClass(field, value, values=None, condition=None, negated=None)\\\\nBases\\\\n\\\\nfield (str)\\\\nvalue (str)\\\\nvalues (Optional[List[str]])\\\\ncondition (Union[str, ConditionClass, None])\\\\nnegated (Optional[bool])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty condition str\\\\nThe name of the field that the criterion refers to\\\\n\\\\n\\\\n\\\\nproperty negated str\\\\nThe value of the intended field\\\\n\\\\n\\\\n\\\\nproperty values Aspect\\\\nInformation about a dashboard\\\\n\\\\nParameters None | str | AccessLevelClass\\\\nAccess level for the dashboard\\\\n\\\\n\\\\n\\\\nproperty chartEdges List[str]\\\\nCharts in a dashboard\\\\nDeprecated! Use chartEdges instead.\\\\n\\\\n\\\\n\\\\nproperty customProperties None | str\\\\nURL for the dashboard. This could be used as an external link on DataHub to allow users access/view the dashboard\\\\n\\\\n\\\\n\\\\nproperty datasetEdges List[str]\\\\nDatasets consumed by a dashboard\\\\nDeprecated! Use datasetEdges instead.\\\\n\\\\n\\\\n\\\\nproperty description None | str\\\\nURL where the reference exist\\\\n\\\\n\\\\n\\\\nproperty lastModified None | int\\\\nThe time when this dashboard last refreshed\\\\n\\\\n\\\\n\\\\nproperty title Aspect\\\\nKey for a Dashboard\\\\n\\\\nParameters str\\\\nUnique id for the dashboard. This id should be globally unique for a dashboarding tool even when there are multiple deployments of it. As an example, dashboard URL could be used here for Looker such as \\\\u2018looker.linkedin.com/dashboards/1234\\\\u2019\\\\n\\\\n\\\\n\\\\nproperty dashboardTool DictWrapper\\\\nA metadata snapshot for a specific Dashboard entity.\\\\n\\\\nParameters List[DashboardKeyClass | DashboardInfoClass | EditableDashboardPropertiesClass | OwnershipClass | StatusClass | GlobalTagsClass | BrowsePathsClass | GlossaryTermsClass | InstitutionalMemoryClass | DataPlatformInstanceClass]\\\\nThe list of metadata aspects associated with the dashboard. Depending on the use case, this can either be all, or a selection, of supported aspects.\\\\n\\\\n\\\\n\\\\nproperty urn Aspect\\\\nExperimental (Subject to breaking change) \\\\u2013 Stats corresponding to dashboard\\\\u2019s usage.\\\\nIf this aspect represents the latest snapshot of the statistics about a Dashboard, the eventGranularity field should be null.\\\\nIf this aspect represents a bucketed window of usage statistics (e.g. over a day), then the eventGranularity field should be set accordingly.\\\\n\\\\nParameters ClassVar[str] = \'timeseries\'\\\\n\\\\n\\\\n\\\\nproperty eventGranularity None | int\\\\nThe total number of dashboard executions (refreshes / syncs)\\\\n\\\\n\\\\n\\\\nproperty favoritesCount None | int\\\\nLast viewed at\\\\nThis should not be set in cases where statistics are windowed.\\\\n\\\\n\\\\n\\\\nproperty messageId PartitionSpecClass | None\\\\nThe optional partition specification.\\\\n\\\\n\\\\n\\\\nproperty timestampMillis None | int\\\\nUnique user count\\\\n\\\\n\\\\n\\\\nproperty userCounts None | int\\\\nThe total number of times dashboard has been viewed\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DashboardUserUsageCountsClass(user, viewsCount=None, executionsCount=None, usageCount=None, userEmail=None)\\\\nBases\\\\n\\\\nuser (str)\\\\nviewsCount (Optional[int])\\\\nexecutionsCount (Optional[int])\\\\nusageCount (Optional[int])\\\\nuserEmail (Optional[str])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty executionsCount None | int\\\\nNormalized numeric metric representing user\\\\u2019s dashboard usage \\\\u2013 the number of times the user executed or viewed the dashboard.\\\\n\\\\n\\\\n\\\\nproperty user None | str\\\\nIf useremail is set, we attempt to resolve the user\\\\u2019s urn upon ingest\\\\n\\\\n\\\\n\\\\nproperty viewsCount Aspect\\\\nInformation about a Data processing flow\\\\n\\\\nParameters None | TimeStampClass\\\\nA timestamp documenting when the asset was created in the source Data Platform (not on DataHub)\\\\n\\\\n\\\\n\\\\nproperty customProperties None | str\\\\nFlow description\\\\n\\\\n\\\\n\\\\nproperty externalUrl None | TimeStampClass\\\\nA timestamp documenting when the asset was last modified in the source Data Platform (not on DataHub)\\\\n\\\\n\\\\n\\\\nproperty name None | str\\\\nOptional project/namespace associated with the flow\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DataFlowKeyClass(orchestrator, flowId, cluster)\\\\nBases\\\\n\\\\norchestrator (str)\\\\nflowId (str)\\\\ncluster (str)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty cluster str\\\\nUnique Identifier of the data flow\\\\n\\\\n\\\\n\\\\nproperty orchestrator DictWrapper\\\\nA metadata snapshot for a specific DataFlow entity.\\\\n\\\\nParameters List[DataFlowKeyClass | DataFlowInfoClass | EditableDataFlowPropertiesClass | OwnershipClass | StatusClass | GlobalTagsClass | BrowsePathsClass | GlossaryTermsClass | InstitutionalMemoryClass | DataPlatformInstanceClass]\\\\nThe list of metadata aspects associated with the data flow. Depending on the use case, this can either be all, or a selection, of supported aspects.\\\\n\\\\n\\\\n\\\\nproperty urn Aspect\\\\nInformation about a DataHub Access Token\\\\n\\\\nParameters str\\\\nUrn of the actor to which this access token belongs to.\\\\n\\\\n\\\\n\\\\nproperty createdAt None | str\\\\nDescription of the token if defined.\\\\n\\\\n\\\\n\\\\nproperty expiresAt str\\\\nUser defined name for the access token if defined.\\\\n\\\\n\\\\n\\\\nproperty ownerUrn Aspect\\\\nKey for a DataHub Access Token\\\\n\\\\nParameters str\\\\nAccess token\\\\u2019s SHA-256 hashed JWT signature\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DataHubActorFilterClass(users=None, groups=None, resourceOwners=None, allUsers=None, allGroups=None, roles=None)\\\\nBases\\\\n\\\\nusers (Optional[List[str]])\\\\ngroups (Optional[List[str]])\\\\nresourceOwners (Optional[bool])\\\\nallUsers (Optional[bool])\\\\nallGroups (Optional[bool])\\\\nroles (Optional[List[str]])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty allGroups bool\\\\nWhether the filter should apply to all users.\\\\n\\\\n\\\\n\\\\nproperty groups bool\\\\nWhether the filter should return true for owners of a particular resource.\\\\nOnly applies to policies of type \\\\u2018Metadata\\\\u2019, which have a resource associated with them.\\\\n\\\\n\\\\n\\\\nproperty roles None | List[str]\\\\nA specific set of users to apply the policy to (disjunctive)\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DataHubIngestionSourceConfigClass(recipe, version=None, executorId=None, debugMode=None)\\\\nBases\\\\n\\\\nrecipe (str)\\\\nversion (Optional[str])\\\\nexecutorId (Optional[str])\\\\ndebugMode (Optional[bool])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty debugMode None | str\\\\nThe id of the executor to use to execute the ingestion run\\\\n\\\\n\\\\n\\\\nproperty recipe None | str\\\\nThe PyPI version of the datahub CLI to use when executing a recipe\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DataHubIngestionSourceInfoClass(name, type, config, platform=None, schedule=None)\\\\nBases\\\\n\\\\nname (str)\\\\ntype (str)\\\\nconfig (DataHubIngestionSourceConfigClass)\\\\nplatform (Optional[str])\\\\nschedule (Optional[DataHubIngestionSourceScheduleClass])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty config str\\\\nThe display name of the ingestion source\\\\n\\\\n\\\\n\\\\nproperty platform None | DataHubIngestionSourceScheduleClass\\\\nThe schedule on which the ingestion source is executed\\\\n\\\\n\\\\n\\\\nproperty type Aspect\\\\nKey for a DataHub ingestion source\\\\n\\\\nParameters str\\\\nA unique id for the Ingestion Source, either generated or provided\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DataHubIngestionSourceScheduleClass(interval, timezone)\\\\nBases\\\\n\\\\ninterval (str)\\\\ntimezone (str)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty interval str\\\\nTimezone in which the cron interval applies, e.g. America/Los Angeles\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DataHubPolicyInfoClass(displayName, description, type, state, privileges, actors, resources=None, editable=None, lastUpdatedTimestamp=None)\\\\nBases\\\\n\\\\ndisplayName (str)\\\\ndescription (str)\\\\ntype (str)\\\\nstate (str)\\\\nprivileges (List[str])\\\\nactors (DataHubActorFilterClass)\\\\nresources (Optional[DataHubResourceFilterClass])\\\\neditable (Optional[bool])\\\\nlastUpdatedTimestamp (Optional[int])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty actors str\\\\nDescription of the Policy\\\\n\\\\n\\\\n\\\\nproperty displayName bool\\\\nWhether the policy should be editable via the UI\\\\n\\\\n\\\\n\\\\nproperty lastUpdatedTimestamp List[str]\\\\nThe privileges that the policy grants.\\\\n\\\\n\\\\n\\\\nproperty resources str\\\\nThe state of policy, ACTIVE or INACTIVE\\\\n\\\\n\\\\n\\\\nproperty type Aspect\\\\nKey for a DataHub Policy\\\\n\\\\nParameters str\\\\nA unique id for the DataHub access policy record. Generated on the server side at policy creation time.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DataHubPolicySnapshotClass(urn, aspects)\\\\nBases\\\\n\\\\nurn (str)\\\\naspects (List[Union[DataHubPolicyKeyClass, DataHubPolicyInfoClass]])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty aspects str\\\\nURN for the entity the metadata snapshot is associated with.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DataHubResourceFilterClass(type=None, resources=None, allResources=None, filter=None)\\\\nBases\\\\n\\\\ntype (Optional[str])\\\\nresources (Optional[List[str]])\\\\nallResources (Optional[bool])\\\\nfilter (Optional[PolicyMatchFilterClass])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty allResources None | PolicyMatchFilterClass\\\\nFilter to apply privileges to\\\\n\\\\n\\\\n\\\\nproperty resources None | str\\\\nThe type of resource that the policy applies to. This will most often be a data asset entity name, for\\\\nexample \\\\u2018dataset\\\\u2019. It is not strictly required because in the future we will want to support filtering a resource\\\\nby domain, as well.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DataHubRetentionConfigClass(retention)\\\\nBases\\\\nretention (RetentionClass)\\\\n\\\\n\\\\n\\\\n\\\\nproperty retention Aspect\\\\nKey for a DataHub Retention\\\\n\\\\nParameters str\\\\nAspect name to apply retention to. * (or empty) for applying defaults.\\\\n\\\\n\\\\n\\\\nproperty entityName DictWrapper\\\\nA metadata snapshot for DataHub Access Policy data.\\\\n\\\\nParameters List[DataHubRetentionKeyClass | DataHubRetentionConfigClass]\\\\nThe list of metadata aspects associated with the DataHub access policy.\\\\n\\\\n\\\\n\\\\nproperty urn Aspect\\\\nInformation about a DataHub Role.\\\\n\\\\nParameters str\\\\nDescription of the Role\\\\n\\\\n\\\\n\\\\nproperty editable str\\\\nName of the Role\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DataHubRoleKeyClass(id)\\\\nBases\\\\nid (str)\\\\n\\\\n\\\\n\\\\n\\\\nproperty id Aspect\\\\nKey for a DataHub Secret\\\\n\\\\nParameters str\\\\nA unique id for the Secret\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DataHubSecretValueClass(name, value, description=None, created=None)\\\\nBases\\\\n\\\\nname (str)\\\\nvalue (str)\\\\ndescription (Optional[str])\\\\ncreated (Optional[AuditStampClass])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty created None | str\\\\nDescription of the secret\\\\n\\\\n\\\\n\\\\nproperty name str\\\\nThe AES-encrypted value of the DataHub secret.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DataHubStepStateKeyClass(id)\\\\nBases\\\\nid (str)\\\\n\\\\n\\\\n\\\\n\\\\nproperty id Aspect\\\\nThe properties associated with a DataHub step state\\\\n\\\\nParameters AuditStampClass\\\\nAudit stamp describing the last person to update it.\\\\n\\\\n\\\\n\\\\nproperty properties Aspect\\\\nKey for a DataHubUpgrade\\\\n\\\\nParameters str\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DataHubUpgradeRequestClass(timestampMs, version)\\\\nBases\\\\n\\\\ntimestampMs (int)\\\\nversion (str)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty timestampMs str\\\\nVersion of this upgrade\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DataHubUpgradeResultClass(timestampMs, result=None)\\\\nBases\\\\n\\\\ntimestampMs (int)\\\\nresult (Optional[Dict[str, str]])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty result int\\\\nTimestamp when we started this DataHubUpgrade\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DataHubViewDefinitionClass(entityTypes, filter)\\\\nBases\\\\n\\\\nentityTypes (List[str])\\\\nfilter (FilterClass)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty entityTypes FilterClass\\\\nThe filter criteria, which represents the view itself\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DataHubViewInfoClass(name, type, definition, created, lastModified, description=None)\\\\nBases Understand whether an entity type filter is required.\\\\n\\\\nParameters AuditStampClass\\\\nAudit stamp capturing the time and actor who created the View.\\\\n\\\\n\\\\n\\\\nproperty definition None | str\\\\nDescription of the view\\\\n\\\\n\\\\n\\\\nproperty lastModified str\\\\nThe name of the View\\\\n\\\\n\\\\n\\\\nproperty type Aspect\\\\nKey for a DataHub View\\\\n\\\\nParameters str\\\\nA unique id for the View\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DataHubViewTypeClass\\\\nBases Aspect\\\\nInformation about a Data processing job\\\\n\\\\nParameters None | TimeStampClass\\\\nA timestamp documenting when the asset was created in the source Data Platform (not on DataHub)\\\\n\\\\n\\\\n\\\\nproperty customProperties None | str\\\\nJob description\\\\n\\\\n\\\\n\\\\nproperty externalUrl None | str\\\\nDataFlow urn that this job is part of\\\\n\\\\n\\\\n\\\\nproperty lastModified str\\\\nJob name\\\\n\\\\n\\\\n\\\\nproperty status str | AzkabanJobTypeClass\\\\nDatajob type\\\\nNOTE* Aspect\\\\nInformation about the inputs and outputs of a Data processing job\\\\n\\\\nParameters None | List[FineGrainedLineageClass]\\\\nFine-grained column-level lineages\\\\n\\\\n\\\\n\\\\nproperty inputDatajobEdges None | List[str]\\\\nInput datajobs that this data job depends on\\\\nDeprecated! Use inputDatajobEdges instead.\\\\n\\\\n\\\\n\\\\nproperty inputDatasetEdges None | List[str]\\\\nFields of the input datasets used by this job\\\\n\\\\n\\\\n\\\\nproperty inputDatasets None | List[EdgeClass]\\\\nOutput datasets produced by the data job during processing\\\\n\\\\n\\\\n\\\\nproperty outputDatasetFields List[str]\\\\nOutput datasets produced by the data job during processing\\\\nDeprecated! Use outputDatasetEdges instead.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DataJobKeyClass(flow, jobId)\\\\nBases\\\\n\\\\nflow (str)\\\\njobId (str)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty flow str\\\\nUnique Identifier of the data job\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DataJobSnapshotClass(urn, aspects)\\\\nBases\\\\n\\\\nurn (str)\\\\naspects (List[Union[DataJobKeyClass, DataJobInfoClass, DataJobInputOutputClass, EditableDataJobPropertiesClass, OwnershipClass, StatusClass, GlobalTagsClass, BrowsePathsClass, GlossaryTermsClass, InstitutionalMemoryClass, DataPlatformInstanceClass]])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty aspects str\\\\nURN for the entity the metadata snapshot is associated with.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DataPlatformInfoClass(name, type, datasetNameDelimiter, displayName=None, logoUrl=None)\\\\nBases\\\\n\\\\nname (str)\\\\ntype (Union[str, PlatformTypeClass])\\\\ndatasetNameDelimiter (str)\\\\ndisplayName (Optional[str])\\\\nlogoUrl (Optional[str])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty datasetNameDelimiter None | str\\\\nThe name that will be used for displaying a platform type.\\\\n\\\\n\\\\n\\\\nproperty logoUrl str\\\\nName of the data platform\\\\n\\\\n\\\\n\\\\nproperty type Aspect\\\\nThe specific instance of the data platform that this entity belongs to\\\\n\\\\nParameters None | str\\\\nInstance of the data platform (e.g. db instance)\\\\n\\\\n\\\\n\\\\nproperty platform Aspect\\\\nKey for a Dataset\\\\n\\\\nParameters str\\\\nUnique instance id\\\\n\\\\n\\\\n\\\\nproperty platform Aspect\\\\nProperties associated with a Data Platform Instance\\\\n\\\\nParameters Dict[str, str]\\\\nCustom property bag.\\\\n\\\\n\\\\n\\\\nproperty description None | str\\\\nURL where the reference exist\\\\n\\\\n\\\\n\\\\nproperty name Aspect\\\\nKey for a Data Platform\\\\n\\\\nParameters str\\\\nData platform name i.e. hdfs, oracle, espresso\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DataPlatformSnapshotClass(urn, aspects)\\\\nBases\\\\n\\\\nurn (str)\\\\naspects (List[Union[DataPlatformKeyClass, DataPlatformInfoClass]])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty aspects str\\\\nURN for the entity the metadata snapshot is associated with.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DataProcessInfoClass(inputs=None, outputs=None)\\\\nBases\\\\n\\\\ninputs (Optional[List[str]])\\\\noutputs (Optional[List[str]])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty inputs None | List[str]\\\\nthe outputs of the data process\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DataProcessInstanceInputClass(inputs)\\\\nBases\\\\ninputs (List[str])\\\\n\\\\n\\\\n\\\\n\\\\nproperty inputs Aspect\\\\nKey for an Asset DataProcessInstance\\\\n\\\\nParameters str\\\\nA unique id for the DataProcessInstance . Should be separate from the name used for displaying a DataProcessInstance.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DataProcessInstanceOutputClass(outputs)\\\\nBases\\\\noutputs (List[str])\\\\n\\\\n\\\\n\\\\n\\\\nproperty outputs Aspect\\\\nThe inputs and outputs of this data process\\\\n\\\\nParameters AuditStampClass\\\\nAudit stamp containing who reported the lineage and when\\\\n\\\\n\\\\n\\\\nproperty customProperties None | str\\\\nURL where the reference exist\\\\n\\\\n\\\\n\\\\nproperty name None | str | DataProcessTypeClass\\\\nProcess type\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DataProcessInstanceRelationshipsClass(upstreamInstances, parentTemplate=None, parentInstance=None)\\\\nBases\\\\n\\\\nupstreamInstances (List[str])\\\\nparentTemplate (Optional[str])\\\\nparentInstance (Optional[str])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty parentInstance None | str\\\\nThe parent entity whose run instance it is\\\\n\\\\n\\\\n\\\\nproperty upstreamInstances Aspect\\\\nAn event representing the current status of data process run.\\\\nDataProcessRunEvent should be used for reporting the status of a dataProcess\\\\u2019 run.\\\\n\\\\nParameters ClassVar[str] = \'timeseries\'\\\\n\\\\n\\\\n\\\\nproperty attempt None | TimeWindowSizeClass\\\\nGranularity of the event if applicable\\\\n\\\\n\\\\n\\\\nproperty externalUrl None | str\\\\nThe optional messageId, if provided serves as a custom user-defined unique identifier for an aspect value.\\\\n\\\\n\\\\n\\\\nproperty partitionSpec None | DataProcessInstanceRunResultClass\\\\nThe final result of the Data Processing run.\\\\n\\\\n\\\\n\\\\nproperty status int\\\\nThe event timestamp field as epoch at UTC in milli seconds.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DataProcessInstanceRunResultClass(type, nativeResultType)\\\\nBases\\\\n\\\\ntype (Union[str, RunResultTypeClass])\\\\nnativeResultType (str)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty nativeResultType str | RunResultTypeClass\\\\nThe final result, e.g. SUCCESS, FAILURE, SKIPPED, or UPFORRETRY.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DataProcessKeyClass(name, orchestrator, origin)\\\\nBases\\\\n\\\\nname (str)\\\\norchestrator (str)\\\\norigin (Union[str, FabricTypeClass])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty name str\\\\nStandardized Orchestrator where data process is defined.\\\\nTODO str | FabricTypeClass\\\\nFabric type where dataset belongs to or where it was generated.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DataProcessRunStatusClass\\\\nBases DictWrapper\\\\nA metadata snapshot for a specific Data process entity.\\\\n\\\\nParameters List[DataProcessKeyClass | OwnershipClass | DataProcessInfoClass | StatusClass]\\\\nThe list of metadata aspects associated with the data process. Depending on the use case, this can either be all, or a selection, of supported aspects.\\\\n\\\\n\\\\n\\\\nproperty urn object\\\\n\\\\n\\\\nBATCHADHOC = \'BATCHADHOC\'\\\\n\\\\n\\\\n\\\\nBATCHSCHEDULED = \'BATCHSCHEDULED\'\\\\n\\\\n\\\\n\\\\nSTREAMING = \'STREAMING\'\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DatahubIngestionCheckpointClass(timestampMillis, pipelineName, platformInstanceId, config, state, runId, eventGranularity=None, partitionSpec=None, messageId=None)\\\\nBases\\\\n\\\\ntimestampMillis (int)\\\\npipelineName (str)\\\\nplatformInstanceId (str)\\\\nconfig (str)\\\\nstate (IngestionCheckpointStateClass)\\\\nrunId (str)\\\\neventGranularity (Optional[TimeWindowSizeClass])\\\\npartitionSpec (Optional[PartitionSpecClass])\\\\nmessageId (Optional[str])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nASPECTTYPE str\\\\nJson-encoded string representation of the non-secret members of the config .\\\\n\\\\n\\\\n\\\\nproperty eventGranularity None | str\\\\nThe optional messageId, if provided serves as a custom user-defined unique identifier for an aspect value.\\\\n\\\\n\\\\n\\\\nproperty partitionSpec str\\\\nThe name of the pipeline that ran ingestion, a stable unique user provided identifier.\\\\ne.g. mysnowflake1-to-datahub.\\\\n\\\\n\\\\n\\\\nproperty platformInstanceId Bigquery project ids, MySQL hostnames etc.\\\\n\\\\n\\\\n\\\\nproperty runId IngestionCheckpointStateClass\\\\nOpaque blob of the state representation.\\\\n\\\\n\\\\n\\\\nproperty timestampMillis Aspect\\\\nSummary of a datahub ingestion run for a given platform.\\\\n\\\\nParameters ClassVar[str] = \'timeseries\'\\\\n\\\\n\\\\n\\\\nproperty availableMemory None | str\\\\nThe non-sensitive key-value pairs of the yaml config used as json string.\\\\n\\\\n\\\\n\\\\nproperty customsummary None | TimeWindowSizeClass\\\\nGranularity of the event if applicable\\\\n\\\\n\\\\n\\\\nproperty messageId None | int\\\\nThe total number of aspects produced across all entities.\\\\n\\\\n\\\\n\\\\nproperty numEntities None | int\\\\nNumber of entities skipped.\\\\n\\\\n\\\\n\\\\nproperty numErrors None | int\\\\nThe number of events produced (MCE + MCP).\\\\n\\\\n\\\\n\\\\nproperty numProcessors None | int\\\\nTotal number of sink API calls.\\\\n\\\\n\\\\n\\\\nproperty numSourceAPICalls None | int\\\\nNumber of warnings generated.\\\\n\\\\n\\\\n\\\\nproperty numWorkUnitsCommitted None | int\\\\nThe number of workunits that are produced.\\\\n\\\\n\\\\n\\\\nproperty operatingSystemName PartitionSpecClass | None\\\\nThe optional partition specification.\\\\n\\\\n\\\\n\\\\nproperty pipelineName str\\\\nThe id of the instance against which the ingestion pipeline ran.\\\\ne.g. str\\\\nThe runId for this pipeline instance.\\\\n\\\\n\\\\n\\\\nproperty runStatus None | str\\\\nThe software version of this ingestion.\\\\n\\\\n\\\\n\\\\nproperty systemHostName int\\\\nThe event timestamp field as epoch at UTC in milli seconds.\\\\n\\\\n\\\\n\\\\nproperty totalLatencySinkAPICalls None | int\\\\nTotal latency across all source API calls.\\\\n\\\\n\\\\n\\\\nproperty totalMemory DictWrapper\\\\nAttributes that are applicable to single-Dataset Assertions\\\\n\\\\nParameters None | str | AssertionStdAggregationClass\\\\nStandardized assertion operator\\\\n\\\\n\\\\n\\\\nproperty dataset None | List[str]\\\\nOne or more dataset schema fields that are targeted by this assertion\\\\n\\\\n\\\\n\\\\nproperty logic None | Dict[str, str]\\\\nNative parameters required for the assertion.\\\\n\\\\n\\\\n\\\\nproperty nativeType str | AssertionStdOperatorClass\\\\nStandardized assertion operator\\\\n\\\\n\\\\n\\\\nproperty parameters str | DatasetAssertionScopeClass\\\\nScope of the Assertion. What part of the dataset does this assertion apply to?\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DatasetAssertionScopeClass\\\\nBases Aspect\\\\nDataset deprecation status\\\\nDeprecated! This aspect is deprecated in favor of the more-general-purpose \\\\u2018Deprecation\\\\u2019 aspect.\\\\n\\\\nParameters None | str\\\\nThe corpuser URN which will be credited for modifying this deprecation content.\\\\n\\\\n\\\\n\\\\nproperty decommissionTime bool\\\\nWhether the dataset is deprecated by owner.\\\\n\\\\n\\\\n\\\\nproperty note DictWrapper\\\\nFor non-urn based foregin keys.\\\\n\\\\nParameters List[str]\\\\nList of fields in hosting(current) SchemaMetadata that conform a foreign key. List can contain a single entry or multiple entries if several entries in hosting schema conform a foreign key in a single parent dataset.\\\\n\\\\n\\\\n\\\\nproperty parentDataset str\\\\nSchemaField@fieldPath that uniquely identify field in parent dataset that this field references.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DatasetFieldMappingClass(created, transformation, sourceFields, destinationField)\\\\nBases\\\\n\\\\ncreated (AuditStampClass)\\\\ntransformation (Union[str, TransformationTypeClass, UDFTransformerClass])\\\\nsourceFields (List[str])\\\\ndestinationField (str)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty created str\\\\nDestination field which is derived from source fields\\\\n\\\\n\\\\n\\\\nproperty sourceFields str | TransformationTypeClass | UDFTransformerClass\\\\nTransfomration function between the fields involved\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DatasetFieldProfileClass(fieldPath, uniqueCount=None, uniqueProportion=None, nullCount=None, nullProportion=None, min=None, max=None, mean=None, median=None, stdev=None, quantiles=None, distinctValueFrequencies=None, histogram=None, sampleValues=None)\\\\nBases\\\\n\\\\nfieldPath (str)\\\\nuniqueCount (Optional[int])\\\\nuniqueProportion (Optional[float])\\\\nnullCount (Optional[int])\\\\nnullProportion (Optional[float])\\\\nmin (Optional[str])\\\\nmax (Optional[str])\\\\nmean (Optional[str])\\\\nmedian (Optional[str])\\\\nstdev (Optional[str])\\\\nquantiles (Optional[List[QuantileClass]])\\\\ndistinctValueFrequencies (Optional[List[ValueFrequencyClass]])\\\\nhistogram (Optional[HistogramClass])\\\\nsampleValues (Optional[List[str]])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty distinctValueFrequencies str\\\\n\\\\n\\\\n\\\\nproperty histogram None | str\\\\n\\\\n\\\\n\\\\nproperty mean None | str\\\\n\\\\n\\\\n\\\\nproperty min None | int\\\\n\\\\n\\\\n\\\\nproperty nullProportion None | List[QuantileClass]\\\\n\\\\n\\\\n\\\\nproperty sampleValues None | str\\\\n\\\\n\\\\n\\\\nproperty uniqueCount None | float\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DatasetFieldUsageCountsClass(fieldPath, count)\\\\nBases\\\\n\\\\nfieldPath (str)\\\\ncount (int)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty count str\\\\nThe name of the field.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DatasetKeyClass(platform, name, origin)\\\\nBases\\\\n\\\\nplatform (str)\\\\nname (str)\\\\norigin (Union[str, FabricTypeClass])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty name str | FabricTypeClass\\\\nFabric type where dataset belongs to or where it was generated.\\\\n\\\\n\\\\n\\\\nproperty platform object\\\\nThe various types of supported dataset lineage\\\\n\\\\n\\\\nCOPY = \'COPY\'\\\\nTransformed data with modification (format or content change)\\\\n\\\\n\\\\n\\\\nTRANSFORMED = \'TRANSFORMED\'\\\\nRepresents a view defined on the sources e.g. Hive view defined on underlying hive tables or a Hive table pointing to a HDFS dataset or DALI view defined on multiple sources\\\\n\\\\n\\\\n\\\\nVIEW = \'VIEW\'\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DatasetProfileClass(timestampMillis, eventGranularity=None, partitionSpec=None, messageId=None, rowCount=None, columnCount=None, fieldProfiles=None, sizeInBytes=None)\\\\nBases\\\\n\\\\ntimestampMillis (int)\\\\neventGranularity (Optional[TimeWindowSizeClass])\\\\npartitionSpec (Optional[PartitionSpecClass])\\\\nmessageId (Optional[str])\\\\nrowCount (Optional[int])\\\\ncolumnCount (Optional[int])\\\\nfieldProfiles (Optional[List[DatasetFieldProfileClass]])\\\\nsizeInBytes (Optional[int])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nASPECTTYPE None | int\\\\nThe total number of columns (or schema fields)\\\\n\\\\n\\\\n\\\\nproperty eventGranularity None | List[DatasetFieldProfileClass]\\\\nProfiles for each column (or schema field)\\\\n\\\\n\\\\n\\\\nproperty messageId PartitionSpecClass | None\\\\nThe optional partition specification.\\\\n\\\\n\\\\n\\\\nproperty rowCount None | int\\\\nStorage size in bytes\\\\n\\\\n\\\\n\\\\nproperty timestampMillis Aspect\\\\nProperties associated with a Dataset\\\\n\\\\nParameters None | TimeStampClass\\\\nA timestamp documenting when the asset was created in the source Data Platform (not on DataHub)\\\\n\\\\n\\\\n\\\\nproperty customProperties None | str\\\\nDocumentation of the dataset\\\\n\\\\n\\\\n\\\\nproperty externalUrl None | TimeStampClass\\\\nA timestamp documenting when the asset was last modified in the source Data Platform (not on DataHub)\\\\n\\\\n\\\\n\\\\nproperty name None | str\\\\nFully-qualified name of the Dataset\\\\n\\\\n\\\\n\\\\nproperty tags None | str\\\\n///dir/filename. Uri should not include any environment specific properties. Some datasets might not have a standardized uri, which makes this field optional (i.e. kafka topic).\\\\n\\\\nType\\\\n///data/tracking/PageViewEvent, file\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DatasetSnapshotClass(urn, aspects)\\\\nBases\\\\n\\\\nurn (str)\\\\naspects (List[Union[DatasetKeyClass, DatasetPropertiesClass, EditableDatasetPropertiesClass, DatasetDeprecationClass, DatasetUpstreamLineageClass, UpstreamLineageClass, InstitutionalMemoryClass, OwnershipClass, StatusClass, SchemaMetadataClass, EditableSchemaMetadataClass, GlobalTagsClass, GlossaryTermsClass, BrowsePathsClass, DataPlatformInstanceClass, ViewPropertiesClass]])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty aspects str\\\\nURN for the entity the metadata snapshot is associated with.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DatasetUpstreamLineageClass(fieldMappings)\\\\nBases\\\\nfieldMappings (List[DatasetFieldMappingClass])\\\\n\\\\n\\\\n\\\\n\\\\nproperty fieldMappings Aspect\\\\nStats corresponding to dataset\\\\u2019s usage.\\\\n\\\\nParameters ClassVar[str] = \'timeseries\'\\\\n\\\\n\\\\n\\\\nproperty eventGranularity None | List[DatasetFieldUsageCountsClass]\\\\nField-level usage stats\\\\n\\\\n\\\\n\\\\nproperty messageId PartitionSpecClass | None\\\\nThe optional partition specification.\\\\n\\\\n\\\\n\\\\nproperty timestampMillis None | List[str]\\\\nFrequent SQL queries; mostly makes sense for datasets in SQL databases\\\\n\\\\n\\\\n\\\\nproperty totalSqlQueries None | int\\\\nUnique user count\\\\n\\\\n\\\\n\\\\nproperty userCounts DictWrapper\\\\nRecords a single user\\\\u2019s usage counts for a given resource\\\\n\\\\nParameters int\\\\nNumber of times the dataset has been used by the user.\\\\n\\\\n\\\\n\\\\nproperty user None | str\\\\nIf useremail is set, we attempt to resolve the user\\\\u2019s urn upon ingest\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DateTypeClass\\\\nBases object\\\\nModel endpoint statuses\\\\n\\\\n\\\\nCREATING = \'CREATING\'\\\\nDeployments being updated.\\\\n\\\\n\\\\n\\\\nDELETING = \'DELETING\'\\\\nDeployments with an error state.\\\\n\\\\n\\\\n\\\\nFAILED = \'FAILED\'\\\\nDeployments with unknown/unmappable state.\\\\n\\\\n\\\\n\\\\nINSERVICE = \'INSERVICE\'\\\\nDeployments being deleted.\\\\n\\\\n\\\\n\\\\nOUTOFSERVICE = \'OUTOFSERVICE\'\\\\nDeployments being created.\\\\n\\\\n\\\\n\\\\nROLLINGBACK = \'ROLLINGBACK\'\\\\nDeployments that are active.\\\\n\\\\n\\\\n\\\\nUNKNOWN = \'UNKNOWN\'\\\\n\\\\n\\\\n\\\\nUPDATING = \'UPDATING\'\\\\nDeployments being reverted to a previous version.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DeprecationClass(deprecated, note, actor, decommissionTime=None)\\\\nBases\\\\n\\\\ndeprecated (bool)\\\\nnote (str)\\\\nactor (str)\\\\ndecommissionTime (Optional[int])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty actor None | int\\\\nThe time user plan to decommission this entity.\\\\n\\\\n\\\\n\\\\nproperty deprecated str\\\\nAdditional information about the entity deprecation plan, such as the wiki, doc, RB.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DomainKeyClass(id)\\\\nBases\\\\nid (str)\\\\n\\\\n\\\\n\\\\n\\\\nproperty id Aspect\\\\nInformation about a Domain\\\\n\\\\nParameters None | AuditStampClass\\\\nCreated Audit stamp\\\\n\\\\n\\\\n\\\\nproperty description str\\\\nDisplay name of the Domain\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.DomainsClass(domains)\\\\nBases\\\\ndomains (List[str])\\\\n\\\\n\\\\n\\\\n\\\\nproperty domains DictWrapper\\\\nInformation about a relatonship edge.\\\\n\\\\nParameters AuditStampClass\\\\nAudit stamp containing who created this relationship edge and when\\\\n\\\\n\\\\n\\\\nproperty destinationUrn AuditStampClass\\\\nAudit stamp containing who last modified this relationship edge and when\\\\n\\\\n\\\\n\\\\nproperty properties str\\\\nUrn of the source of this relationship edge.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.EditableChartPropertiesClass(created=None, lastModified=None, deleted=None, description=None)\\\\nBases\\\\n\\\\ncreated (Optional[AuditStampClass])\\\\nlastModified (Optional[AuditStampClass])\\\\ndeleted (Optional[AuditStampClass])\\\\ndescription (Optional[str])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty created None | AuditStampClass\\\\nAn AuditStamp corresponding to the deletion of this resource/association/sub-resource. Logically, deleted MUST have a later timestamp than creation. It may or may not have the same time as lastModified depending upon the resource/association/sub-resource semantics.\\\\n\\\\n\\\\n\\\\nproperty description AuditStampClass\\\\nAn AuditStamp corresponding to the last modification of this resource/association/sub-resource. If no modification has happened since creation, lastModified should be the same as created. A value of 0 for time indicates missing data.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.EditableContainerPropertiesClass(description=None)\\\\nBases\\\\ndescription (Optional[str])\\\\n\\\\n\\\\n\\\\n\\\\nproperty description Aspect\\\\nStores editable changes made to properties. This separates changes made from\\\\ningestion pipelines and edits in the UI to avoid accidental overwrites of user-provided data by ingestion pipelines\\\\n\\\\nParameters AuditStampClass\\\\nAn AuditStamp corresponding to the creation of this resource/association/sub-resource. A value of 0 for time indicates missing data.\\\\n\\\\n\\\\n\\\\nproperty deleted None | str\\\\nEdited documentation of the dashboard\\\\n\\\\n\\\\n\\\\nproperty lastModified Aspect\\\\nStores editable changes made to properties. This separates changes made from\\\\ningestion pipelines and edits in the UI to avoid accidental overwrites of user-provided data by ingestion pipelines\\\\n\\\\nParameters AuditStampClass\\\\nAn AuditStamp corresponding to the creation of this resource/association/sub-resource. A value of 0 for time indicates missing data.\\\\n\\\\n\\\\n\\\\nproperty deleted None | str\\\\nEdited documentation of the data flow\\\\n\\\\n\\\\n\\\\nproperty lastModified Aspect\\\\nStores editable changes made to properties. This separates changes made from\\\\ningestion pipelines and edits in the UI to avoid accidental overwrites of user-provided data by ingestion pipelines\\\\n\\\\nParameters AuditStampClass\\\\nAn AuditStamp corresponding to the creation of this resource/association/sub-resource. A value of 0 for time indicates missing data.\\\\n\\\\n\\\\n\\\\nproperty deleted None | str\\\\nEdited documentation of the data job\\\\n\\\\n\\\\n\\\\nproperty lastModified Aspect\\\\nEditableDatasetProperties stores editable changes made to dataset properties. This separates changes made from\\\\ningestion pipelines and edits in the UI to avoid accidental overwrites of user-provided data by ingestion pipelines\\\\n\\\\nParameters AuditStampClass\\\\nAn AuditStamp corresponding to the creation of this resource/association/sub-resource. A value of 0 for time indicates missing data.\\\\n\\\\n\\\\n\\\\nproperty deleted None | str\\\\nDocumentation of the dataset\\\\n\\\\n\\\\n\\\\nproperty lastModified Aspect\\\\nProperties associated with a MLFeature editable from the UI\\\\n\\\\nParameters None | str\\\\nDocumentation of the MLFeature\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.EditableMLFeatureTablePropertiesClass(description=None)\\\\nBases\\\\ndescription (Optional[str])\\\\n\\\\n\\\\n\\\\n\\\\nproperty description Aspect\\\\nProperties associated with an ML Model Group editable from the UI\\\\n\\\\nParameters None | str\\\\nDocumentation of the ml model group\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.EditableMLModelPropertiesClass(description=None)\\\\nBases\\\\ndescription (Optional[str])\\\\n\\\\n\\\\n\\\\n\\\\nproperty description Aspect\\\\nProperties associated with a MLPrimaryKey editable from the UI\\\\n\\\\nParameters None | str\\\\nDocumentation of the MLPrimaryKey\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.EditableNotebookPropertiesClass(created=None, lastModified=None, deleted=None, description=None)\\\\nBases This is IN BETA version\\\\n\\\\nParameters AuditStampClass\\\\nAn AuditStamp corresponding to the creation of this resource/association/sub-resource. A value of 0 for time indicates missing data.\\\\n\\\\n\\\\n\\\\nproperty deleted None | str\\\\nEdited documentation of the Notebook\\\\n\\\\n\\\\n\\\\nproperty lastModified DictWrapper\\\\nSchemaField to describe metadata related to dataset schema.\\\\n\\\\nParameters None | str\\\\nDescription\\\\n\\\\n\\\\n\\\\nproperty fieldPath None | GlobalTagsClass\\\\nTags associated with the field\\\\n\\\\n\\\\n\\\\nproperty glossaryTerms Aspect\\\\nEditableSchemaMetadata stores editable changes made to schema metadata. This separates changes made from\\\\ningestion pipelines and edits in the UI to avoid accidental overwrites of user-provided data by ingestion pipelines.\\\\n\\\\nParameters AuditStampClass\\\\nAn AuditStamp corresponding to the creation of this resource/association/sub-resource. A value of 0 for time indicates missing data.\\\\n\\\\n\\\\n\\\\nproperty deleted List[EditableSchemaFieldInfoClass]\\\\nClient provided a list of fields from document schema.\\\\n\\\\n\\\\n\\\\nproperty lastModified Aspect\\\\nInformation regarding rendering an embed for an asset.\\\\n\\\\nParameters None | str\\\\nAn embed URL to be rendered inside of an iframe.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.EntityChangeEventClass(entityType, entityUrn, category, operation, auditStamp, version, modifier=None, parameters=None)\\\\nBases\\\\n\\\\nentityType (str)\\\\nentityUrn (str)\\\\ncategory (str)\\\\noperation (str)\\\\nauditStamp (AuditStampClass)\\\\nversion (int)\\\\nmodifier (Optional[str])\\\\nparameters (Optional[ParametersClass])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty auditStamp str\\\\nThe category type (TAG, GLOSSARYTERM, OWNERSHIP, TECHNICALSCHEMA, etc). This is used to determine what the rest of the schema will look like.\\\\n\\\\n\\\\n\\\\nproperty entityType str\\\\nThe urn of the entity which was affected.\\\\n\\\\n\\\\n\\\\nproperty modifier str\\\\nThe operation type. This is used to determine what the rest of the schema will look like.\\\\n\\\\n\\\\n\\\\nproperty parameters int\\\\nThe version of the event type, incremented in integers.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.EnumTypeClass\\\\nBases DictWrapper\\\\nSchema text of an espresso table schema.\\\\n\\\\nParameters str\\\\nThe native espresso document schema.\\\\n\\\\n\\\\n\\\\nproperty tableSchema Aspect\\\\nThis section is intended to demonstrate the ethical considerations that went into MLModel development, surfacing ethical challenges and solutions to stakeholders.\\\\n\\\\nParameters None | List[str]\\\\nDoes the MLModel use any sensitive data (e.g., protected classes)?\\\\n\\\\n\\\\n\\\\nproperty humanLife None | List[str]\\\\nWhat risk mitigation strategies were used during MLModel development?\\\\n\\\\n\\\\n\\\\nproperty risksAndHarms None | List[str]\\\\nAre there any known MLModel use cases that are especially fraught? This may connect directly to the intended use section\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.EvaluationDataClass(evaluationData)\\\\nBases\\\\nevaluationData (List[BaseDataClass])\\\\n\\\\n\\\\n\\\\n\\\\nproperty evaluationData Aspect\\\\nAn request to execution some remote logic or action.\\\\nTODO\\\\n\\\\ntask (str)\\\\nargs (Dict[str, str])\\\\nexecutorId (str)\\\\nsource (ExecutionRequestSourceClass)\\\\nrequestedAt (int)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty args str\\\\nspecify a specific executor to route the request to. If none is provided, a \\\\u201cdefault\\\\u201d executor is used.\\\\n\\\\nType int\\\\nTime at which the execution request input was created\\\\n\\\\n\\\\n\\\\nproperty source str\\\\nThe name of the task to execute, for example RUNINGEST\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.ExecutionRequestKeyClass(id)\\\\nBases\\\\nid (str)\\\\n\\\\n\\\\n\\\\n\\\\nproperty id Aspect\\\\nThe result of an execution request\\\\n\\\\nParameters None | int\\\\nDuration in milliseconds\\\\n\\\\n\\\\n\\\\nproperty report None | int\\\\nTime at which the request was created\\\\n\\\\n\\\\n\\\\nproperty status None | StructuredExecutionReportClass\\\\nA structured report if available.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.ExecutionRequestSignalClass(signal, createdAt, executorId=None)\\\\nBases\\\\n\\\\nsignal (str)\\\\ncreatedAt (AuditStampClass)\\\\nexecutorId (Optional[str])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty createdAt None | str\\\\nspecify a specific executor to route the request to. If none is provided, a \\\\u201cdefault\\\\u201d executor is used.\\\\n\\\\nType str\\\\nThe signal to issue, e.g. KILL\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.ExecutionRequestSourceClass(type, ingestionSource=None)\\\\nBases\\\\n\\\\ntype (str)\\\\ningestionSource (Optional[str])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty ingestionSource str\\\\nThe type of the execution request source, e.g. INGESTIONSOURCE\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.FabricTypeClass\\\\nBases DictWrapper\\\\nRecords field-level usage counts for a given resource\\\\n\\\\nParameters int\\\\n\\\\n\\\\n\\\\nproperty fieldName DictWrapper\\\\nThe filter for finding a record or a collection of records\\\\n\\\\nParameters None | List[CriterionClass]\\\\nDeprecated! A list of conjunctive criterion for the filter. If \\\\u201cor\\\\u201d field is provided, then this field is ignored.\\\\n\\\\n\\\\n\\\\nproperty or DictWrapper\\\\nA fine-grained lineage from upstream fields/datasets to downstream field(s)\\\\n\\\\nParameters float\\\\nThe confidence in this lineage between 0 (low confidence) and 1 (high confidence)\\\\n\\\\n\\\\n\\\\nproperty downstreamType None | List[str]\\\\nDownstream fields in the lineage\\\\n\\\\n\\\\n\\\\nproperty transformOperation str | FineGrainedLineageUpstreamTypeClass\\\\nThe type of upstream entity\\\\n\\\\n\\\\n\\\\nproperty upstreams object\\\\nThe type of downstream field(s) in a fine-grained lineage\\\\n\\\\n\\\\nFIELD = \'FIELD\'\\\\nIndicates that the lineage is for a set of downstream fields\\\\n\\\\n\\\\n\\\\nFIELDSET = \'FIELDSET\'\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.FineGrainedLineageUpstreamTypeClass\\\\nBases DictWrapper\\\\nFixed field type.\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.ForeignKeyConstraintClass(name, foreignFields, sourceFields, foreignDataset)\\\\nBases\\\\n\\\\nname (str)\\\\nforeignFields (List[str])\\\\nsourceFields (List[str])\\\\nforeignDataset (str)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty foreignDataset List[str]\\\\nFields the constraint maps to on the foreign dataset\\\\n\\\\n\\\\n\\\\nproperty name List[str]\\\\nFields the constraint maps to on the source dataset\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.ForeignKeySpecClass(foreignKey)\\\\nBases\\\\nforeignKey (Union[DatasetFieldForeignKeyClass, UrnForeignKeyClass])\\\\n\\\\n\\\\n\\\\n\\\\nproperty foreignKey DictWrapper\\\\nGeneric record structure for serializing an Aspect\\\\n\\\\nParameters str\\\\nThe content type, which represents the fashion in which the aspect was serialized.\\\\nThe only type currently supported is application/json.\\\\n\\\\n\\\\n\\\\nproperty value DictWrapper\\\\nGeneric payload record structure for serializing a Platform Event.\\\\n\\\\nParameters str\\\\nThe content type, which represents the fashion in which the event was serialized.\\\\nThe only type currently supported is application/json.\\\\n\\\\n\\\\n\\\\nproperty value Aspect\\\\nDataHub Global platform settings. Careful - these should not be modified by the outside world!\\\\n\\\\nParameters None | GlobalViewsSettingsClass\\\\nSettings related to the Views Feature\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.GlobalSettingsKeyClass(id)\\\\nBases\\\\nid (str)\\\\n\\\\n\\\\n\\\\n\\\\nproperty idglobalSettings\\\\nId for the settings. There should be only 1 global settings urn\\\\n\\\\nType Aspect\\\\nTag aspect used for applying tags to an entity\\\\n\\\\nParameters List[TagAssociationClass]\\\\nTags associated with a given entity\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.GlobalViewsSettingsClass(defaultView=None)\\\\nBases\\\\ndefaultView (Optional[str])\\\\n\\\\n\\\\n\\\\n\\\\nproperty defaultView Aspect\\\\nProperties associated with a GlossaryNode\\\\n\\\\nParameters str\\\\nDefinition of business node\\\\n\\\\n\\\\n\\\\nproperty id None | str\\\\nDisplay name of the node\\\\n\\\\n\\\\n\\\\nproperty parentNode Aspect\\\\nKey for a GlossaryNode\\\\n\\\\nParameters str\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.GlossaryNodeSnapshotClass(urn, aspects)\\\\nBases\\\\n\\\\nurn (str)\\\\naspects (List[Union[GlossaryNodeKeyClass, GlossaryNodeInfoClass, OwnershipClass, StatusClass]])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty aspects str\\\\nURN for the entity the metadata snapshot is associated with.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.GlossaryRelatedTermsClass(isRelatedTerms=None, hasRelatedTerms=None, values=None, relatedTerms=None)\\\\nBases\\\\n\\\\nisRelatedTerms (Optional[List[str]])\\\\nhasRelatedTerms (Optional[List[str]])\\\\nvalues (Optional[List[str]])\\\\nrelatedTerms (Optional[List[str]])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty hasRelatedTerms None | List[str]\\\\nThe relationship Is A with glossary term\\\\n\\\\n\\\\n\\\\nproperty relatedTerms None | List[str]\\\\nThe relationship Has Value with glossary term.\\\\nThese are fixed value a term has. For example a ColorEnum where RED, GREEN and YELLOW are fixed values.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.GlossaryTermAssociationClass(urn, context=None)\\\\nBases\\\\n\\\\nurn (str)\\\\ncontext (Optional[str])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty context str\\\\nUrn of the applied glossary term\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.GlossaryTermInfoClass(definition, termSource, customProperties=None, id=None, name=None, parentNode=None, sourceRef=None, sourceUrl=None, rawSchema=None)\\\\nBases\\\\n\\\\ndefinition (str)\\\\ntermSource (str)\\\\ncustomProperties (Optional[Dict[str, str]])\\\\nid (Optional[str])\\\\nname (Optional[str])\\\\nparentNode (Optional[str])\\\\nsourceRef (Optional[str])\\\\nsourceUrl (Optional[str])\\\\nrawSchema (Optional[str])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty customProperties str\\\\nDefinition of business term.\\\\n\\\\n\\\\n\\\\nproperty id None | str\\\\nDisplay name of the term\\\\n\\\\n\\\\n\\\\nproperty parentNode None | str\\\\nSchema definition of the glossary term\\\\n\\\\n\\\\n\\\\nproperty sourceRef None | str\\\\n//spec.edmcouncil.org/fibo/ontology/FBC/FinancialInstruments/FinancialInstruments/CashInstrument.\\\\n\\\\nType str\\\\nSource of the Business Term (INTERNAL or EXTERNAL) with default value as INTERNAL\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.GlossaryTermKeyClass(name)\\\\nBases\\\\nname (str)\\\\n\\\\n\\\\n\\\\n\\\\nproperty name DictWrapper\\\\nA metadata snapshot for a specific GlossaryTerm entity.\\\\n\\\\nParameters List[GlossaryTermKeyClass | GlossaryTermInfoClass | OwnershipClass | StatusClass | BrowsePathsClass | GlossaryRelatedTermsClass]\\\\nThe list of metadata aspects associated with the GlossaryTerm. Depending on the use case, this can either be all, or a selection, of supported aspects.\\\\n\\\\n\\\\n\\\\nproperty urn Aspect\\\\nRelated business terms information\\\\n\\\\nParameters AuditStampClass\\\\nAudit stamp containing who reported the related business term\\\\n\\\\n\\\\n\\\\nproperty terms Aspect\\\\nCarries information about the CorpGroups a user is in.\\\\n\\\\nParameters List[str]\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.HistogramClass(boundaries, heights)\\\\nBases\\\\n\\\\nboundaries (List[str])\\\\nheights (List[float])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty boundaries List[float]\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.IngestionCheckpointStateClass(formatVersion, serde, payload=None)\\\\nBases\\\\n\\\\nformatVersion (str)\\\\nserde (str)\\\\npayload (Optional[bytes])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty formatVersion None | bytes\\\\nOpaque blob of the state representation.\\\\n\\\\n\\\\n\\\\nproperty serde DictWrapper\\\\nInformation about a field a chart or dashboard references\\\\n\\\\nParameters None | SchemaFieldClass\\\\nCopied version of the referenced schema field object for indexing purposes\\\\n\\\\n\\\\n\\\\nproperty schemaFieldUrn Aspect\\\\nInformation about the fields a chart or dashboard references\\\\n\\\\nParameters List[InputFieldClass]\\\\nList of fields being referenced\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.InstitutionalMemoryClass(elements)\\\\nBases\\\\nelements (List[InstitutionalMemoryMetadataClass])\\\\n\\\\n\\\\n\\\\n\\\\nproperty elements DictWrapper\\\\nMetadata corresponding to a record of institutional memory.\\\\n\\\\nParameters AuditStampClass\\\\nAudit stamp associated with creation of this record\\\\n\\\\n\\\\n\\\\nproperty description str\\\\nLink to an engineering design document or a wiki page.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.IntendedUseClass(primaryUses=None, primaryUsers=None, outOfScopeUses=None)\\\\nBases\\\\n\\\\nprimaryUses (Optional[List[str]])\\\\nprimaryUsers (Optional[List[Union[str, IntendedUserTypeClass]]])\\\\noutOfScopeUses (Optional[List[str]])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty outOfScopeUses None | List[str | IntendedUserTypeClass]\\\\nPrimary Intended Users - For example, was the MLModel developed for entertainment purposes, for hobbyists, or enterprise solutions?\\\\n\\\\n\\\\n\\\\nproperty primaryUses object\\\\n\\\\n\\\\nENTERPRISE = \'ENTERPRISE\'\\\\n\\\\n\\\\n\\\\nENTERTAINMENT = \'ENTERTAINMENT\'\\\\n\\\\n\\\\n\\\\nHOBBY = \'HOBBY\'\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.InviteTokenClass(token, role=None)\\\\nBases\\\\n\\\\ntoken (str)\\\\nrole (Optional[str])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty role str\\\\nThe encrypted invite token.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.InviteTokenKeyClass(id)\\\\nBases\\\\nid (str)\\\\n\\\\n\\\\n\\\\n\\\\nproperty id object\\\\nJob statuses\\\\n\\\\n\\\\nCOMPLETED = \'COMPLETED\'\\\\nJobs that have failed.\\\\n\\\\n\\\\n\\\\nFAILED = \'FAILED\'\\\\nJobs with unknown status (either unmappable or unavailable)\\\\n\\\\n\\\\n\\\\nINPROGRESS = \'INPROGRESS\'\\\\nJobs being stopped.\\\\n\\\\n\\\\n\\\\nSKIPPED = \'SKIPPED\'\\\\n\\\\n\\\\n\\\\nSTARTING = \'STARTING\'\\\\nJobs currently running.\\\\n\\\\n\\\\n\\\\nSTOPPED = \'STOPPED\'\\\\nJobs with successful completion.\\\\n\\\\n\\\\n\\\\nSTOPPING = \'STOPPING\'\\\\nJobs that have stopped.\\\\n\\\\n\\\\n\\\\nUNKNOWN = \'UNKNOWN\'\\\\nJobs that have been skipped.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.KafkaAuditHeaderClass(time, server, appName, messageId, instance=None, auditVersion=None, fabricUrn=None, clusterConnectionString=None)\\\\nBases\\\\n\\\\ntime (int)\\\\nserver (str)\\\\nappName (str)\\\\nmessageId (bytes)\\\\ninstance (Optional[str])\\\\nauditVersion (Optional[int])\\\\nfabricUrn (Optional[str])\\\\nclusterConnectionString (Optional[str])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty appName None | int\\\\nif the schema has an outer KafkaAuditHeader, use the outer audit header timestamp for bucketing; else if the EventHeader has an inner KafkaAuditHeader use that inner audit header\\\\u2019s timestamp for bucketing\\\\n\\\\nType None | str\\\\nThis is a String that the client uses to establish some kind of connection with the Kafka cluster. The exact format of it depends on specific versions of clients and brokers. This information could potentially identify the fabric and cluster with which the client is producing to or consuming from.\\\\n\\\\n\\\\n\\\\nproperty fabricUrn. See go/fabric.\\\\n\\\\nType\\\\nli\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty instance bytes\\\\nA unique identifier for the message\\\\n\\\\n\\\\n\\\\nproperty server int\\\\nThe time at which the event was emitted into kafka.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.KafkaSchemaClass(documentSchema, keySchema=None)\\\\nBases\\\\n\\\\ndocumentSchema (str)\\\\nkeySchema (Optional[str])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty documentSchema None | str\\\\nThe native kafka key schema as retrieved from Schema Registry\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.KeyValueSchemaClass(keySchema, valueSchema)\\\\nBases\\\\n\\\\nkeySchema (str)\\\\nvalueSchema (str)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty keySchema str\\\\nThe raw schema for the value in the key-value store.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.MLFeatureDataTypeClass\\\\nBases\\\\nSequence Data Type ex\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nNOMINAL = \'NOMINAL\'\\\\nOrdinal data are discrete integers that can be ranked or sorted.\\\\nFor example, the distance between first and second may not be the same as the distance between second and third.\\\\n\\\\n\\\\n\\\\nORDINAL = \'ORDINAL\'\\\\nBinary data is discrete data that can be in only one of two categories - either yes or no, 1 or 0, off or on, etc\\\\n\\\\n\\\\n\\\\nSEQUENCE = \'SEQUENCE\'\\\\nset, frozenset\\\\n\\\\nType\\\\nMapping Data Type ex\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nTIME = \'TIME\'\\\\nInterval data has equal spaces between the numbers and does not represent a temporal pattern.\\\\nExamples include percentages, temperatures, and income.\\\\n\\\\n\\\\n\\\\nUNKNOWN = \'UNKNOWN\'\\\\n\\\\n\\\\n\\\\nUSELESS = \'USELESS\'\\\\nNominal data is made of discrete values with no numerical relationship between the different categories - mean and median are meaningless.\\\\nAnimal species is one example. For example, pig is not higher than bird and lower than fish.\\\\n\\\\n\\\\n\\\\nVIDEO = \'VIDEO\'\\\\nAudio Data\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.MLFeatureKeyClass(featureNamespace, name)\\\\nBases\\\\n\\\\nfeatureNamespace (str)\\\\nname (str)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty featureNamespace str\\\\nName of the feature\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.MLFeaturePropertiesClass(description=None, dataType=None, version=None, sources=None)\\\\nBases\\\\n\\\\ndescription (Optional[str])\\\\ndataType (Union[None, str, MLFeatureDataTypeClass])\\\\nversion (Optional[VersionTagClass])\\\\nsources (Optional[List[str]])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty dataType None | str\\\\nDocumentation of the MLFeature\\\\n\\\\n\\\\n\\\\nproperty sources None | VersionTagClass\\\\nVersion of the MLFeature\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.MLFeatureSnapshotClass(urn, aspects)\\\\nBases\\\\n\\\\nurn (str)\\\\naspects (List[Union[MLFeatureKeyClass, MLFeaturePropertiesClass, OwnershipClass, InstitutionalMemoryClass, StatusClass, DeprecationClass, BrowsePathsClass, GlobalTagsClass, DataPlatformInstanceClass]])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty aspects str\\\\nURN for the entity the metadata snapshot is associated with.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.MLFeatureTableKeyClass(platform, name)\\\\nBases\\\\n\\\\nplatform (str)\\\\nname (str)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty name str\\\\nData platform urn associated with the feature table\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.MLFeatureTablePropertiesClass(customProperties=None, description=None, mlFeatures=None, mlPrimaryKeys=None)\\\\nBases\\\\n\\\\ncustomProperties (Optional[Dict[str, str]])\\\\ndescription (Optional[str])\\\\nmlFeatures (Optional[List[str]])\\\\nmlPrimaryKeys (Optional[List[str]])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty customProperties None | str\\\\nDocumentation of the MLFeatureTable\\\\n\\\\n\\\\n\\\\nproperty mlFeatures None | List[str]\\\\nList of primary keys in the feature table (if multiple, assumed to act as a composite key)\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.MLFeatureTableSnapshotClass(urn, aspects)\\\\nBases\\\\n\\\\nurn (str)\\\\naspects (List[Union[MLFeatureTableKeyClass, MLFeatureTablePropertiesClass, OwnershipClass, InstitutionalMemoryClass, StatusClass, DeprecationClass, BrowsePathsClass, GlobalTagsClass, DataPlatformInstanceClass]])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty aspects str\\\\nURN for the entity the metadata snapshot is associated with.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.MLHyperParamClass(name, description=None, value=None, createdAt=None)\\\\nBases\\\\n\\\\nname (str)\\\\ndescription (Optional[str])\\\\nvalue (Optional[str])\\\\ncreatedAt (Optional[int])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty createdAt None | str\\\\nDocumentation of the MLHyperParam\\\\n\\\\n\\\\n\\\\nproperty name None | str\\\\nThe value of the MLHyperParam\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.MLMetricClass(name, description=None, value=None, createdAt=None)\\\\nBases\\\\n\\\\nname (str)\\\\ndescription (Optional[str])\\\\nvalue (Optional[str])\\\\ncreatedAt (Optional[int])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty createdAt None | str\\\\nDocumentation of the mlMetric\\\\n\\\\n\\\\n\\\\nproperty name None | str\\\\nThe value of the mlMetric\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.MLModelDeploymentKeyClass(platform, name, origin)\\\\nBases\\\\n\\\\nplatform (str)\\\\nname (str)\\\\norigin (Union[str, FabricTypeClass])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty name str | FabricTypeClass\\\\nFabric type where model Deployment belongs to or where it was generated\\\\n\\\\n\\\\n\\\\nproperty platform Aspect\\\\nProperties associated with an ML Model Deployment\\\\n\\\\nParameters None | int\\\\nDate when the MLModelDeployment was developed\\\\n\\\\n\\\\n\\\\nproperty customProperties None | str\\\\nDocumentation of the MLModelDeployment\\\\n\\\\n\\\\n\\\\nproperty externalUrl None | str | DeploymentStatusClass\\\\nStatus of the deployment\\\\n\\\\n\\\\n\\\\nproperty version DictWrapper\\\\n\\\\nParameters List[MLModelDeploymentKeyClass | MLModelDeploymentPropertiesClass | OwnershipClass | StatusClass | DeprecationClass | GlobalTagsClass | DataPlatformInstanceClass]\\\\nThe list of metadata aspects associated with the MLModelDeployment. Depending on the use case, this can either be all, or a selection, of supported aspects.\\\\n\\\\n\\\\n\\\\nproperty urn Aspect\\\\nPrompts which affect the performance of the MLModel\\\\n\\\\nParameters None | List[MLModelFactorsClass]\\\\nWhich factors are being reported, and why were these chosen?\\\\n\\\\n\\\\n\\\\nproperty relevantFactors DictWrapper\\\\nFactors affecting the performance of the MLModel.\\\\n\\\\nParameters None | List[str]\\\\nA further factor affecting MLModel performance is the environment in which it is deployed.\\\\n\\\\n\\\\n\\\\nproperty groups None | List[str]\\\\nThe performance of a MLModel can vary depending on what instruments were used to capture the input to the MLModel.\\\\nFor example, a face detection model may perform differently depending on the camera\\\\u2019s hardware and software,\\\\nincluding lens, image stabilization, high dynamic range techniques, and background blurring for portrait mode.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.MLModelGroupKeyClass(platform, name, origin)\\\\nBases\\\\n\\\\nplatform (str)\\\\nname (str)\\\\norigin (Union[str, FabricTypeClass])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty name str | FabricTypeClass\\\\nFabric type where model group belongs to or where it was generated\\\\n\\\\n\\\\n\\\\nproperty platform Aspect\\\\nProperties associated with an ML Model Group\\\\n\\\\nParameters None | int\\\\nDate when the MLModelGroup was developed\\\\n\\\\n\\\\n\\\\nproperty customProperties None | str\\\\nDocumentation of the MLModelGroup\\\\n\\\\n\\\\n\\\\nproperty version DictWrapper\\\\n\\\\nParameters List[MLModelGroupKeyClass | MLModelGroupPropertiesClass | OwnershipClass | StatusClass | DeprecationClass | BrowsePathsClass | GlobalTagsClass | DataPlatformInstanceClass]\\\\nThe list of metadata aspects associated with the MLModelGroup. Depending on the use case, this can either be all, or a selection, of supported aspects.\\\\n\\\\n\\\\n\\\\nproperty urn Aspect\\\\nKey for an ML model\\\\n\\\\nParameters str\\\\nName of the MLModel\\\\n\\\\n\\\\n\\\\nproperty origin str\\\\nStandardized platform urn for the model\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.MLModelPropertiesClass(customProperties=None, externalUrl=None, description=None, date=None, version=None, type=None, hyperParameters=None, hyperParams=None, trainingMetrics=None, onlineMetrics=None, mlFeatures=None, tags=None, deployments=None, trainingJobs=None, downstreamJobs=None, groups=None)\\\\nBases\\\\n\\\\ncustomProperties (Optional[Dict[str, str]])\\\\nexternalUrl (Optional[str])\\\\ndescription (Optional[str])\\\\ndate (Optional[int])\\\\nversion (Optional[VersionTagClass])\\\\ntype (Optional[str])\\\\nhyperParameters (Optional[Dict[str, Union[str, int, float, bool]]])\\\\nhyperParams (Optional[List[MLHyperParamClass]])\\\\ntrainingMetrics (Optional[List[MLMetricClass]])\\\\nonlineMetrics (Optional[List[MLMetricClass]])\\\\nmlFeatures (Optional[List[str]])\\\\ntags (Optional[List[str]])\\\\ndeployments (Optional[List[str]])\\\\ntrainingJobs (Optional[List[str]])\\\\ndownstreamJobs (Optional[List[str]])\\\\ngroups (Optional[List[str]])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty customProperties None | int\\\\nDate when the MLModel was developed\\\\n\\\\n\\\\n\\\\nproperty deployments None | str\\\\nDocumentation of the MLModel\\\\n\\\\n\\\\n\\\\nproperty downstreamJobs None | str\\\\nURL where the reference exist\\\\n\\\\n\\\\n\\\\nproperty groups None | Dict[str, str | int | float | bool]\\\\nHyper Parameters of the MLModel\\\\nNOTE None | List[MLHyperParamClass]\\\\nHyperparameters of the MLModel\\\\n\\\\n\\\\n\\\\nproperty mlFeatures None | List[MLMetricClass]\\\\nMetrics of the MLModel used in production\\\\n\\\\n\\\\n\\\\nproperty tags None | List[str]\\\\nList of jobs (if any) used to train the model\\\\n\\\\n\\\\n\\\\nproperty trainingMetrics None | str\\\\nType of Algorithm or MLModel such as whether it is a Naive Bayes classifier, Convolutional Neural Network, etc\\\\n\\\\n\\\\n\\\\nproperty version DictWrapper\\\\nMLModel Snapshot entity details.\\\\n\\\\nParameters List[MLModelKeyClass | OwnershipClass | MLModelPropertiesClass | IntendedUseClass | MLModelFactorPromptsClass | MetricsClass | EvaluationDataClass | TrainingDataClass | QuantitativeAnalysesClass | EthicalConsiderationsClass | CaveatsAndRecommendationsClass | InstitutionalMemoryClass | SourceCodeClass | StatusClass | CostClass | DeprecationClass | BrowsePathsClass | GlobalTagsClass | DataPlatformInstanceClass]\\\\nThe list of metadata aspects associated with the MLModel. Depending on the use case, this can either be all, or a selection, of supported aspects.\\\\n\\\\n\\\\n\\\\nproperty urn Aspect\\\\nKey for an MLPrimaryKey\\\\n\\\\nParameters str\\\\nNamespace for the primary key\\\\n\\\\n\\\\n\\\\nproperty name Aspect\\\\nProperties associated with a MLPrimaryKey\\\\n\\\\nParameters None | str | MLFeatureDataTypeClass\\\\nData Type of the MLPrimaryKey\\\\n\\\\n\\\\n\\\\nproperty description List[str]\\\\nSource of the MLPrimaryKey\\\\n\\\\n\\\\n\\\\nproperty version DictWrapper\\\\n\\\\nParameters List[MLPrimaryKeyKeyClass | MLPrimaryKeyPropertiesClass | OwnershipClass | InstitutionalMemoryClass | StatusClass | DeprecationClass | GlobalTagsClass | DataPlatformInstanceClass]\\\\nThe list of metadata aspects associated with the MLPrimaryKey. Depending on the use case, this can either be all, or a selection, of supported aspects.\\\\n\\\\n\\\\n\\\\nproperty urn DictWrapper\\\\nMap field type.\\\\n\\\\nParameters None | str\\\\nKey type in a map\\\\n\\\\n\\\\n\\\\nproperty valueType DictWrapper\\\\nCarries information about which roles a user is assigned to.\\\\n\\\\nParameters str\\\\nWhere the media content is stored.\\\\n\\\\n\\\\n\\\\nproperty type object\\\\nEnum defining the type of content a Media object holds.\\\\n\\\\n\\\\nIMAGE = \'IMAGE\'\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.MetadataChangeEventClass(proposedSnapshot, auditHeader=None, proposedDelta=None, systemMetadata=None)\\\\nBases\\\\n\\\\nproposedSnapshot (Union[ChartSnapshotClass, CorpGroupSnapshotClass, CorpUserSnapshotClass, DashboardSnapshotClass, DataFlowSnapshotClass, DataJobSnapshotClass, DatasetSnapshotClass, DataProcessSnapshotClass, DataPlatformSnapshotClass, MLModelSnapshotClass, MLPrimaryKeySnapshotClass, MLFeatureSnapshotClass, MLFeatureTableSnapshotClass, MLModelDeploymentSnapshotClass, MLModelGroupSnapshotClass, TagSnapshotClass, GlossaryTermSnapshotClass, GlossaryNodeSnapshotClass, DataHubPolicySnapshotClass, SchemaFieldSnapshotClass, DataHubRetentionSnapshotClass])\\\\nauditHeader (Optional[KafkaAuditHeaderClass])\\\\nproposedDelta (None)\\\\nsystemMetadata (Optional[SystemMetadataClass])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty auditHeader None\\\\nDelta of the proposed metadata partial update.\\\\n\\\\n\\\\n\\\\nproperty proposedSnapshot None | SystemMetadataClass\\\\nMetadata around how the snapshot was ingested\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.MetadataChangeLogClass(entityType, changeType, auditHeader=None, entityUrn=None, entityKeyAspect=None, aspectName=None, aspect=None, systemMetadata=None, previousAspectValue=None, previousSystemMetadata=None, created=None)\\\\nBases\\\\n\\\\nentityType (str)\\\\nchangeType (Union[str, ChangeTypeClass])\\\\nauditHeader (Optional[KafkaAuditHeaderClass])\\\\nentityUrn (Optional[str])\\\\nentityKeyAspect (Optional[GenericAspectClass])\\\\naspectName (Optional[str])\\\\naspect (Optional[GenericAspectClass])\\\\nsystemMetadata (Optional[SystemMetadataClass])\\\\npreviousAspectValue (Optional[GenericAspectClass])\\\\npreviousSystemMetadata (Optional[SystemMetadataClass])\\\\ncreated (Optional[AuditStampClass])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty aspect None | str\\\\nAspect of the entity being written to\\\\nNot filling this out implies that the writer wants to affect the entire entity\\\\nNote None | KafkaAuditHeaderClass\\\\nKafka audit header. Currently remains unused in the open source.\\\\n\\\\n\\\\n\\\\nproperty changeType None | AuditStampClass\\\\nAn audit stamp detailing who and when the aspect was changed by. Required for all intents and purposes.\\\\n\\\\n\\\\n\\\\nproperty entityKeyAspect str\\\\nType of the entity being written to\\\\n\\\\n\\\\n\\\\nproperty entityUrn None | GenericAspectClass\\\\nThe previous value of the aspect that has changed.\\\\n\\\\n\\\\n\\\\nproperty previousSystemMetadata None | SystemMetadataClass\\\\nA string-&gt;string map of custom properties that one might want to attach to an event\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.MetadataChangeProposalClass(entityType, changeType, auditHeader=None, entityUrn=None, entityKeyAspect=None, aspectName=None, aspect=None, systemMetadata=None)\\\\nBases\\\\n\\\\nentityType (str)\\\\nchangeType (Union[str, ChangeTypeClass])\\\\nauditHeader (Optional[KafkaAuditHeaderClass])\\\\nentityUrn (Optional[str])\\\\nentityKeyAspect (Optional[GenericAspectClass])\\\\naspectName (Optional[str])\\\\naspect (Optional[GenericAspectClass])\\\\nsystemMetadata (Optional[SystemMetadataClass])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty aspect None | str\\\\nAspect of the entity being written to\\\\nNot filling this out implies that the writer wants to affect the entire entity\\\\nNote None | KafkaAuditHeaderClass\\\\nKafka audit header. Currently remains unused in the open source.\\\\n\\\\n\\\\n\\\\nproperty changeType None | GenericAspectClass\\\\nKey aspect of the entity being written\\\\n\\\\n\\\\n\\\\nproperty entityType None | str\\\\nUrn of the entity being written\\\\n\\\\n\\\\n\\\\nproperty systemMetadata Aspect\\\\nMetrics to be featured for the MLModel.\\\\n\\\\nParameters None | List[str]\\\\nDecision Thresholds used (if any)?\\\\n\\\\n\\\\n\\\\nproperty performanceMeasures DictWrapper\\\\nSchema holder for MySql data definition language that describes an MySql table.\\\\n\\\\nParameters str\\\\nThe native schema in the dataset\\\\u2019s platform. This is a human readable (json blob) table schema.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.NativeGroupMembershipClass(nativeGroups)\\\\nBases\\\\nnativeGroups (List[str])\\\\n\\\\n\\\\n\\\\n\\\\nproperty nativeGroups DictWrapper\\\\nA record of all supported cells for a Notebook. Only one type of cell will be non-null.\\\\n\\\\nParameters None | ChartCellClass\\\\nThe chart cell content. The will be non-null only when all other cell field is null.\\\\n\\\\n\\\\n\\\\nproperty queryCell None | TextCellClass\\\\nThe text cell content. The will be non-null only when all other cell field is null.\\\\n\\\\n\\\\n\\\\nproperty type object\\\\nType of Notebook Cell\\\\n\\\\n\\\\nCHARTCELL = \'CHARTCELL\'\\\\n\\\\n\\\\n\\\\nQUERYCELL = \'QUERYCELL\'\\\\nCHART Notebook cell type. The cell content is chart only.\\\\n\\\\n\\\\n\\\\nTEXTCELL = \'TEXTCELL\'\\\\nQUERY Notebook cell type. The cell context is query only.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.NotebookContentClass(cells=None)\\\\nBases This is IN BETA version\\\\n\\\\nParameters List[NotebookCellClass]\\\\nThe content of a Notebook which is composed by a list of NotebookCell\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.NotebookInfoClass(title, changeAuditStamps, customProperties=None, externalUrl=None, description=None)\\\\nBases This is IN BETA version\\\\n\\\\nParameters ChangeAuditStampsClass\\\\nCaptures information about who created/last modified/deleted this Notebook and when\\\\n\\\\n\\\\n\\\\nproperty customProperties None | str\\\\nDetailed description about the Notebook\\\\n\\\\n\\\\n\\\\nproperty externalUrl str\\\\nTitle of the Notebook\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.NotebookKeyClass(notebookTool, notebookId)\\\\nBases\\\\n\\\\nnotebookTool (str)\\\\nnotebookId (str)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty notebookId str\\\\nThe name of the Notebook tool such as QueryBook, etc.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.NullTypeClass\\\\nBases DictWrapper\\\\nNumber data type Aspect\\\\nOperational info for an entity.\\\\n\\\\nParameters ClassVar[str] = \'timeseries\'\\\\n\\\\n\\\\n\\\\nproperty actor None | List[str]\\\\nWhich other datasets were affected by this operation.\\\\n\\\\n\\\\n\\\\nproperty customOperationType None | Dict[str, str]\\\\nCustom properties\\\\n\\\\n\\\\n\\\\nproperty eventGranularity int\\\\nThe time at which the operation occurred. Would be better named \\\\u2018operationTime\\\\u2019\\\\n\\\\n\\\\n\\\\nproperty messageId None | int\\\\nHow many rows were affected by this operation.\\\\n\\\\n\\\\n\\\\nproperty operationType PartitionSpecClass | None\\\\nThe optional partition specification.\\\\n\\\\n\\\\n\\\\nproperty sourceType int\\\\nThe event timestamp field as epoch at UTC in milli seconds.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.OperationSourceTypeClass\\\\nBases object\\\\nEnum to define the operation type when an entity changes.\\\\n\\\\n\\\\nALTER = \'ALTER\'\\\\nAsset was dropped\\\\n\\\\n\\\\n\\\\nCREATE = \'CREATE\'\\\\nAsset was altered\\\\n\\\\n\\\\n\\\\nCUSTOM = \'CUSTOM\'\\\\n\\\\n\\\\n\\\\nDELETE = \'DELETE\'\\\\nAsset was created\\\\n\\\\n\\\\n\\\\nDROP = \'DROP\'\\\\nCustom asset operation\\\\n\\\\n\\\\n\\\\nINSERT = \'INSERT\'\\\\nRows were updated\\\\n\\\\n\\\\n\\\\nUNKNOWN = \'UNKNOWN\'\\\\n\\\\n\\\\n\\\\nUPDATE = \'UPDATE\'\\\\nRows were deleted\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.OracleDDLClass(tableSchema)\\\\nBases\\\\ntableSchema (str)\\\\n\\\\n\\\\n\\\\n\\\\nproperty tableSchema DictWrapper\\\\nSchema text of an ORC schema.\\\\n\\\\nParameters str\\\\nThe native schema for ORC file format.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.OriginClass(type, externalType=None)\\\\nBases\\\\n\\\\ntype (Union[str, OriginTypeClass])\\\\nexternalType (Optional[str])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty externalType str | OriginTypeClass\\\\nWhere an entity originated from. Either NATIVE or EXTERNAL.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.OriginTypeClass\\\\nBases DictWrapper\\\\nSchema holder for undefined schema types.\\\\n\\\\nParameters str\\\\nThe native schema in the dataset\\\\u2019s platform.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.OwnerClass(owner, type, source=None)\\\\nBases\\\\n\\\\nowner (str)\\\\ntype (Union[str, OwnershipTypeClass])\\\\nsource (Optional[OwnershipSourceClass])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty ownerldap, urncorpGrouplimpname\\\\n(Caveat\\\\nOwner URN, e.g. urn\\\\n\\\\nType None | OwnershipSourceClass\\\\nSource information for the ownership\\\\n\\\\n\\\\n\\\\nproperty type Aspect\\\\nOwnership information of an entity.\\\\n\\\\nParameters AuditStampClass\\\\nAudit stamp containing who last modified the record and when. A value of 0 in the time field indicates missing data.\\\\n\\\\n\\\\n\\\\nproperty owners DictWrapper\\\\nSource/provider of the ownership information\\\\n\\\\nParameters str | OwnershipSourceTypeClass\\\\nThe type of the source\\\\n\\\\n\\\\n\\\\nproperty url object\\\\nAuditing system or audit logs\\\\n\\\\n\\\\nAUDIT = \'AUDIT\'\\\\nDatabase, e.g. GRANTS table\\\\n\\\\n\\\\n\\\\nDATABASE = \'DATABASE\'\\\\nFile system, e.g. file/directory owner\\\\n\\\\n\\\\n\\\\nFILESYSTEM = \'FILESYSTEM\'\\\\nIssue tracking system, e.g. Jira\\\\n\\\\n\\\\n\\\\nISSUETRACKINGSYSTEM = \'ISSUETRACKINGSYSTEM\'\\\\nManually provided by a user\\\\n\\\\n\\\\n\\\\nMANUAL = \'MANUAL\'\\\\nOther ownership-like service, e.g. Nuage, ACL service etc\\\\n\\\\n\\\\n\\\\nOTHER = \'OTHER\'\\\\n\\\\n\\\\n\\\\nSERVICE = \'SERVICE\'\\\\nSCM system, e.g. GIT, SVN\\\\n\\\\n\\\\n\\\\nSOURCECONTROL = \'SOURCECONTROL\'\\\\nOther sources\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.OwnershipTypeClass\\\\nBases DictWrapper\\\\nArbitrary key-value parameters for an Entity Change Event. (any record).\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.PartitionSpecClass(partition, type=None, timePartition=None)\\\\nBases\\\\n\\\\npartition (str)\\\\ntype (Union[str, PartitionTypeClass, None])\\\\ntimePartition (Optional[TimeWindowClass])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty partition None | TimeWindowClass\\\\nTime window of the partition if applicable\\\\n\\\\n\\\\n\\\\nproperty type object\\\\n\\\\n\\\\nFULLTABLE = \'FULLTABLE\'\\\\n\\\\n\\\\n\\\\nPARTITION = \'PARTITION\'\\\\n\\\\n\\\\n\\\\nQUERY = \'QUERY\'\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.PlatformEventClass(header, name, payload)\\\\nBases\\\\n\\\\nheader (PlatformEventHeaderClass)\\\\nname (str)\\\\npayload (GenericPayloadClass)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty header str\\\\nThe name of the event, e.g. the type of event. For example, \\\\u2018notificationRequestEvent\\\\u2019, \\\\u2018entityChangeEvent\\\\u2019\\\\n\\\\n\\\\n\\\\nproperty payload DictWrapper\\\\nA header included with each DataHub platform event.\\\\n\\\\nParameters int\\\\nThe event timestamp field as epoch at UTC in milli seconds.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.PlatformTypeClass\\\\nBases object\\\\nThe matching condition in a filter criterion\\\\n\\\\n\\\\nEQUALS = \'EQUALS\'\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.PolicyMatchCriterionClass(field, values, condition=None)\\\\nBases\\\\n\\\\nfield (str)\\\\nvalues (List[str])\\\\ncondition (Union[str, PolicyMatchConditionClass, None])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty condition str\\\\nThe name of the field that the criterion refers to\\\\n\\\\n\\\\n\\\\nproperty values DictWrapper\\\\nThe filter for specifying the resource or actor to apply privileges to\\\\n\\\\nParameters List[PolicyMatchCriterionClass]\\\\nA list of criteria to apply conjunctively (so all criteria must pass)\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.PostContentClass(title, type, description=None, link=None, media=None)\\\\nBases\\\\n\\\\ntitle (str)\\\\ntype (Union[str, PostContentTypeClass])\\\\ndescription (Optional[str])\\\\nlink (Optional[str])\\\\nmedia (Optional[MediaClass])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty description None | str\\\\nOptional link that the post is associated with.\\\\n\\\\n\\\\n\\\\nproperty media str\\\\nTitle of the post.\\\\n\\\\n\\\\n\\\\nproperty type object\\\\nEnum defining the type of content held in a Post.\\\\n\\\\n\\\\nLINK = \'LINK\'\\\\n\\\\n\\\\n\\\\nTEXT = \'TEXT\'\\\\nLink content\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.PostInfoClass(type, content, created, lastModified)\\\\nBases\\\\n\\\\ntype (Union[str, PostTypeClass])\\\\ncontent (PostContentClass)\\\\ncreated (int)\\\\nlastModified (int)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty content int\\\\nThe time at which the post was initially created\\\\n\\\\n\\\\n\\\\nproperty lastModified str | PostTypeClass\\\\nType of the Post.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.PostKeyClass(id)\\\\nBases\\\\nid (str)\\\\n\\\\n\\\\n\\\\n\\\\nproperty id object\\\\nEnum defining types of Posts.\\\\n\\\\n\\\\nHOMEPAGEANNOUNCEMENT = \'HOMEPAGEANNOUNCEMENT\'\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.PrestoDDLClass(rawSchema)\\\\nBases\\\\nrawSchema (str)\\\\n\\\\n\\\\n\\\\n\\\\nproperty rawSchema DictWrapper\\\\n\\\\nParameters str\\\\n\\\\n\\\\n\\\\nproperty value Aspect\\\\nQuantitative analyses should be disaggregated, that is, broken down by the chosen factors. Quantitative analyses should provide the results of evaluating the MLModel according to the chosen metrics, providing confidence interval values when possible.\\\\n\\\\nParameters None | str\\\\nLink to a dashboard with results showing how the MLModel performed with respect to the intersection of evaluated factors?\\\\n\\\\n\\\\n\\\\nproperty unitaryResults DictWrapper\\\\nQuery cell in a Notebook, which will present content in query format\\\\n\\\\nParameters str\\\\nUnique id for the cell. This id should be globally unique for a Notebook tool even when there are multiple deployments of it. As an example, Notebook URL could be used here for QueryBook such as \\\\u2018querybook.com/notebook/773/?cellId=1234\\\\u2019\\\\n\\\\n\\\\n\\\\nproperty cellTitle ChangeAuditStampsClass\\\\nCaptures information about who created/last modified/deleted this Notebook cell and when\\\\n\\\\n\\\\n\\\\nproperty lastExecuted str\\\\nRaw query to explain some specific logic in a Notebook\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.QueryKeyClass(id)\\\\nBases\\\\nid (str)\\\\n\\\\n\\\\n\\\\n\\\\nproperty id object\\\\nA SQL Query\\\\n\\\\n\\\\nSQL = \'SQL\'\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.QueryPropertiesClass(statement, source, created, lastModified, name=None, description=None)\\\\nBases\\\\n\\\\nstatement (QueryStatementClass)\\\\nsource (Union[str, QuerySourceClass])\\\\ncreated (AuditStampClass)\\\\nlastModified (AuditStampClass)\\\\nname (Optional[str])\\\\ndescription (Optional[str])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty created None | str\\\\nThe Query description.\\\\n\\\\n\\\\n\\\\nproperty lastModified None | str\\\\nOptional display name to identify the query.\\\\n\\\\n\\\\n\\\\nproperty source QueryStatementClass\\\\nThe Query Statement.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.QuerySourceClass\\\\nBases DictWrapper\\\\nA query statement against one or more data assets.\\\\n\\\\nParameters str | QueryLanguageClass\\\\nThe language of the Query, e.g. SQL.\\\\n\\\\n\\\\n\\\\nproperty value DictWrapper\\\\nA single subject of a particular query.\\\\nIn the future, we may evolve this model to include richer details\\\\nabout the Query Subject in relation to the query.\\\\n\\\\nParameters str\\\\nAn entity which is the subject of a query.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.QuerySubjectsClass(subjects)\\\\nBases\\\\nsubjects (List[QuerySubjectClass])\\\\n\\\\n\\\\n\\\\n\\\\nproperty subjects DictWrapper\\\\nRecord field type.\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.RetentionClass(version=None, time=None)\\\\nBases\\\\n\\\\nversion (Optional[VersionBasedRetentionClass])\\\\ntime (Optional[TimeBasedRetentionClass])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty time None | VersionBasedRetentionClass\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.RoleMembershipClass(roles)\\\\nBases\\\\nroles (List[str])\\\\n\\\\n\\\\n\\\\n\\\\nproperty roles object\\\\nThe Run Succeeded\\\\n\\\\n\\\\nFAILURE = \'FAILURE\'\\\\nThe Run Skipped\\\\n\\\\n\\\\n\\\\nSKIPPED = \'SKIPPED\'\\\\nThe Run Failed and will Retry\\\\n\\\\n\\\\n\\\\nSUCCESS = \'SUCCESS\'\\\\nThe Run Failed\\\\n\\\\n\\\\n\\\\nUPFORRETRY = \'UPFORRETRY\'\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.SchemaFieldClass(fieldPath, type, nativeDataType, jsonPath=None, nullable=None, description=None, label=None, created=None, lastModified=None, recursive=None, globalTags=None, glossaryTerms=None, isPartOfKey=None, isPartitioningKey=None, jsonProps=None)\\\\nBases\\\\n\\\\nfieldPath (str)\\\\ntype (SchemaFieldDataTypeClass)\\\\nnativeDataType (str)\\\\njsonPath (Optional[str])\\\\nnullable (Optional[bool])\\\\ndescription (Optional[str])\\\\nlabel (Optional[str])\\\\ncreated (Optional[AuditStampClass])\\\\nlastModified (Optional[AuditStampClass])\\\\nrecursive (Optional[bool])\\\\nglobalTags (Optional[GlobalTagsClass])\\\\nglossaryTerms (Optional[GlossaryTermsClass])\\\\nisPartOfKey (Optional[bool])\\\\nisPartitioningKey (Optional[bool])\\\\njsonProps (Optional[str])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty created None | str\\\\nDescription\\\\n\\\\n\\\\n\\\\nproperty fieldPath None | GlobalTagsClass\\\\nTags associated with the field\\\\n\\\\n\\\\n\\\\nproperty glossaryTerms bool\\\\nFor schema fields that are part of complex keys, set this field to true\\\\nWe do this to easily distinguish between value and key fields\\\\n\\\\n\\\\n\\\\nproperty isPartitioningKey None | str\\\\nFlattened name of a field in JSON Path notation.\\\\n\\\\n\\\\n\\\\nproperty jsonProps None | str\\\\nLabel of the field. Provides a more human-readable name for the field than field path. Some sources will\\\\nprovide this metadata but not all sources have the concept of a label. If just one string is associated with\\\\na field in a source, that is most likely a description.\\\\n\\\\n\\\\n\\\\nproperty lastModified str\\\\nThe native type of the field in the dataset\\\\u2019s platform as declared by platform schema.\\\\n\\\\n\\\\n\\\\nproperty nullable bool\\\\nThere are use cases when a field in type B references type A. A field in A references field of type B. In such cases, we will mark the first field as recursive.\\\\n\\\\n\\\\n\\\\nproperty type DictWrapper\\\\nSchema field data types\\\\n\\\\nParameters BooleanTypeClass | FixedTypeClass | StringTypeClass | BytesTypeClass | NumberTypeClass | DateTypeClass | TimeTypeClass | EnumTypeClass | NullTypeClass | MapTypeClass | ArrayTypeClass | UnionTypeClass | RecordTypeClass\\\\nData platform specific types\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.SchemaFieldKeyClass(parent, fieldPath)\\\\nBases\\\\n\\\\nparent (str)\\\\nfieldPath (str)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty fieldPath str\\\\nParent associated with the schema field\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.SchemaFieldSnapshotClass(urn, aspects)\\\\nBases\\\\n\\\\nurn (str)\\\\naspects (List[SchemaFieldKeyClass])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty aspects str\\\\nURN for the entity the metadata snapshot is associated with.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.SchemaMetadataClass(schemaName, platform, version, hash, platformSchema, fields, created=None, lastModified=None, deleted=None, dataset=None, cluster=None, primaryKeys=None, foreignKeysSpecs=None, foreignKeys=None)\\\\nBases\\\\n\\\\nschemaName (str)\\\\nplatform (str)\\\\nversion (int)\\\\nhash (str)\\\\nplatformSchema (Union[EspressoSchemaClass, OracleDDLClass, MySqlDDLClass, PrestoDDLClass, KafkaSchemaClass, BinaryJsonSchemaClass, OrcSchemaClass, SchemalessClass, KeyValueSchemaClass, OtherSchemaClass])\\\\nfields (List[SchemaFieldClass])\\\\ncreated (Optional[AuditStampClass])\\\\nlastModified (Optional[AuditStampClass])\\\\ndeleted (Optional[AuditStampClass])\\\\ndataset (Optional[str])\\\\ncluster (Optional[str])\\\\nprimaryKeys (Optional[List[str]])\\\\nforeignKeysSpecs (Optional[Dict[str, ForeignKeySpecClass]])\\\\nforeignKeys (Optional[List[ForeignKeyConstraintClass]])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty cluster AuditStampClass\\\\nAn AuditStamp corresponding to the creation of this resource/association/sub-resource. A value of 0 for time indicates missing data.\\\\n\\\\n\\\\n\\\\nproperty dataset None | AuditStampClass\\\\nAn AuditStamp corresponding to the deletion of this resource/association/sub-resource. Logically, deleted MUST have a later timestamp than creation. It may or may not have the same time as lastModified depending upon the resource/association/sub-resource semantics.\\\\n\\\\n\\\\n\\\\nproperty fields None | List[ForeignKeyConstraintClass]\\\\nList of foreign key constraints for the schema\\\\n\\\\n\\\\n\\\\nproperty foreignKeysSpecs str\\\\nthe SHA1 hash of the schema content\\\\n\\\\n\\\\n\\\\nproperty lastModified str\\\\nplatform\\\\nStandardized platform urn where schema is defined. The data platform Urn (urn\\\\n\\\\nType EspressoSchemaClass | OracleDDLClass | MySqlDDLClass | PrestoDDLClass | KafkaSchemaClass | BinaryJsonSchemaClass | OrcSchemaClass | SchemalessClass | KeyValueSchemaClass | OtherSchemaClass\\\\nThe native schema in the dataset\\\\u2019s platform.\\\\n\\\\n\\\\n\\\\nproperty primaryKeys str\\\\nSchema name e.g. PageViewEvent, identity.Profile, ams.accountmanagementtracking\\\\n\\\\n\\\\n\\\\nproperty version DictWrapper\\\\nThe dataset has no specific schema associated with it\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.SiblingsClass(siblings, primary)\\\\nBases\\\\n\\\\nsiblings (List[str])\\\\nprimary (bool)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty primary List[str]\\\\nList of sibling entities\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.SourceCodeClass(sourceCode)\\\\nBases\\\\nsourceCode (List[SourceCodeUrlClass])\\\\n\\\\n\\\\n\\\\n\\\\nproperty sourceCode DictWrapper\\\\nSource Code Url Entity\\\\n\\\\nParameters str\\\\nSource Code Url\\\\n\\\\n\\\\n\\\\nproperty type object\\\\n\\\\n\\\\nEVALUATIONPIPELINESOURCECODE = \'EVALUATIONPIPELINESOURCECODE\'\\\\n\\\\n\\\\n\\\\nMLMODELSOURCECODE = \'MLMODELSOURCECODE\'\\\\n\\\\n\\\\n\\\\nTRAININGPIPELINESOURCECODE = \'TRAININGPIPELINESOURCECODE\'\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.StatusClass(removed=None)\\\\nBases\\\\nremoved (Optional[bool])\\\\n\\\\n\\\\n\\\\n\\\\nproperty removed DictWrapper\\\\nString field type.\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.StructuredExecutionReportClass(type, serializedValue, contentType)\\\\nBases\\\\n\\\\ntype (str)\\\\nserializedValue (str)\\\\ncontentType (str)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty contentType str\\\\nThe serialized value of the structured report\\\\n\\\\n\\\\n\\\\nproperty type Aspect\\\\nSub Types. Use this aspect to specialize a generic Entity\\\\ne.g. Making a Dataset also be a View or also be a LookerExplore\\\\n\\\\nParameters List[str]\\\\nThe names of the specific types.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.SystemMetadataClass(lastObserved=None, runId=None, registryName=None, registryVersion=None, properties=None)\\\\nBases\\\\n\\\\nlastObserved (Optional[int])\\\\nrunId (Optional[str])\\\\nregistryName (Optional[str])\\\\nregistryVersion (Optional[str])\\\\nproperties (Optional[Dict[str, str]])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty lastObserved None | Dict[str, str]\\\\nAdditional properties\\\\n\\\\n\\\\n\\\\nproperty registryName None | str\\\\nThe model registry version that was used to process this event\\\\n\\\\n\\\\n\\\\nproperty runId DictWrapper\\\\nProperties of an applied tag. For now, just an Urn. In the future we can extend this with other properties, e.g.\\\\npropagation parameters.\\\\n\\\\nParameters None | str\\\\nAdditional context about the association\\\\n\\\\n\\\\n\\\\nproperty tag Aspect\\\\nKey for a Tag\\\\n\\\\nParameters str\\\\nThe tag name, which serves as a unique id\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.TagPropertiesClass(name, description=None, colorHex=None)\\\\nBases\\\\n\\\\nname (str)\\\\ndescription (Optional[str])\\\\ncolorHex (Optional[str])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty colorHex None | str\\\\nDocumentation of the tag\\\\n\\\\n\\\\n\\\\nproperty name DictWrapper\\\\nA metadata snapshot for a specific dataset entity.\\\\n\\\\nParameters List[TagKeyClass | OwnershipClass | TagPropertiesClass | StatusClass]\\\\nThe list of metadata aspects associated with the dataset. Depending on the use case, this can either be all, or a selection, of supported aspects.\\\\n\\\\n\\\\n\\\\nproperty urn Aspect\\\\nA simple wrapper around a String to persist the client ID for telemetry in DataHub\\\\u2019s backend DB\\\\n\\\\nParameters str\\\\nA string representing the telemetry client ID\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.TelemetryKeyClass(name)\\\\nBases\\\\nname (str)\\\\n\\\\n\\\\n\\\\n\\\\nproperty name DictWrapper\\\\n\\\\nParameters None | str\\\\nJSON format configuration for the test\\\\n\\\\n\\\\n\\\\nproperty type object\\\\nJSON / YAML test def\\\\n\\\\n\\\\nJSON = \'JSON\'\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.TestInfoClass(name, category, definition, description=None)\\\\nBases\\\\n\\\\nname (str)\\\\ncategory (str)\\\\ndefinition (TestDefinitionClass)\\\\ndescription (Optional[str])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty category TestDefinitionClass\\\\nConfiguration for the Test\\\\n\\\\n\\\\n\\\\nproperty description str\\\\nThe name of the test\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.TestKeyClass(id)\\\\nBases\\\\nid (str)\\\\n\\\\n\\\\n\\\\n\\\\nproperty id DictWrapper\\\\nInformation about a Test Result\\\\n\\\\nParameters str\\\\nThe urn of the test\\\\n\\\\n\\\\n\\\\nproperty type object\\\\nThe Test Succeeded\\\\n\\\\n\\\\nFAILURE = \'FAILURE\'\\\\n\\\\n\\\\n\\\\nSUCCESS = \'SUCCESS\'\\\\nThe Test Failed\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.TestResultsClass(failing, passing)\\\\nBases\\\\n\\\\nfailing (List[TestResultClass])\\\\npassing (List[TestResultClass])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty failing List[TestResultClass]\\\\nResults that are passing\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.TextCellClass(cellId, changeAuditStamps, text, cellTitle=None)\\\\nBases\\\\n\\\\ncellId (str)\\\\nchangeAuditStamps (ChangeAuditStampsClass)\\\\ntext (str)\\\\ncellTitle (Optional[str])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty cellId None | str\\\\nTitle of the cell\\\\n\\\\n\\\\n\\\\nproperty changeAuditStamps str\\\\nThe actual text in a TextCell in a Notebook\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.TimeBasedRetentionClass(maxAgeInSeconds)\\\\nBases\\\\nmaxAgeInSeconds (int)\\\\n\\\\n\\\\n\\\\n\\\\nproperty maxAgeInSeconds DictWrapper\\\\nA standard event timestamp\\\\n\\\\nParameters None | str\\\\nThe actor urn involved in the event.\\\\n\\\\nType int\\\\nWhen did the event occur\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.TimeTypeClass\\\\nBases DictWrapper\\\\n\\\\nParameters TimeWindowSizeClass\\\\nThe length of the window.\\\\n\\\\n\\\\n\\\\nproperty startTimeMillis DictWrapper\\\\nDefines the size of a time window.\\\\n\\\\nParameters int\\\\nHow many units. Defaults to 1.\\\\n\\\\n\\\\n\\\\nproperty unit Aspect\\\\nIdeally, the MLModel card would contain as much information about the training data as the evaluation data. However, there might be cases where it is not feasible to provide this level of detailed information about the training data. For example, the data may be proprietary, or require a non-disclosure agreement. In these cases, we advocate for basic details about the distributions over groups in the data, as well as any other details that could inform stakeholders on the kinds of biases the model may have encoded.\\\\n\\\\nParameters List[BaseDataClass]\\\\nDetails on the dataset(s) used for training the MLModel\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.TransformationTypeClass\\\\nBases DictWrapper\\\\nField transformation expressed in UDF\\\\n\\\\nParameters str\\\\nA UDF mentioning how the source fields got transformed to destination field. This is the FQCN(Fully Qualified Class Name) of the udf.\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.UnionTypeClass(nestedTypes=None)\\\\nBases\\\\nnestedTypes (Optional[List[str]])\\\\n\\\\n\\\\n\\\\n\\\\nproperty nestedTypes DictWrapper\\\\nUpstream lineage information about a dataset including the source reporting the lineage\\\\n\\\\nParameters AuditStampClass\\\\nAudit stamp containing who reported the lineage and when.\\\\n\\\\n\\\\n\\\\nproperty created str\\\\nThe upstream dataset the lineage points to\\\\n\\\\n\\\\n\\\\nproperty properties str | DatasetLineageTypeClass\\\\nThe type of the lineage\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.UpstreamLineageClass(upstreams, fineGrainedLineages=None)\\\\nBases\\\\n\\\\nupstreams (List[UpstreamClass])\\\\nfineGrainedLineages (Optional[List[FineGrainedLineageClass]])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty fineGrainedLineages List[UpstreamClass]\\\\nList of upstream dataset lineage information\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.UrnForeignKeyClass(currentFieldPath)\\\\nBases\\\\ncurrentFieldPath (str)\\\\n\\\\n\\\\n\\\\n\\\\nproperty currentFieldPath DictWrapper\\\\nUsage data for a given resource, rolled up into a bucket.\\\\n\\\\nParameters int\\\\nBucket start time in milliseconds\\\\n\\\\n\\\\n\\\\nproperty duration UsageAggregationMetricsClass\\\\nMetrics associated with this bucket\\\\n\\\\n\\\\n\\\\nproperty resource DictWrapper\\\\nMetrics for usage data for a given resource and bucket. Not all fields\\\\nmake sense for all buckets, so every field is optional.\\\\n\\\\nParameters None | List[FieldUsageCountsClass]\\\\nField-level usage stats\\\\n\\\\n\\\\n\\\\nproperty topSqlQueries None | int\\\\nTotal SQL query count\\\\n\\\\n\\\\n\\\\nproperty uniqueUserCount None | List[UserUsageCountsClass]\\\\nUsers within this bucket, with frequency counts\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.UserUsageCountsClass(count, user=None, userEmail=None)\\\\nBases\\\\n\\\\ncount (int)\\\\nuser (Optional[str])\\\\nuserEmail (Optional[str])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty count None | str\\\\n\\\\n\\\\n\\\\nproperty userEmail DictWrapper\\\\n\\\\nParameters int\\\\n\\\\n\\\\n\\\\nproperty value DictWrapper\\\\nKeep max N latest records\\\\n\\\\nParameters int\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.VersionInfoClass(version, versionType, customProperties=None, externalUrl=None)\\\\nBases\\\\n\\\\nversion (str)\\\\nversionType (str)\\\\ncustomProperties (Optional[Dict[str, str]])\\\\nexternalUrl (Optional[str])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproperty customProperties None | str\\\\nURL where the reference exist\\\\n\\\\n\\\\n\\\\nproperty version str\\\\nThe type of the version like git hash or md5 hash\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.VersionTagClass(versionTag=None)\\\\nBases\\\\nversionTag (Optional[str])\\\\n\\\\n\\\\n\\\\n\\\\nproperty versionTag Aspect\\\\nDetails about a View.\\\\ne.g. Gets activated when subTypes is view\\\\n\\\\nParameters bool\\\\nWhether the view is materialized\\\\n\\\\n\\\\n\\\\nproperty viewLanguage str\\\\nThe view logic\\\\n\\\\n\\\\n\\\\n\\\\nclass datahub.metadata.schemaclasses.WindowDurationClass\\\\nBases\\\\nfullname (str)\\\\n\\\\nReturn type:\\\\nRecordSchema\\\\n\\\\n\\\\n\\\\n\\\\n\\"}}>","sidebar":"overviewSidebar"},"README":{"id":"README","title":"Introduction","description":"DataHub is a data discovery application built on an extensible metadata platform that helps you tame the complexity of diverse data ecosystems."},"releases":{"id":"releases","title":"DataHub Releases","description":"Summary","sidebar":"overviewSidebar"}}}')}}]);